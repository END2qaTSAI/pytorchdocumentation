{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v1_question_generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aweZgxXBDsOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a31b9b-3824-4826-ec2e-4407aa945b6d"
      },
      "source": [
        "!pip install -U transformers==3.0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.96)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.62.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FARi6xuQ4IZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e46ce827-89c9-45d4-b1cf-89af972c950e"
      },
      "source": [
        "!python -m nltk.downloader punkt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFldiKn8EIp5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ead904-3f7a-4c82-c0fb-bfdf9db47ca8"
      },
      "source": [
        "!git clone https://github.com/patil-suraj/question_generation.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'question_generation' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAOgL63nEKIx"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import re \n",
        "import csv\n",
        "import urllib.request as req\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk3X75xXRH5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f5fcb6-e555-42fa-e0e8-5aa662b324f0"
      },
      "source": [
        "users = [\"Naveen M\",  \"Parinita Bora\", \"Anish V\", \"Megha Shyam T\", \"Nikhil Shrimali\", \"Deepak Hazarika\",  \"Vishak Bharadwaj\", \"Ritambhra Korpal\", \"Arghya\"]\n",
        "\n",
        "with open('/content/teamurlallocation.csv' , 'r') as dh:\n",
        "  usrdh = False\n",
        "  teamurllist = []\n",
        "  i = 0\n",
        "  j = 0\n",
        "  for line in dh.readlines():\n",
        "    #print(line)\n",
        "    \n",
        "    i += 1\n",
        "    print(i, line.strip())\n",
        "    teamurllist.append(line.strip())\n",
        "      #print(i , line)\n",
        "    print(j)  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh\n",
            "0\n",
            "2 https://discuss.pytorch.org/\n",
            "0\n",
            "3 https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp\n",
            "0\n",
            "4 https://pytorch.org/docs/stable/testing.html\n",
            "0\n",
            "5 https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag\n",
            "0\n",
            "6 https://pytorch.org/docs/stable/torch.html#math-operations\n",
            "0\n",
            "7 https://pytorch.org/docs/stable/torch.overrides.html\n",
            "0\n",
            "8 https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma\n",
            "0\n",
            "9 https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine\n",
            "0\n",
            "10 https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d\n",
            "0\n",
            "11 https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside\n",
            "0\n",
            "12 https://pytorch.org/docs/stable/generated/torch.is_storage.html#torch.is_storage\n",
            "0\n",
            "13 https://pytorch.org/docs/stable/generated/torch.lobpcg.html#torch.lobpcg\n",
            "0\n",
            "14 https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d\n",
            "0\n",
            "15 https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp\n",
            "0\n",
            "16 https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types\n",
            "0\n",
            "17 https://pytorch.org/docs/stable/generated/torch.set_num_interop_threads.html#torch.set_num_interop_threads\n",
            "0\n",
            "18 https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter\n",
            "0\n",
            "19 https://pytorch.org/docs/stable/generated/torch.stack.html#torch.stack\n",
            "0\n",
            "20 https://pytorch.org/blog/\n",
            "0\n",
            "21 https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type\n",
            "0\n",
            "22 https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat\n",
            "0\n",
            "23 https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack\n",
            "0\n",
            "24 https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide\n",
            "0\n",
            "25 https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv\n",
            "0\n",
            "26 https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian\n",
            "0\n",
            "27 https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided\n",
            "0\n",
            "28 https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan\n",
            "0\n",
            "29 https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d\n",
            "0\n",
            "30 https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor\n",
            "0\n",
            "31 https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh\n",
            "0\n",
            "32 https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll\n",
            "0\n",
            "33 https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile\n",
            "0\n",
            "34 https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk\n",
            "0\n",
            "35 https://pytorch.org/docs/stable/nn.functional.html\n",
            "0\n",
            "36 https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt\n",
            "0\n",
            "37 https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum\n",
            "0\n",
            "38 https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide\n",
            "0\n",
            "39 https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn\n",
            "0\n",
            "40 https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like\n",
            "0\n",
            "41 https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad\n",
            "0\n",
            "42 https://pytorch.org/docs/stable/generated/torch.get_num_threads.html#torch.get_num_threads\n",
            "0\n",
            "43 https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex\n",
            "0\n",
            "44 https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes\n",
            "0\n",
            "45 https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor\n",
            "0\n",
            "46 https://pytorch.org/docs/stable/generated/torch.clip.html#torch.clip\n",
            "0\n",
            "47 https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh\n",
            "0\n",
            "48 https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype\n",
            "0\n",
            "49 https://pytorch.org/docs/stable/generated/torch.absolute.html#torch.absolute\n",
            "0\n",
            "50 https://pytorch.org/docs/stable/generated/torch.median.html#torch.median\n",
            "0\n",
            "51 https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin\n",
            "0\n",
            "52 https://pytorch.org/docs/stable/onnx.html\n",
            "0\n",
            "53 https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices\n",
            "0\n",
            "54 https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub\n",
            "0\n",
            "55 https://pytorch.org/docs/stable/amp.html\n",
            "0\n",
            "56 https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet\n",
            "0\n",
            "57 https://pytorch.org/resources\n",
            "0\n",
            "58 https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt\n",
            "0\n",
            "59 https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor\n",
            "0\n",
            "60 https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan\n",
            "0\n",
            "61 https://pytorch.org/docs/stable/generated/torch.square.html#torch.square\n",
            "0\n",
            "62 https://pytorch.org/docs/stable/generated/torch.take.html#torch.take\n",
            "0\n",
            "63 https://pytorch.org/docs/stable/generated/torch.arccosh.html#torch.arccosh\n",
            "0\n",
            "64 https://pytorch.org/docs/stable/torch.html#torch.torch.default_generator\n",
            "0\n",
            "65 https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal\n",
            "0\n",
            "66 https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean\n",
            "0\n",
            "67 https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims\n",
            "0\n",
            "68 https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack\n",
            "0\n",
            "69 https://pytorch.org/docs/stable/special.html#torch.special.erfinv\n",
            "0\n",
            "70 https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf\n",
            "0\n",
            "71 https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot\n",
            "0\n",
            "72 https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp\n",
            "0\n",
            "73 https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm\n",
            "0\n",
            "74 https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp\n",
            "0\n",
            "75 https://pytorch.org/docs/stable/generated/torch.scatter.html#torch.scatter\n",
            "0\n",
            "76 https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat\n",
            "0\n",
            "77 https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist\n",
            "0\n",
            "78 https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select\n",
            "0\n",
            "79 https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc\n",
            "0\n",
            "80 https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve\n",
            "0\n",
            "81 https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin\n",
            "0\n",
            "82 https://pytorch.org/docs/stable/distributions.html\n",
            "0\n",
            "83 https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine\n",
            "0\n",
            "84 https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive\n",
            "0\n",
            "85 https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit\n",
            "0\n",
            "86 https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf\n",
            "0\n",
            "87 https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p\n",
            "0\n",
            "88 https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater\n",
            "0\n",
            "89 https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm\n",
            "0\n",
            "90 https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc\n",
            "0\n",
            "91 https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff\n",
            "0\n",
            "92 https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli\n",
            "0\n",
            "93 https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin\n",
            "0\n",
            "94 https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal\n",
            "0\n",
            "95 https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not\n",
            "0\n",
            "96 https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse\n",
            "0\n",
            "97 https://pytorch.org/docs/stable/generated/torch.blackman_window.html#torch.blackman_window\n",
            "0\n",
            "98 https://pytorch.org/docs/stable/_sources/torch.rst.txt\n",
            "0\n",
            "99 https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf\n",
            "0\n",
            "100 https://pytorch.org/docs/stable/torch.html#spectral-ops\n",
            "0\n",
            "101 https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos\n",
            "0\n",
            "102 https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract\n",
            "0\n",
            "103 https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk\n",
            "0\n",
            "104 https://pytorch.org/docs/stable/torch.html#parallelism\n",
            "0\n",
            "105 https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs\n",
            "0\n",
            "106 https://pytorch.org/audio/stable\n",
            "0\n",
            "107 https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy\n",
            "0\n",
            "108 https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel\n",
            "0\n",
            "109 https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc\n",
            "0\n",
            "110 https://pytorch.org/docs/stable/benchmark_utils.html\n",
            "0\n",
            "111 https://pytorch.org/docs/stable/generated/torch.save.html#torch.save\n",
            "0\n",
            "112 https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled\n",
            "0\n",
            "113 https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations\n",
            "0\n",
            "114 https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv\n",
            "0\n",
            "115 https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax\n",
            "0\n",
            "116 https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger\n",
            "0\n",
            "117 https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel\n",
            "0\n",
            "118 https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace\n",
            "0\n",
            "119 https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh\n",
            "0\n",
            "120 https://pytorch.org/docs/stable/package.html\n",
            "0\n",
            "121 https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum\n",
            "0\n",
            "122 https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power\n",
            "0\n",
            "123 https://pytorch.org/docs/stable/torch.html#quasi-random-sampling\n",
            "0\n",
            "124 https://pytorch.org/docs/stable/profiler.html\n",
            "0\n",
            "125 https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique\n",
            "0\n",
            "126 https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin\n",
            "0\n",
            "127 https://pytorch.org/docs/stable/optim.html\n",
            "0\n",
            "128 https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_\n",
            "0\n",
            "129 https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile\n",
            "0\n",
            "130 https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig\n",
            "0\n",
            "131 https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid\n",
            "0\n",
            "132 https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd\n",
            "0\n",
            "133 https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp\n",
            "0\n",
            "134 https://pytorch.org/docs/stable/generated/torch.is_floating_point.html#torch.is_floating_point\n",
            "0\n",
            "135 https://pytorch.org/docs/stable/nn.html\n",
            "0\n",
            "136 https://pytorch.org/docs/stable/hub.html\n",
            "0\n",
            "137 https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10\n",
            "0\n",
            "138 https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin\n",
            "0\n",
            "139 https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer\n",
            "0\n",
            "140 https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan\n",
            "0\n",
            "141 https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr\n",
            "0\n",
            "142 https://twitter.com/pytorch\n",
            "0\n",
            "143 https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod\n",
            "0\n",
            "144 https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner\n",
            "0\n",
            "145 https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose\n",
            "0\n",
            "146 https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt\n",
            "0\n",
            "147 https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted\n",
            "0\n",
            "148 https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag\n",
            "0\n",
            "149 https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv\n",
            "0\n",
            "150 https://pytorch.org/docs/stable/backends.html\n",
            "0\n",
            "151 https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md\n",
            "0\n",
            "152 https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or\n",
            "0\n",
            "153 https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac\n",
            "0\n",
            "154 https://pytorch.org/docs/stable/fft.html\n",
            "0\n",
            "155 https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint\n",
            "0\n",
            "156 https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select\n",
            "0\n",
            "157 https://pytorch.org/docs/stable/generated/torch.randn_like.html#torch.randn_like\n",
            "0\n",
            "158 https://pytorch.org/docs/stable/torch.html#tensors\n",
            "0\n",
            "159 https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm\n",
            "0\n",
            "160 https://pytorch.org/docs/stable/generated/torch.hamming_window.html#torch.hamming_window\n",
            "0\n",
            "161 https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim\n",
            "0\n",
            "162 https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm\n",
            "0\n",
            "163 https://pytorch.org/docs/stable/torch.html#serialization\n",
            "0\n",
            "164 https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod\n",
            "0\n",
            "165 https://pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2\n",
            "0\n",
            "166 https://pytorch.org/docs/stable/generated/torch.where.html#torch.where\n",
            "0\n",
            "167 https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like\n",
            "0\n",
            "168 https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm\n",
            "0\n",
            "169 https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn\n",
            "0\n",
            "170 https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson\n",
            "0\n",
            "171 https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack\n",
            "0\n",
            "172 https://pytorch.org/docs/stable/generated/torch.set_warn_always.html#torch.set_warn_always\n",
            "0\n",
            "173 https://pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like\n",
            "0\n",
            "174 https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape\n",
            "0\n",
            "175 https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize\n",
            "0\n",
            "176 https://pytorch.org/docs/stable/generated/torch.scatter_add.html#torch.scatter_add\n",
            "0\n",
            "177 https://pytorch.org/docs/stable/cpp_index.html\n",
            "0\n",
            "178 https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign\n",
            "0\n",
            "179 https://pytorch.org/docs/stable/generated/torch._assert.html#torch._assert\n",
            "0\n",
            "180 https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue\n",
            "0\n",
            "181 https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky\n",
            "0\n",
            "182 https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc\n",
            "0\n",
            "183 https://pytorch.org/docs/stable/torch.html#creation-ops\n",
            "0\n",
            "184 https://pytorch.org/docs/stable/torch.html#torch\n",
            "0\n",
            "185 https://pytorch.org/text/stable\n",
            "0\n",
            "186 https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq\n",
            "0\n",
            "187 https://pytorch.org/docs/stable/torch.html#utilities\n",
            "0\n",
            "188 https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze\n",
            "0\n",
            "189 https://pytorch.org/docs/stable/torch.html#random-sampling\n",
            "0\n",
            "190 https://pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank\n",
            "0\n",
            "191 https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax\n",
            "0\n",
            "192 https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag\n",
            "0\n",
            "193 https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp\n",
            "0\n",
            "194 https://pytorch.org/docs/stable/generated/torch.initial_seed.html#torch.initial_seed\n",
            "0\n",
            "195 https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_\n",
            "0\n",
            "196 https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle\n",
            "0\n",
            "197 https://pytorch.org/docs/stable/special.html#torch.special.exp2\n",
            "0\n",
            "198 https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_\n",
            "0\n",
            "199 https://pytorch.org/docs/stable/generated/torch.range.html#torch.range\n",
            "0\n",
            "200 https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to\n",
            "0\n",
            "201 https://pytorch.org/docs/stable/generated/torch.is_complex.html#torch.is_complex\n",
            "0\n",
            "202 https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze\n",
            "0\n",
            "203 https://pytorch.org/ecosystem\n",
            "0\n",
            "204 https://pytorch.org/docs/stable/random.html\n",
            "0\n",
            "205 https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr\n",
            "0\n",
            "206 https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse\n",
            "0\n",
            "207 https://pytorch.org/docs/stable/notes/randomness.html\n",
            "0\n",
            "208 https://pytorch.org/docs/stable/generated/torch.det.html#torch.det\n",
            "0\n",
            "209 https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit\n",
            "0\n",
            "210 https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast\n",
            "0\n",
            "211 https://pytorch.org/features\n",
            "0\n",
            "212 https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet\n",
            "0\n",
            "213 https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr\n",
            "0\n",
            "214 https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num\n",
            "0\n",
            "215 https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet\n",
            "0\n",
            "216 https://pytorch.org/docs/stable/generated/torch.div.html#torch.div\n",
            "0\n",
            "217 https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix\n",
            "0\n",
            "218 https://pytorch.org/docs/stable/jit.html\n",
            "0\n",
            "219 https://pytorch.org/vision/stable/index.html\n",
            "0\n",
            "220 https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal\n",
            "0\n",
            "221 https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack\n",
            "0\n",
            "222 https://pytorch.org/docs/stable/generated/torch.is_tensor.html\n",
            "0\n",
            "223 https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal\n",
            "0\n",
            "224 https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum\n",
            "0\n",
            "225 https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp\n",
            "0\n",
            "226 https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum\n",
            "0\n",
            "227 https://pytorch.org/docs/stable/generated/torch.bartlett_window.html#torch.bartlett_window\n",
            "0\n",
            "228 https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm\n",
            "0\n",
            "229 https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros\n",
            "0\n",
            "230 https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh\n",
            "0\n",
            "231 https://pytorch.org/docs/stable/generated/torch.any.html#torch.any\n",
            "0\n",
            "232 https://pytorch.org/audio/stable/index.html\n",
            "0\n",
            "233 https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve\n",
            "0\n",
            "234 https://github.com/rtfd/sphinx_rtd_theme\n",
            "0\n",
            "235 https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort\n",
            "0\n",
            "236 https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax\n",
            "0\n",
            "237 https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather\n",
            "0\n",
            "238 https://www.facebook.com/pytorch\n",
            "0\n",
            "239 https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal\n",
            "0\n",
            "240 https://pytorch.org/docs/stable/nn.init.html\n",
            "0\n",
            "241 https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm\n",
            "0\n",
            "242 https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv\n",
            "0\n",
            "243 https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type\n",
            "0\n",
            "244 https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype\n",
            "0\n",
            "245 https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin\n",
            "0\n",
            "246 https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode\n",
            "0\n",
            "247 https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal\n",
            "0\n",
            "248 https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor\n",
            "0\n",
            "249 https://pytorch.org/docs/stable/distributed.optim.html\n",
            "0\n",
            "250 https://pytorch.org/serve/\n",
            "0\n",
            "251 https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft\n",
            "0\n",
            "252 https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange\n",
            "0\n",
            "253 https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot\n",
            "0\n",
            "254 https://pytorch.org/docs/stable/torch.html#in-place-random-sampling\n",
            "0\n",
            "255 https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided\n",
            "0\n",
            "256 https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose\n",
            "0\n",
            "257 https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross\n",
            "0\n",
            "258 https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90\n",
            "0\n",
            "259 https://pytorch.org/docs/stable/torch.html#pointwise-ops\n",
            "0\n",
            "260 https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_\n",
            "0\n",
            "261 https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu\n",
            "0\n",
            "262 https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma\n",
            "0\n",
            "263 https://pytorch.org/docs/stable/generated/torch.var.html#torch.var\n",
            "0\n",
            "264 https://pytorch.org/docs/stable/special.html#torch.special.erf\n",
            "0\n",
            "265 https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose\n",
            "0\n",
            "266 https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "0\n",
            "267 https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos\n",
            "0\n",
            "268 https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n",
            "0\n",
            "269 https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "0\n",
            "270 https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "0\n",
            "271 https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads\n",
            "0\n",
            "272 https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero\n",
            "0\n",
            "273 https://pytorch.org/docs/stable/tensors.html\n",
            "0\n",
            "274 https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit\n",
            "0\n",
            "275 https://pytorch.org/docs/stable/sparse.html\n",
            "0\n",
            "276 https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin\n",
            "0\n",
            "277 https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like\n",
            "0\n",
            "278 https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft\n",
            "0\n",
            "279 https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient\n",
            "0\n",
            "280 https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount\n",
            "0\n",
            "281 https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state\n",
            "0\n",
            "282 https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj\n",
            "0\n",
            "283 https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive\n",
            "0\n",
            "284 https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan\n",
            "0\n",
            "285 https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack\n",
            "0\n",
            "286 https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_\n",
            "0\n",
            "287 https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor\n",
            "0\n",
            "288 https://pytorch.org/#community-module\n",
            "0\n",
            "289 https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices\n",
            "0\n",
            "290 https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder\n",
            "0\n",
            "291 https://pytorch.org/docs/stable/generated/torch.seed.html#torch.seed\n",
            "0\n",
            "292 https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide\n",
            "0\n",
            "293 https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads\n",
            "0\n",
            "294 https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye\n",
            "0\n",
            "295 https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv\n",
            "0\n",
            "296 https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc\n",
            "0\n",
            "297 https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2\n",
            "0\n",
            "298 https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1\n",
            "0\n",
            "299 https://pytorch.org/vision/stable\n",
            "0\n",
            "300 https://pytorch.org/elastic/\n",
            "0\n",
            "301 https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window\n",
            "0\n",
            "302 https://pytorch.org/docs/stable/linalg.html\n",
            "0\n",
            "303 https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor\n",
            "0\n",
            "304 https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or\n",
            "0\n",
            "305 https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve\n",
            "0\n",
            "306 https://pytorch.org/docs/stable/cuda.html\n",
            "0\n",
            "307 https://pytorch.org/docs/stable/special.html#torch.special.logit\n",
            "0\n",
            "308 https://pytorch.org/docs/stable/__config__.html\n",
            "0\n",
            "309 https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander\n",
            "0\n",
            "310 https://pytorch.org/docs/stable/distributed.elastic.html\n",
            "0\n",
            "311 https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp\n",
            "0\n",
            "312 https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv\n",
            "0\n",
            "313 https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis\n",
            "0\n",
            "314 https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma\n",
            "0\n",
            "315 https://pytorch.org/docs/stable/futures.html\n",
            "0\n",
            "316 https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod\n",
            "0\n",
            "317 https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist\n",
            "0\n",
            "318 https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply\n",
            "0\n",
            "319 https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty\n",
            "0\n",
            "320 https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum\n",
            "0\n",
            "321 https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal\n",
            "0\n",
            "322 https://pytorch.org/docs/stable/generated/torch.le.html#torch.le\n",
            "0\n",
            "323 https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation\n",
            "0\n",
            "324 https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand\n",
            "0\n",
            "325 https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim\n",
            "0\n",
            "326 https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf\n",
            "0\n",
            "327 https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and\n",
            "0\n",
            "328 https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg\n",
            "0\n",
            "329 https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace\n",
            "0\n",
            "330 https://pytorch.org/docs/stable/distributed.html\n",
            "0\n",
            "331 https://pytorch.org/docs/stable/index.html\n",
            "0\n",
            "332 https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray\n",
            "0\n",
            "333 https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil\n",
            "0\n",
            "334 https://pytorch.org/docs/stable/torch.html#comparison-ops\n",
            "0\n",
            "335 https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det\n",
            "0\n",
            "336 https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero\n",
            "0\n",
            "337 https://pytorch.org/\n",
            "0\n",
            "338 https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal\n",
            "0\n",
            "339 https://pytorch.org/docs/stable/generated/torch.is_warn_always_enabled.html#torch.is_warn_always_enabled\n",
            "0\n",
            "340 https://pytorch.org/docs/stable/torch.html#\n",
            "0\n",
            "341 https://pytorch.org/docs/stable/generated/torch.dequantize.html#torch.dequantize\n",
            "0\n",
            "342 https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort\n",
            "0\n",
            "343 https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power\n",
            "0\n",
            "344 https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed\n",
            "0\n",
            "345 https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac\n",
            "0\n",
            "346 https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product\n",
            "0\n",
            "347 https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril\n",
            "0\n",
            "348 https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0\n",
            "0\n",
            "349 https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr\n",
            "0\n",
            "350 https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack\n",
            "0\n",
            "351 https://pytorch.org/docs/stable/mobile_optimizer.html\n",
            "0\n",
            "352 https://pytorch.org/docs/stable/generated/torch.set_printoptions.html#torch.set_printoptions\n",
            "0\n",
            "353 https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow\n",
            "0\n",
            "354 https://pytorch.org/docs/stable/special.html\n",
            "0\n",
            "355 https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor\n",
            "0\n",
            "356 https://pytorch.org/text/stable/index.html\n",
            "0\n",
            "357 https://pytorch.org/mobile\n",
            "0\n",
            "358 https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode\n",
            "0\n",
            "359 https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor\n",
            "0\n",
            "360 https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz\n",
            "0\n",
            "361 https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul\n",
            "0\n",
            "362 https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal\n",
            "0\n",
            "363 https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp\n",
            "0\n",
            "364 https://pytorch.org/docs/stable/generated/torch.max.html#torch.max\n",
            "0\n",
            "365 https://pytorch.org/docs/stable/model_zoo.html\n",
            "0\n",
            "366 https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean\n",
            "0\n",
            "367 https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile\n",
            "0\n",
            "368 https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot\n",
            "0\n",
            "369 https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh\n",
            "0\n",
            "370 https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window\n",
            "0\n",
            "371 https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq\n",
            "0\n",
            "372 https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled\n",
            "0\n",
            "373 https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real\n",
            "0\n",
            "374 https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite\n",
            "0\n",
            "375 https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_\n",
            "0\n",
            "376 https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud\n",
            "0\n",
            "377 https://pytorch.org/docs/stable/generated/torch.full.html#torch.full\n",
            "0\n",
            "378 https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_\n",
            "0\n",
            "379 https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator\n",
            "0\n",
            "380 https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled\n",
            "0\n",
            "381 https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad\n",
            "0\n",
            "382 https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean\n",
            "0\n",
            "383 https://pytorch.org/docs/stable/storage.html\n",
            "0\n",
            "384 https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex\n",
            "0\n",
            "385 https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve\n",
            "0\n",
            "386 https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones\n",
            "0\n",
            "387 https://pytorch.org/docs/stable/generated/torch.t.html#torch.t\n",
            "0\n",
            "388 https://pytorch.org/docs/stable/special.html#torch.special.expm1\n",
            "0\n",
            "389 https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt\n",
            "0\n",
            "390 https://pytorch.org/docs/stable/generated/torch.round.html#torch.round\n",
            "0\n",
            "391 https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank\n",
            "0\n",
            "392 https://www.youtube.com/pytorch\n",
            "0\n",
            "393 https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow\n",
            "0\n",
            "394 https://pytorch.org/docs/stable/generated/torch.min.html#torch.min\n",
            "0\n",
            "395 https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf\n",
            "0\n",
            "396 https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial\n",
            "0\n",
            "397 https://pytorch.org/docs/stable/notes/modules.html\n",
            "0\n",
            "398 https://pytorch.org/docs/stable/tensors.html#torch.Tensor\n",
            "0\n",
            "399 https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip\n",
            "0\n",
            "400 https://pytorch.org/docs/stable/special.html#torch.special.erfc\n",
            "0\n",
            "401 https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd\n",
            "0\n",
            "402 https://pytorch.org/docs/stable/generated/torch.real.html#torch.real\n",
            "0\n",
            "403 https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy\n",
            "0\n",
            "404 https://pytorch.org/docs/stable/dlpack.html\n",
            "0\n",
            "405 https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel\n",
            "0\n",
            "406 https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad\n",
            "0\n",
            "407 https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig\n",
            "0\n",
            "408 https://pytorch.org/docs/stable/data.html\n",
            "0\n",
            "409 https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum\n",
            "0\n",
            "410 https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not\n",
            "0\n",
            "411 https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit\n",
            "0\n",
            "412 https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit\n",
            "0\n",
            "413 https://pytorch.org/docs/stable/special.html#torch.special.expit\n",
            "0\n",
            "414 https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr\n",
            "0\n",
            "415 https://pytorch.org/javadoc/\n",
            "0\n",
            "416 https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh\n",
            "0\n",
            "417 https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg\n",
            "0\n",
            "418 https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_\n",
            "0\n",
            "419 https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes\n",
            "0\n",
            "420 https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype\n",
            "0\n",
            "421 https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod\n",
            "0\n",
            "422 https://pytorch.org/docs/stable/torch.html#generators\n",
            "0\n",
            "423 https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like\n",
            "0\n",
            "424 https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten\n",
            "0\n",
            "425 https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and\n",
            "0\n",
            "426 https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone\n",
            "0\n",
            "427 https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma\n",
            "0\n",
            "428 https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops\n",
            "0\n",
            "429 https://pytorch.org/serve\n",
            "0\n",
            "430 https://pytorch.org/docs/stable/fx.html\n",
            "0\n",
            "431 https://pytorch.org/docs/stable/generated/torch.split.html#torch.split\n",
            "0\n",
            "432 https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign\n",
            "0\n",
            "433 https://pytorch.org/docs/stable/cpp_extension.html\n",
            "0\n",
            "434 https://pytorch.org/docs/stable/torch.html#other-operations\n",
            "0\n",
            "435 https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave\n",
            "0\n",
            "436 https://pytorch.org/docs/stable/generated/torch.add.html#torch.add\n",
            "0\n",
            "437 https://pytorch.org/docs/stable/generated/torch.get_rng_state.html#torch.get_rng_state\n",
            "0\n",
            "438 https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron\n",
            "0\n",
            "439 https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar\n",
            "0\n",
            "440 https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2\n",
            "0\n",
            "441 https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms\n",
            "0\n",
            "442 https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul\n",
            "0\n",
            "443 https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax\n",
            "0\n",
            "444 https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split\n",
            "0\n",
            "445 https://pytorch.org/docs/stable/generated/torch.rand_like.html#torch.rand_like\n",
            "0\n",
            "446 https://pytorch.org/docs/stable/generated/torch.std.html#torch.std\n",
            "0\n",
            "447 https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos\n",
            "0\n",
            "448 https://pytorch.org/docs/stable/quantization.html\n",
            "0\n",
            "449 https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html#torch.is_grad_enabled\n",
            "0\n",
            "450 https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine\n",
            "0\n",
            "451 https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_\n",
            "0\n",
            "452 https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm\n",
            "0\n",
            "453 https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge\n",
            "0\n",
            "454 https://pytorch.org/docs/stable/generated/torch.all.html#torch.all\n",
            "0\n",
            "455 https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot\n",
            "0\n",
            "456 https://pytorch.org/docs/stable/tensorboard.html\n",
            "0\n",
            "457 https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace\n",
            "0\n",
            "458 https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm\n",
            "0\n",
            "459 https://pytorch.org/docs/stable/autograd.html\n",
            "0\n",
            "460 https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul\n",
            "0\n",
            "461 https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh\n",
            "0\n",
            "462 https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2\n",
            "0\n",
            "463 https://pytorch.org/docs/stable/torch.html#reduction-ops\n",
            "0\n",
            "464 https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne\n",
            "0\n",
            "465 https://pytorch.org/tutorials\n",
            "0\n",
            "466 https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort\n",
            "0\n",
            "467 https://github.com/pytorch/pytorch\n",
            "0\n",
            "468 https://pytorch.org/docs/stable/bottleneck.html\n",
            "0\n",
            "469 https://pytorch.org/docs/stable/checkpoint.html\n",
            "0\n",
            "470 https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind\n",
            "0\n",
            "471 https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative\n",
            "0\n",
            "472 https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse\n",
            "0\n",
            "473 https://pytorch.org/docs/stable/generated/torch.compiled_with_cxx11_abi.html#torch.compiled_with_cxx11_abi\n",
            "0\n",
            "474 https://pytorch.org/docs/stable/generated/torch.matrix_power.html#torch.matrix_power\n",
            "0\n",
            "475 https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu\n",
            "0\n",
            "476 https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma\n",
            "0\n",
            "477 https://pytorch.org/docs/stable/generated/torch.load.html#torch.load\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyW13th-Z0qv"
      },
      "source": [
        "qalist = []\n",
        "idn = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SkLtulvGO78",
        "outputId": "af5e4a2b-80dd-4cd7-ee25-e04442b0dcac"
      },
      "source": [
        "j = 1\n",
        "for i in range(53,478,53):\n",
        "  print(j , i)\n",
        "  j += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 53\n",
            "2 106\n",
            "3 159\n",
            "4 212\n",
            "5 265\n",
            "6 318\n",
            "7 371\n",
            "8 424\n",
            "9 477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0qTpSFpZ0mr"
      },
      "source": [
        "start_idx = 265\n",
        "end_idx = 270"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE8sqwmXZ0is"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvBFbCVGELuW"
      },
      "source": [
        "## Single task QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-NADwap0Eaf"
      },
      "source": [
        "text2 = \"\"\"PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n",
        "Features described in this documentation are classified by release status:\n",
        "Stable:  These features will be maintained long-term and there should generally\n",
        "be no major performance limitations or gaps in documentation.\n",
        "We also expect to maintain backwards compatibility (although\n",
        "breaking changes can happen and notice will be given one release ahead\n",
        "of time).\n",
        "Beta:  These features are tagged as Beta because the API may change based on\n",
        "user feedback, because the performance needs to improve, or because\n",
        "coverage across operators is not yet complete. For Beta features, we are\n",
        "committing to seeing the feature through to the Stable classification.\n",
        "We are not, however, committing to backwards compatibility.\n",
        "Prototype:  These features are typically not available as part of\n",
        "binary distributions like PyPI or Conda, except sometimes behind run-time\n",
        "flags, and are at an early stage for feedback and testing.\n",
        "Notes\n",
        "Language Bindings\n",
        "Python API\n",
        "Libraries\n",
        "Community\n",
        "Index\n",
        "Module Inde\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy_T1CiVVNuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9470b7df-bd75-4936-dbb1-db6ff66ca6f0"
      },
      "source": [
        "%cd question_generation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/question_generation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxoSS2_WEMvx"
      },
      "source": [
        "from pipelines import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFSZiIc0StHY"
      },
      "source": [
        "nlp = pipeline(\"question-generation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZy5F8sjSv2W"
      },
      "source": [
        "#nlp(text3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHB0dDqTb-o"
      },
      "source": [
        "If you want to use the t5-base model, then pass the path through model parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_050CddNTWeU"
      },
      "source": [
        "nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-Rd7yZmkQi8"
      },
      "source": [
        "nlp_mtask = pipeline(\"multitask-qa-qg\", model=\"valhalla/t5-base-qa-qg-hl\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDY-DUTrqnRU",
        "outputId": "0d5fe19f-be40-4060-802b-5fc74ef402e3"
      },
      "source": [
        "txt = \"\"\"Matrix product of two tensors.The behavior depends on the dimensionality of the tensors as follows:If both tensors are 1-dimensional, the dot product (scalar) is returned.If both arguments are 2-dimensional, the matrix-matrix product is returned.If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
        "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
        "After the matrix multiply, the prepended dimension is removed.If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
        "the matrix-vector product is returned.If both arguments are at least 1-dimensional and at least one argument is\n",
        "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
        "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
        "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
        "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.The non-matrix (i.e. )\"\"\"\n",
        "\n",
        "\"\"\"# is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
        "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
        "must be broadcastable).\"\"\"\n",
        "\n",
        "nlp(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'answer': 'Matrix product of two tensors',\n",
              "  'question': 'What is the behavior dependent on the dimensionality of the tensors?'},\n",
              " {'answer': 'the matrix multiply',\n",
              "  'question': 'When is the prepended dimension removed?'},\n",
              " {'answer': 'batched matrix multiply',\n",
              "  'question': 'If the first argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of what?'},\n",
              " {'answer': 'non-matrix',\n",
              "  'question': 'What is another term for a matrix product of two tensors?'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJDsvgjVrKIg",
        "outputId": "e3f73eb8-2ce5-4087-9007-955cca44e77a"
      },
      "source": [
        "len(txt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1032"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3stIEi9XkaHx"
      },
      "source": [
        "### 512 length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc48Q2VGQPIz"
      },
      "source": [
        "uniqdict = {}\n",
        "qadict = {}\n",
        "\n",
        "#nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
        "lengthlist = [512 , 1024, 2048 , 3072 , 4096]\n",
        "\n",
        "\n",
        "for url in teamurllist[start_idx:end_idx]:\n",
        "  #url = 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc' \n",
        "  # 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'\n",
        "  #'https://pytorch.org/docs/stable/autograd.html'\n",
        "  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n",
        "  #print(url)\n",
        "  try:\n",
        "    html = req.urlopen(url).read()\n",
        "  except:\n",
        "    continue\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  article = soup.find('article')\n",
        "  \n",
        "  if article:\n",
        "\n",
        "    sctns = article.find_all('div', class_=\"section\")\n",
        "\n",
        "    for sctn in sctns:\n",
        "      ptags = sctn.find_all('p')\n",
        "\n",
        "      \n",
        "      ptextlist = []\n",
        "\n",
        "      i = 1\n",
        "      for ptag in ptags:\n",
        "        ptextlist.append([ i , ptag.get_text(strip=True)])\n",
        "        i += 1\n",
        "\n",
        "      #print(ptextlist)\n",
        "\n",
        "      ptextlistlen = len(ptextlist)\n",
        "\n",
        "      ptextlt1024list = []\n",
        "      alltextdone = False\n",
        "\n",
        "      for j in range(40):\n",
        "        ptext = \"\"\n",
        "        if not alltextdone:\n",
        "\n",
        "          for k in range(j,ptextlistlen):\n",
        "          \n",
        "            arry = ptextlist[k]\n",
        "            #print(arry)\n",
        "            if len(ptext) + len(arry[1]) + 1 < 512: #1024:\n",
        "              ptext += arry[1]\n",
        "              ptext += ' '\n",
        "\n",
        "              if arry[0] == ptextlistlen:\n",
        "\n",
        "                #print('len 1',len(ptext))\n",
        "                try:\n",
        "                  listofdict = nlp(ptext)\n",
        "\n",
        "                  for qas in listofdict:\n",
        "                    qas['context'] = ptext\n",
        "                    qas['source'] = url\n",
        "                    qadict[qas['question']] = qas\n",
        "\n",
        "                except:\n",
        "                  1 == 1\n",
        "                \n",
        "                \n",
        "                try:\n",
        "                  listofdict = nlp_mtask(ptext)\n",
        "\n",
        "                  for qas in listofdict:\n",
        "                    qas['context'] = ptext\n",
        "                    qas['source'] = url\n",
        "                    qadict[qas['question']] = qas\n",
        "\n",
        "                except:\n",
        "                  1 == 1\n",
        "\n",
        "\n",
        "                alltextdone = True\n",
        "                break\n",
        "          \n",
        "            else:\n",
        "              \n",
        "              #print('len 2', '*'*40 , ptext , '*'*40, len(ptext))\n",
        "              try:\n",
        "                listofdict = nlp(ptext)\n",
        "\n",
        "                for qas in listofdict:\n",
        "                  #print(qas)\n",
        "                  qas['context'] = ptext\n",
        "                  qas['source'] = url\n",
        "\n",
        "                  qadict[qas['question']] = qas\n",
        "              except:\n",
        "                1 == 1\n",
        "              \n",
        "              try:\n",
        "                listofdict = nlp_mtask(ptext)\n",
        "\n",
        "                for qas in listofdict:\n",
        "                  qas['context'] = ptext\n",
        "                  qas['source'] = url\n",
        "                  qadict[qas['question']] = qas\n",
        "\n",
        "              except:\n",
        "                1 == 1\n",
        "            \n",
        "\n",
        "              break\n",
        "            \n",
        "        else:\n",
        "          break\n",
        "     \n",
        "          \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXCmtHOMRfxC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBYhwYeslI1K"
      },
      "source": [
        "### 1024 length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px6v-bKOTy87"
      },
      "source": [
        "uniqdict = {}\n",
        "qadict_1024 = {}\n",
        "\n",
        "#nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
        "lengthlist = [512 , 1024, 2048 , 3072 , 4096]\n",
        "\n",
        "\n",
        "for url in teamurllist[start_idx:end_idx]:\n",
        "  #url = 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc' \n",
        "  # 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'\n",
        "  #'https://pytorch.org/docs/stable/autograd.html'\n",
        "  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n",
        "  #print(url)\n",
        "  try:\n",
        "    html = req.urlopen(url).read()\n",
        "  except:\n",
        "    continue\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  article = soup.find('article')\n",
        "  \n",
        "  if article:\n",
        "\n",
        "    sctns = article.find_all('div', class_=\"section\")\n",
        "\n",
        "    for sctn in sctns:\n",
        "      ptags = sctn.find_all('p')\n",
        "\n",
        "      \n",
        "      ptextlist = []\n",
        "\n",
        "      i = 1\n",
        "      for ptag in ptags:\n",
        "        ptextlist.append([ i , ptag.get_text(strip=True)])\n",
        "        i += 1\n",
        "\n",
        "      #print(ptextlist)\n",
        "\n",
        "      ptextlistlen = len(ptextlist)\n",
        "\n",
        "      ptextlt1024list = []\n",
        "      alltextdone = False\n",
        "\n",
        "      for j in range(40):\n",
        "        ptext = \"\"\n",
        "        if not alltextdone:\n",
        "\n",
        "          for k in range(j,ptextlistlen):\n",
        "          \n",
        "            arry = ptextlist[k]\n",
        "            #print(arry)\n",
        "            if len(ptext) + len(arry[1]) + 1 < 1024:\n",
        "              ptext += arry[1]\n",
        "              ptext += ' '\n",
        "\n",
        "              if arry[0] == ptextlistlen:\n",
        "\n",
        "                #print('len 1',len(ptext))\n",
        "                try:\n",
        "                  listofdict = nlp(ptext)\n",
        "\n",
        "                  for qas in listofdict:\n",
        "                    qas['context'] = ptext\n",
        "                    qas['source'] = url\n",
        "                    qadict_1024[qas['question']] = qas\n",
        "\n",
        "                except:\n",
        "                  1 == 1\n",
        "\n",
        "                try:\n",
        "                  listofdict = nlp_mtask(ptext)\n",
        "\n",
        "                  for qas in listofdict:\n",
        "                    qas['context'] = ptext\n",
        "                    qas['source'] = url\n",
        "                    qadict[qas['question']] = qas\n",
        "\n",
        "                except:\n",
        "                  1 == 1\n",
        "\n",
        "              \n",
        "                alltextdone = True\n",
        "                break\n",
        "          \n",
        "            else:\n",
        "              \n",
        "              #print('len 2', '*'*40 , ptext , '*'*40, len(ptext))\n",
        "              try:\n",
        "                listofdict = nlp(ptext)\n",
        "\n",
        "                for qas in listofdict:\n",
        "                  #print(qas)\n",
        "                  qas['context'] = ptext\n",
        "                  qas['source'] = url\n",
        "\n",
        "                  qadict_1024[qas['question']] = qas\n",
        "              except:\n",
        "                1 == 1\n",
        "              \n",
        "              try:\n",
        "                listofdict = nlp_mtask(ptext)\n",
        "\n",
        "                for qas in listofdict:\n",
        "                  qas['context'] = ptext\n",
        "                  qas['source'] = url\n",
        "                  qadict[qas['question']] = qas\n",
        "\n",
        "              except:\n",
        "                1 == 1\n",
        "\n",
        "            \n",
        "              break\n",
        "            \n",
        "        else:\n",
        "          break\n",
        "     \n",
        "          \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IklVYjH3lRR6"
      },
      "source": [
        "### 2048 length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8lkv29QgHPv"
      },
      "source": [
        "uniqdict = {}\n",
        "qadict_2048 = {}\n",
        "\n",
        "#nlp = pipeline(\"question-generation\", model=\"valhalla/t5-base-qg-hl\")\n",
        "lengthlist = [512 , 1024, 2048 , 3072 , 4096]\n",
        "\n",
        "\n",
        "for url in teamurllist[start_idx:end_idx]:\n",
        "  #url = 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc' \n",
        "  # 'https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc'\n",
        "  #'https://pytorch.org/docs/stable/autograd.html'\n",
        "  # 'https://pytorch.org/docs/stable/generated/torch.load.html#torch.load'\n",
        "  #print(url)\n",
        "  try:\n",
        "    html = req.urlopen(url).read()\n",
        "  except:\n",
        "    continue\n",
        "  soup = BeautifulSoup(html)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  article = soup.find('article')\n",
        "  \n",
        "  if article:\n",
        "\n",
        "    sctns = article.find_all('div', class_=\"section\")\n",
        "\n",
        "    for sctn in sctns:\n",
        "      ptags = sctn.find_all('p')\n",
        "\n",
        "      \n",
        "      ptextlist = []\n",
        "\n",
        "      i = 1\n",
        "      for ptag in ptags:\n",
        "        ptextlist.append([ i , ptag.get_text(strip=True)])\n",
        "        i += 1\n",
        "\n",
        "      #print(ptextlist)\n",
        "\n",
        "      ptextlistlen = len(ptextlist)\n",
        "\n",
        "      ptextlt1024list = []\n",
        "      alltextdone = False\n",
        "\n",
        "      for j in range(40):\n",
        "        ptext = \"\"\n",
        "        if not alltextdone:\n",
        "\n",
        "          for k in range(j,ptextlistlen):\n",
        "          \n",
        "            arry = ptextlist[k]\n",
        "            #print(arry)\n",
        "            if len(ptext) + len(arry[1]) + 1 < 2*1024:\n",
        "              ptext += arry[1]\n",
        "              ptext += ' '\n",
        "\n",
        "              if arry[0] == ptextlistlen:\n",
        "\n",
        "                #print('len 1',len(ptext))\n",
        "                try:\n",
        "                  listofdict = nlp(ptext)\n",
        "\n",
        "                  for qas in listofdict:\n",
        "                    qas['context'] = ptext\n",
        "                    qas['source'] = url\n",
        "                    qadict_2048[qas['question']] = qas\n",
        "\n",
        "                except:\n",
        "                  1 == 1\n",
        "                \n",
        "                try:\n",
        "                  listofdict = nlp_mtask(ptext)\n",
        "\n",
        "                  for qas in listofdict:\n",
        "                    qas['context'] = ptext\n",
        "                    qas['source'] = url\n",
        "                    qadict[qas['question']] = qas\n",
        "\n",
        "                except:\n",
        "                  1 == 1\n",
        "\n",
        "              \n",
        "                alltextdone = True\n",
        "                break\n",
        "          \n",
        "            else:\n",
        "              \n",
        "              #print('len 2', '*'*40 , ptext , '*'*40, len(ptext))\n",
        "              try:\n",
        "                listofdict = nlp(ptext)\n",
        "\n",
        "                for qas in listofdict:\n",
        "                  #print(qas)\n",
        "                  qas['context'] = ptext\n",
        "                  qas['source'] = url\n",
        "\n",
        "                  qadict_2048[qas['question']] = qas\n",
        "              except:\n",
        "                1 == 1\n",
        "              try:\n",
        "                listofdict = nlp_mtask(ptext)\n",
        "\n",
        "                for qas in listofdict:\n",
        "                  qas['context'] = ptext\n",
        "                  qas['source'] = url\n",
        "                  qadict[qas['question']] = qas\n",
        "\n",
        "              except:\n",
        "                1 == 1\n",
        "              \n",
        "            \n",
        "              break\n",
        "            \n",
        "        else:\n",
        "          break\n",
        "     \n",
        "          \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ROlveBJRxgO",
        "outputId": "8c7eb696-ee0a-4ad7-ef9a-1903773a97a0"
      },
      "source": [
        "\n",
        "k = 0\n",
        "for v in qadict.values():\n",
        "  print( k , '*'*40,'\\n question :-' , v['question'] ,'\\n answer:-' , v['answer'] ,'\\n context:-' , v['context'] ,'\\n source:-' , v['source'])\n",
        "  k += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 **************************************** \n",
            " question :- What does the behavior depend on the dimensionality of the tensors as follows? \n",
            " answer:- Matrix product of two tensors \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "1 **************************************** \n",
            " question :- What is returned if both tensors are 1-dimensional? \n",
            " answer:- the dot product \n",
            " context:- If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "2 **************************************** \n",
            " question :- What is returned if both arguments are 2-dimensional? \n",
            " answer:- matrix-matrix product \n",
            " context:- If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "3 **************************************** \n",
            " question :- What is the dimension of the first argument? \n",
            " answer:- 1 \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "4 **************************************** \n",
            " question :- What happens after the matrix multiply? \n",
            " answer:- the prepended dimension is removed \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "5 **************************************** \n",
            " question :- What product of two tensors is returned? \n",
            " answer:- Matrix product \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "6 **************************************** \n",
            " question :- What product is returned if both arguments are 2-dimensional? \n",
            " answer:- matrix-matrix \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "7 **************************************** \n",
            " question :- What is the dimensionality of the first argument? \n",
            " answer:- 1 \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "8 **************************************** \n",
            " question :- What is removed after the matrix multiply? \n",
            " answer:- the prepended dimension \n",
            " context:- If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "9 **************************************** \n",
            " question :- When is the prepended dimension removed? \n",
            " answer:- After the matrix multiply \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "10 **************************************** \n",
            " question :- If the first argument is 1-dimensional and the second argument is 2-dimensional, what is prepended to its dimension for the purpose of the matrix multiply? \n",
            " answer:- a 1 \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "11 **************************************** \n",
            " question :- What is returned if the first argument is 2-dimensional and the second argument is 1-dimensional? \n",
            " answer:- matrix-vector product \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "12 **************************************** \n",
            " question :- What product is returned if the first argument is 2-dimensional and the second argument is 1-dimensional? \n",
            " answer:- matrix-vector \n",
            " context:- If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "13 **************************************** \n",
            " question :- If the first argument is 2-dimensional and the second is 2-dimensional, how many dimensions does the matrix-matrix product return? \n",
            " answer:- 1 \n",
            " context:- If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "14 **************************************** \n",
            " question :- What happens to the prepended dimension after the matrix multiply? \n",
            " answer:- removed \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "15 **************************************** \n",
            " question :- The matrix-vector product is returned if the first argument is what? \n",
            " answer:- 2-dimensional \n",
            " context:- If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "16 **************************************** \n",
            " question :- If the first argument is 2-dimensional and the second argument is what? \n",
            " answer:- 1-dimensional \n",
            " context:- If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "17 **************************************** \n",
            " question :- What does the broadcasting logic only look at when determining if the inputs are broadcastable? \n",
            " answer:- the batch dimensions \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "18 **************************************** \n",
            " question :- What are the final two dimensions? \n",
            " answer:- matrix dimensions \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "19 **************************************** \n",
            " question :- What does this operator support? \n",
            " answer:- TensorFloat32 \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "20 **************************************** \n",
            " question :- What are different in the matrix dimensions? \n",
            " answer:- final two dimensions \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "21 **************************************** \n",
            " question :- What dot product version of this function does not support anoutparameter? \n",
            " answer:- 1-dimensional \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "22 **************************************** \n",
            " question :- What is the first tensor to be multiplied other(Tensor)? \n",
            " answer:- input(Tensor) \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "23 **************************************** \n",
            " question :- What is the output tensor? \n",
            " answer:- Example: \n",
            " context:- This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "24 **************************************** \n",
            " question :- What is the name of the plant? \n",
            " answer:- Alias fortorch.acos \n",
            " context:- Alias fortorch.acos().  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos\n",
            "25 **************************************** \n",
            " question :- What is another name for Alias fortorch.acos? \n",
            " answer:- Alias fortorch.acos \n",
            " context:- Alias fortorch.acos().  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos\n",
            "26 **************************************** \n",
            " question :- What does inputalong the givendim do? \n",
            " answer:- Counts the number of non-zero values in the tensor \n",
            " context:- Counts the number of non-zero values in the tensorinputalong the givendim.\n",
            "If no dim is specified then all non-zeros in the tensor are counted. input(Tensor)  the input tensor. dim(intortuple of python:ints,optional)  Dim or tuple of dims along which to count non-zeros. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n",
            "27 **************************************** \n",
            " question :- What is counted if no dim is specified? \n",
            " answer:- all non-zeros \n",
            " context:- Counts the number of non-zero values in the tensorinputalong the givendim.\n",
            "If no dim is specified then all non-zeros in the tensor are counted. input(Tensor)  the input tensor. dim(intortuple of python:ints,optional)  Dim or tuple of dims along which to count non-zeros. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n",
            "28 **************************************** \n",
            " question :- What is the input tensor? \n",
            " answer:- input(Tensor) \n",
            " context:- Counts the number of non-zero values in the tensorinputalong the givendim.\n",
            "If no dim is specified then all non-zeros in the tensor are counted. input(Tensor)  the input tensor. dim(intortuple of python:ints,optional)  Dim or tuple of dims along which to count non-zeros. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n",
            "29 **************************************** \n",
            " question :- Intortuple of what is optional? \n",
            " answer:- python:ints \n",
            " context:- Counts the number of non-zero values in the tensorinputalong the givendim.\n",
            "If no dim is specified then all non-zeros in the tensor are counted. input(Tensor)  the input tensor. dim(intortuple of python:ints,optional)  Dim or tuple of dims along which to count non-zeros. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n",
            "30 **************************************** \n",
            " question :- What is an example of a tuple of dims along which to count non-zeros? \n",
            " answer:- Example: \n",
            " context:- Counts the number of non-zero values in the tensorinputalong the givendim.\n",
            "If no dim is specified then all non-zeros in the tensor are counted. input(Tensor)  the input tensor. dim(intortuple of python:ints,optional)  Dim or tuple of dims along which to count non-zeros. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero\n",
            "31 **************************************** \n",
            " question :- What is the implementation based on? \n",
            " answer:- Algorithm 5.1 \n",
            " context:- The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n)  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "32 **************************************** \n",
            " question :- To obtain repeatable results, reset the seed for what? \n",
            " answer:- pseudorandom number generator \n",
            " context:- To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "33 **************************************** \n",
            " question :- How much higher performance characteristics do full-rank SVD implementations have? \n",
            " answer:- 10-fold \n",
            " context:- In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "34 **************************************** \n",
            " question :- What cannot handle huge sparse matrices? \n",
            " answer:- thattorch.linalg.svd() \n",
            " context:- Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "35 **************************************** \n",
            " question :- When was the Algorithm 5.1 implemented? \n",
            " answer:- 2009 \n",
            " context:- The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n)  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "36 **************************************** \n",
            " question :- What is the input assumed to be? \n",
            " answer:- low-rank matrix \n",
            " context:- The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "37 **************************************** \n",
            " question :- Why should you use the full-rank SVD implementationtorch.linalg.svd() for dense matrices? \n",
            " answer:- 10-fold higher performance characteristics \n",
            " context:- Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "38 **************************************** \n",
            " question :- What is the low-rank SVD useful for? \n",
            " answer:- huge sparse matrices \n",
            " context:- In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "39 **************************************** \n",
            " question :- What can the low-rank SVD be useful for large sparse matrices? \n",
            " answer:- thattorch.linalg.svd() \n",
            " context:- In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "40 **************************************** \n",
            " question :- What is the input tensor of size(,m,n)(*,m,n)(*,m,n) \n",
            " answer:- A (Tensor) \n",
            " context:- The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n)  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "41 **************************************** \n",
            " question :- What is the input of A (Tensor)? \n",
            " answer:- tensor \n",
            " context:- The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n)  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "42 **************************************** \n",
            " question :- What is the input tensor of size(,m,n)? \n",
            " answer:- A \n",
            " context:- To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "43 **************************************** \n",
            " question :- Why should you use full-rank SVD implementationtorch.linalg.svd() for dense matrices? \n",
            " answer:- 10-fold higher performance characteristics \n",
            " context:- To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "44 **************************************** \n",
            " question :- What is the tensor of size(,m,n)(*, m, n)(,m,n \n",
            " answer:- q \n",
            " context:- To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "45 **************************************** \n",
            " question :- What must be a nonnegative integer? \n",
            " answer:- niter \n",
            " context:- In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "46 **************************************** \n",
            " question :- What type of matrices can low-rank SVD be useful for? \n",
            " answer:- huge sparse matrices \n",
            " context:- The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "47 **************************************** \n",
            " question :- A (Tensor) is the input tensor of size(,m,n)(*, m, n \n",
            " answer:- q \n",
            " context:- In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "48 **************************************** \n",
            " question :- How much higher performance characteristics do full-rank SVD implementations of dense matrices have? \n",
            " answer:- 10-fold \n",
            " context:- The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "49 **************************************** \n",
            " question :- Why should you use full-rank SVD for dense matrices? \n",
            " answer:- 10-fold higher performance characteristics \n",
            " context:- In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "50 **************************************** \n",
            " question :- What is a slightly overestimated rank of A. conduct? \n",
            " answer:- q \n",
            " context:- A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "51 **************************************** \n",
            " question :- What is the name of the book that Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp published in 2009? \n",
            " answer:- arXiv:0909.4061 \n",
            " context:- A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "52 **************************************** \n",
            " question :- What is defined by expanding theiiithinput over dimensions defined by other inputs? \n",
            " answer:- theiiithgrid \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "53 **************************************** \n",
            " question :- What can NNNtensors be? \n",
            " answer:- scalar or 1-dimensional vector \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "54 **************************************** \n",
            " question :- What is tensors(list of Tensor)? \n",
            " answer:- list of scalars or 1 dimensional tensors \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "55 **************************************** \n",
            " question :- What will be treated as tensors of size(1,)(1,)(1,)automatically? \n",
            " answer:- Scalars \n",
            " context:- tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "56 **************************************** \n",
            " question :- How is theiiithgrid defined? \n",
            " answer:- expanding theiiithinput over dimensions defined by other inputs \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "57 **************************************** \n",
            " question :- What are tensors? \n",
            " answer:- list of scalars or 1 dimensional tensors \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "58 **************************************** \n",
            " question :- What is tensors? \n",
            " answer:- list of scalars or 1 dimensional tensors \n",
            " context:- tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "59 **************************************** \n",
            " question :- What is an example of a sequence of Tensors? \n",
            " answer:- seq \n",
            " context:- tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "60 **************************************** \n",
            " question :- What is the first argument of the matrix-matrix product? \n",
            " answer:- 1 \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "61 **************************************** \n",
            " question :- What is returned if both arguments are at least 1-dimensional and at least one argument is N-dimensional? \n",
            " answer:- a batched matrix multiply \n",
            " context:- If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "62 **************************************** \n",
            " question :- If the first argument is 2-dimensional and the second argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched \n",
            " answer:- 1 \n",
            " context:- If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "63 **************************************** \n",
            " question :- If the second argument is what, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after? \n",
            " answer:- 1-dimensional \n",
            " context:- If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "64 **************************************** \n",
            " question :- What type of dimensions are broadcastable? \n",
            " answer:- non-matrix \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "65 **************************************** \n",
            " question :- What is another term for non-matrix dimensions? \n",
            " answer:- batch \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "66 **************************************** \n",
            " question :- What dimension is the first argument in a batched matrix multiply? \n",
            " answer:- 1 \n",
            " context:- If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "67 **************************************** \n",
            " question :- What must the non-matrix dimensions be? \n",
            " answer:- broadcastable \n",
            " context:- If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "68 **************************************** \n",
            " question :- Inputs are valid for broadcasting even though what are different? \n",
            " answer:- final two dimensions \n",
            " context:- Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "69 **************************************** \n",
            " question :- What product is returned if both tensors are 1-dimensional? \n",
            " answer:- Matrix product of two tensors \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "70 **************************************** \n",
            " question :- If both tensors are what dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, \n",
            " answer:- 1-dimensional \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "71 **************************************** \n",
            " question :- If both arguments are 2-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, what product is returned? \n",
            " answer:- matrix-matrix \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "72 **************************************** \n",
            " question :- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of what? \n",
            " answer:- matrix multiply \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "73 **************************************** \n",
            " question :- After the matrix multiply, what happens to the prepended dimension? \n",
            " answer:- the prepended dimension is removed \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "74 **************************************** \n",
            " question :- If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), what is returned? \n",
            " answer:- batched matrix multiply \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "75 **************************************** \n",
            " question :- If the first argument is what dimension, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after? \n",
            " answer:- 1-dimensional \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "76 **************************************** \n",
            " question :- If the second argument is what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after? \n",
            " answer:- 1-dimensional \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "77 **************************************** \n",
            " question :- The non-matrix dimensions are Broadcasted and therefore must be what? \n",
            " answer:- broadcastable \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "78 **************************************** \n",
            " question :- What only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions? \n",
            " answer:- the broadcasting logic \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "79 **************************************** \n",
            " question :- If both tensors are what, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix \n",
            " answer:- 1-dimensional \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "80 **************************************** \n",
            " question :- The matrix-matrix product is returned if the first argument is what dimension? \n",
            " answer:- 1-dimensional \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "81 **************************************** \n",
            " question :- If both arguments are at least 1-dimensional and at least one argument is N-dimensional, what is returned? \n",
            " answer:- batched matrix multiply \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "82 **************************************** \n",
            " question :- What only looks at the batch dimensions when determining if the inputs are broadcastable? \n",
            " answer:- the broadcasting logic \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o190LjvEBW-M",
        "outputId": "f16d1fba-1fbd-40df-899f-9adc4dd439d6"
      },
      "source": [
        "k = 0\n",
        "for v in qadict_1024.values():\n",
        "  print( k , '*'*40,'\\n question :-' , v['question'] ,'\\n answer:-' , v['answer'] ,'\\n context:-' , v['context'] ,'\\n source:-' , v['source'])\n",
        "  k += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 **************************************** \n",
            " question :- What does the behavior depend on the dimensionality of the tensors as follows? \n",
            " answer:- Matrix product of two tensors \n",
            " context:- Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "1 **************************************** \n",
            " question :- What is returned if both tensors are 1-dimensional? \n",
            " answer:- dot product \n",
            " context:- If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "2 **************************************** \n",
            " question :- What is returned if both arguments are 2-dimensional? \n",
            " answer:- matrix-matrix product \n",
            " context:- If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "3 **************************************** \n",
            " question :- What is the dimension of the first argument? \n",
            " answer:- 1 \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "4 **************************************** \n",
            " question :- When is the prepended dimension removed? \n",
            " answer:- After the matrix multiply \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "5 **************************************** \n",
            " question :- What is returned if the first argument is 2-dimensional and the second argument is 1-dimensional? \n",
            " answer:- matrix-vector product \n",
            " context:- If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "6 **************************************** \n",
            " question :- If the first argument is 2-dimensional and the second is 2-dimensional, how many dimensions does the matrix-matrix product return? \n",
            " answer:- 1 \n",
            " context:- If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned.  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "7 **************************************** \n",
            " question :- What is the name of the plant? \n",
            " answer:- Alias fortorch.acos \n",
            " context:- Alias fortorch.acos().  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos\n",
            "8 **************************************** \n",
            " question :- What is the singular value decomposition(U,S,V)of? \n",
            " answer:- a matrix \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "9 **************************************** \n",
            " question :- What is computed for the matrixAMA - MAM? \n",
            " answer:- SVD \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "10 **************************************** \n",
            " question :- What is the implementation based on? \n",
            " answer:- Algorithm 5.1 \n",
            " context:- Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "11 **************************************** \n",
            " question :- To obtain repeatable results, reset the seed for what? \n",
            " answer:- pseudorandom number generator \n",
            " context:- Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "12 **************************************** \n",
            " question :- How much higher performance characteristics do full-rank SVD implementations of dense matrices have? \n",
            " answer:- 10-fold \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "13 **************************************** \n",
            " question :- What can't handle the low-rank SVD? \n",
            " answer:- thattorch.linalg.svd() \n",
            " context:- Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "14 **************************************** \n",
            " question :- What must be a nonnegative integer? \n",
            " answer:- niter \n",
            " context:- Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "15 **************************************** \n",
            " question :- How much higher is the performance of full-rank SVD? \n",
            " answer:- 10-fold \n",
            " context:- Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "16 **************************************** \n",
            " question :- What is the name of the book that Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp published in 2009? \n",
            " answer:- arXiv:0909.4061 \n",
            " context:- Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "17 **************************************** \n",
            " question :- What is defined by expanding theiiithinput over dimensions defined by other inputs? \n",
            " answer:- theiiithgrid \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "18 **************************************** \n",
            " question :- What is tensors(list of Tensor)? \n",
            " answer:- list of scalars or 1 dimensional tensors \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "19 **************************************** \n",
            " question :- What will be treated as tensors of size(1,)(1,)(1,)automatically? \n",
            " answer:- Scalars \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "20 **************************************** \n",
            " question :- What is an example of a sequence of Tensors? \n",
            " answer:- seq \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfoxL1V7gPpE",
        "outputId": "c1672cf5-280a-4466-e87b-c2f321c2090e"
      },
      "source": [
        "\n",
        "k = 0\n",
        "for v in qadict_2048.values():\n",
        "  print( k , '*'*40,'\\n question :-' , v['question'] ,'\\n answer:-' , v['answer'] ,'\\n context:-' , v['context'] ,'\\n source:-' , v['source'])\n",
        "  k += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 **************************************** \n",
            " question :- What happens if both tensors are 1-dimensional? \n",
            " answer:- the dot product (scalar) is returned \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "1 **************************************** \n",
            " question :- What is returned if both arguments are 2-dimensional? \n",
            " answer:- matrix-matrix product \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "2 **************************************** \n",
            " question :- If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of what? \n",
            " answer:- matrix multiply \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "3 **************************************** \n",
            " question :- If the first argument is 1-dimensional and the second argument is 2-dimensional, the matrix-vector product is returned. If both arguments are at least 1- \n",
            " answer:- matrix multiply \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "4 **************************************** \n",
            " question :- What is returned if the first argument is 2-dimensional and the second argument is 1-dimensional? \n",
            " answer:- matrix-vector product \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "5 **************************************** \n",
            " question :- If both arguments are at least 1-dimensional and at least one argument is N-dimensional, what is returned? \n",
            " answer:- batched matrix multiply \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "6 **************************************** \n",
            " question :- If the first argument is how many dimensional, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after \n",
            " answer:- 1 \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "7 **************************************** \n",
            " question :- If the second argument is 1-dimensional, a 1 is appended to its dimension for the purpose of what? \n",
            " answer:- batched matrix multiple \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "8 **************************************** \n",
            " question :- What type of dimensions are broadcastable? \n",
            " answer:- non-matrix \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "9 **************************************** \n",
            " question :- The non-matrix dimensions arebroadcasted and therefore must be what? \n",
            " answer:- broadcastable \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "10 **************************************** \n",
            " question :- What does the broadcasting logic only look at when determining if the inputs are broadcastable? \n",
            " answer:- the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions \n",
            " context:- The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
            "a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
            "After the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
            "the matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\n",
            "N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
            "argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
            "batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
            "1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
            "The non-matrix (i.e. batch) dimensions arebroadcasted(and thus\n",
            "must be broadcastable).  For example, ifinputis a(j1nn)(j \\times 1 \\times n \\times n)(j1nn)tensor andotheris a(knn)(k \\times n \\times n)(knn)tensor,outwill be a(jknn)(j \\times k \\times n \\times n)(jknn)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
            "are broadcastable, and not the matrix dimensions. For example, ifinputis a(j1nm)(j \\times 1 \\times n \\times m)(j1nm)tensor andotheris a(kmp)(k \\times m \\times p)(kmp)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
            "matrix dimensions) are different.outwill be a(jknp)(j \\times k \\times n \\times p)(jknp)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor)  the first tensor to be multiplied other(Tensor)  the second tensor to be multiplied out(Tensor,optional)  the output tensor. Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul\n",
            "11 **************************************** \n",
            " question :- What is the name of the plant? \n",
            " answer:- Alias fortorch.acos \n",
            " context:- Alias fortorch.acos().  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos\n",
            "12 **************************************** \n",
            " question :- What is the singular value decomposition(U,S,V)of? \n",
            " answer:- a matrix \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "13 **************************************** \n",
            " question :- What is computed for the matrixAMA - MAM? \n",
            " answer:- SVD \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "14 **************************************** \n",
            " question :- What is the implementation based on? \n",
            " answer:- Algorithm 5.1 \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "15 **************************************** \n",
            " question :- To obtain repeatable results, reset the seed for what? \n",
            " answer:- pseudorandom number generator \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "16 **************************************** \n",
            " question :- How much higher is the performance of full-rank SVD for dense matrices? \n",
            " answer:- 10-fold \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "17 **************************************** \n",
            " question :- What can't handle the low-rank SVD? \n",
            " answer:- thattorch.linalg.svd() \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "18 **************************************** \n",
            " question :- What must be a nonnegative integer? \n",
            " answer:- niter \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "19 **************************************** \n",
            " question :- Where is finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions? \n",
            " answer:- arXiv:0909.4061 \n",
            " context:- Return the singular value decomposition(U,S,V)of a matrix,\n",
            "batches of matrices, or a sparse matrixAAAsuch thatAUdiag(S)VTA \\approx U diag(S) V^TAUdiag(S)VT. In caseMMMis given, then\n",
            "SVD is computed for the matrixAMA - MAM. Note The implementation is based on the Algorithm 5.1 from\n",
            "Halko et al, 2009. Note To obtain repeatable results, reset the seed for the\n",
            "pseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\n",
            "higher performance characteristics. The low-rank SVD\n",
            "will be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(,m,n)(*, m, n)(,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\n",
            "integer, and defaults to 2 (,1,n)(*, 1, n)(,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n",
            "structure with randomness: probabilistic algorithms for\n",
            "constructing approximate matrix decompositions,\n",
            "arXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv).  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank\n",
            "20 **************************************** \n",
            " question :- What is defined by expanding theiiithinput over dimensions defined by other inputs? \n",
            " answer:- theiiithgrid \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "21 **************************************** \n",
            " question :- What is tensors(list of Tensor)? \n",
            " answer:- list of scalars or 1 dimensional tensors \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "22 **************************************** \n",
            " question :- What will be treated as tensors of size(1,)(1,)(1,)automatically? \n",
            " answer:- Scalars \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n",
            "23 **************************************** \n",
            " question :- What is an example of a sequence of Tensors? \n",
            " answer:- seq \n",
            " context:- TakeNNNtensors, each of which can be either scalar or 1-dimensional\n",
            "vector, and createNNNN-dimensional grids, where theiiithgrid is defined by\n",
            "expanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor)  list of scalars or 1 dimensional tensors. Scalars will be\n",
            "treated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1,),(N2,),,(Nk,), then the output would also havekkktensors,\n",
            "where all tensors are of size(N1,N2,,Nk)(N_1, N_2, \\ldots , N_k)(N1,N2,,Nk). seq (sequence of Tensors) Example:  \n",
            " source:- https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H19RaUCYR_61",
        "outputId": "907c6667-254b-4a7d-d343-7a09cdbbee7e"
      },
      "source": [
        "combkeys = list(qadict.keys()) + list(set(qadict.keys()) - set(qadict_1024.keys()))\n",
        "\n",
        "combkeys123 = combkeys + list(set(combkeys) - set(qadict_2048.keys()))\n",
        "\n",
        "len(combkeys123) , len(combkeys) , len(qadict.keys()), len(qadict_1024.keys()) , len(qadict_2048.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(218, 149, 83, 21, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rO4SDg-cftKf",
        "outputId": "5f16f913-b4da-40fd-eb75-ab3e25fce90a"
      },
      "source": [
        "onelist = {}\n",
        "for key in combkeys123:\n",
        "  if key in qadict_2048:\n",
        "    onelist[key] = qadict_2048[key]\n",
        "  elif key in qadict_1024:\n",
        "    onelist[key] = qadict_1024[key]\n",
        "  elif key in qadict:\n",
        "    onelist[key] = qadict[key]\n",
        "\n",
        "len(onelist)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKPEJm-CixRX"
      },
      "source": [
        "\n",
        "with open('/content/onlytextpytorch.json', 'w') as file:\n",
        "  file.write(json.dumps(onelist, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egRuC8QFUy0v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uiG6_NQVCIz"
      },
      "source": [
        "## Multitask QA-QG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_PKMG28VhxM"
      },
      "source": [
        "### small-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAkVmsH9VEIu"
      },
      "source": [
        "nlp2 = pipeline(\"multitask-qa-qg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dRiLecTVk8E"
      },
      "source": [
        "#### QG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKA65C51VLGu"
      },
      "source": [
        "nlp(text)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAAxbcwzVXlV"
      },
      "source": [
        "nlp2(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a81EN_WWVpae"
      },
      "source": [
        "nlp2(text4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9c2CkhhVsxs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuOL3X_XV28R"
      },
      "source": [
        "#### QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe-v3I8aV4En"
      },
      "source": [
        "nlp2({\n",
        "  \"question\": \"Who created Python ?\",\n",
        "  \"context\": text\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZchZFXPuWI62"
      },
      "source": [
        "nlp2({\n",
        "    \"question\": \"Who wrote Forrest Gump ?\",\n",
        "     \"context\": text4\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARBRwBj5WVga"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEk-EU9UWaBr"
      },
      "source": [
        "### base-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx1KjJzaWa-a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL5Cgm2XWikl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwCA0sjtWldK"
      },
      "source": [
        "#### QG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xwby-LJpWldj"
      },
      "source": [
        "nlp2(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YIZtVu8Wldw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh46cnicWld1"
      },
      "source": [
        "#### QA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ew7vRA2Wld6"
      },
      "source": [
        "\"\"\"nlp({\n",
        "    \"question\": \"Who wrote Forrest Gump ?\",\n",
        "     \"context\": text4\n",
        "})\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CkkYm6aWld-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Wq-2nuXDvl"
      },
      "source": [
        "## End-to-End QG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xS4XfQh0X3W8"
      },
      "source": [
        "### small model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzJjXzpmXG7t"
      },
      "source": [
        "nlp3 = pipeline(\"e2e-qg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd_2-fvYXaXb"
      },
      "source": [
        "nlp3(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDo01sKdXjfG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNBlsV_eX9mm"
      },
      "source": [
        "### base-model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5oUJ5_0X_Je"
      },
      "source": [
        "nlp4 = pipeline(\"e2e-qg\", model=\"valhalla/t5-base-e2e-qg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42YUWV1Dnhfb"
      },
      "source": [
        "s "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWzd3JFQYI2O"
      },
      "source": [
        "nlp4(text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMY7ZLdYYUkL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}