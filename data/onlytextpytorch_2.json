{
    "InferenceMode is an analogous to what?": {
        "answer": "tono_grad",
        "question": "InferenceMode is an analogous to what?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "Code run under InferenceMode gets better performance by disabling what?": {
        "answer": "view tracking and version counter bumps",
        "question": "Code run under InferenceMode gets better performance by disabling what?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What type of context manager is InferenceMode?": {
        "answer": "thread local",
        "question": "What type of context manager is InferenceMode?",
        "context": "Context-manager that enables or disables inference mode InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "InferenceMode functions as what?": {
        "answer": "decorator",
        "question": "InferenceMode functions as what?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What do you make sure to do with InferenceMode?": {
        "answer": "instantiate with parenthesis",
        "question": "What do you make sure to do with InferenceMode?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What does InferenceMode do?": {
        "answer": "Note",
        "question": "What does InferenceMode do?",
        "context": "InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What is the context manager?": {
        "answer": "thread local",
        "question": "What is the context manager?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What does the context manager function as?": {
        "answer": "decorator",
        "question": "What does the context manager function as?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What does the context manager make sure to do?": {
        "answer": "instantiate with parenthesis",
        "question": "What does the context manager make sure to do?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What is one of several mechanisms that can enable or disable gradients locally?": {
        "answer": "Note Inference mode",
        "question": "What is one of several mechanisms that can enable or disable gradients locally?",
        "context": "Context-manager that enables or disables inference mode InferenceMode is a new context manager analogous tono_gradto be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What does mode(bool) do?": {
        "answer": "Flag whether to enable or disable inference mode",
        "question": "What does mode(bool) do?",
        "context": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode ",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    "What rule is used to estimate ydxint y,dxydxalongdim?": {
        "answer": "trapezoid rule",
        "question": "What rule is used to estimate ydxint y,dxydxalongdim?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What is the value of the function to integrate?": {
        "answer": "x(Tensor)",
        "question": "What is the value of the function to integrate?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "Ifxis not in ascending order, intervals on which it is decreasing contribute negatively to the estimated integral?": {
        "answer": "Ifxis not in ascending order",
        "question": "Ifxis not in ascending order, intervals on which it is decreasing contribute negatively to the estimated integral?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What is the dimension along which to integrate?": {
        "answer": "dim(int)",
        "question": "What is the dimension along which to integrate?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "By default, what is the dimension along which to integrate?": {
        "answer": "the last dimension",
        "question": "By default, what is the dimension along which to integrate?",
        "context": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "What is the point at which the functionyis sampled?": {
        "answer": "x(Tensor)",
        "question": "What is the point at which the functionyis sampled?",
        "context": "y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "If the functionyis not in ascending order, what contributes negatively to the estimated integral?": {
        "answer": "Ifxis not in ascending order",
        "question": "If the functionyis not in ascending order, what contributes negatively to the estimated integral?",
        "context": "y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    "If inputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should": {
        "answer": "integer",
        "question": "If inputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the number to be multiplied to each element of input out(Tensor,optional)?": {
        "answer": "output tensor",
        "question": "What is the number to be multiplied to each element of input out(Tensor,optional)?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "Each element of the tensorinputis multiplied by the corresponding element of what?": {
        "answer": "Tensorother",
        "question": "Each element of the tensorinputis multiplied by the corresponding element of what?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What happens to the resulting tensor?": {
        "answer": "returned",
        "question": "What happens to the resulting tensor?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "The shapes of inputandothermust be what?": {
        "answer": "bebroadcastable",
        "question": "The shapes of inputandothermust be what?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the first multiplicand tensor?": {
        "answer": "input(Tensor)",
        "question": "What is the first multiplicand tensor?",
        "context": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    "What is the name of the file?": {
        "answer": "Alias fortorch.ge()",
        "question": "What is the name of the file?",
        "context": "Alias fortorch.ge(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal"
    },
    "What is returned for each row of the inputtensor in the given dimensiondim?": {
        "answer": "the log of summed exponentials",
        "question": "What is returned for each row of the inputtensor in the given dimensiondim?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "How is the computation handled?": {
        "answer": "numerically stabilized",
        "question": "How is the computation handled?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "What is the result for summation indexjjjgiven bydimand other indicesiii?": {
        "answer": "IfkeepdimisTrue",
        "question": "What is the result for summation indexjjjgiven bydimand other indicesiii?",
        "context": "For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "If keepdimisTrue, the output tensor has how many fewer dimension(s)?": {
        "answer": "1",
        "question": "If keepdimisTrue, the output tensor has how many fewer dimension(s)?",
        "context": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "What is the name of the input tensor?": {
        "answer": "input(Tensor)",
        "question": "What is the name of the input tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "If the output tensor is of the same size as input, how many dimension(s) does it have?": {
        "answer": "1",
        "question": "If the output tensor is of the same size as input, how many dimension(s) does it have?",
        "context": "For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    "What is the name of the dimension or dimensions to reduce?": {
        "answer": "dim",
        "question": "What is the name of the dimension or dimensions to reduce?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What determines whether the output tensor hasdimretained or not?": {
        "answer": "keepdim(bool)",
        "question": "What determines whether the output tensor hasdimretained or not?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "Returns the maximum value of all elements in what?": {
        "answer": "theinputtensor",
        "question": "Returns the maximum value of all elements in what?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is input(Tensor)?": {
        "answer": "input tensor",
        "question": "What is input(Tensor)?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the maximum value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the maximum value of each row of the inputtensor in the given dimensiondim?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the index location of each maximum value found?": {
        "answer": "Andindicesis the index location of each maximum value found",
        "question": "What is the index location of each maximum value found?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "When are output tensors of the same size as input?": {
        "answer": "IfkeepdimisTrue",
        "question": "When are output tensors of the same size as input?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "If keepdimisTrue, the output tensors have what?": {
        "answer": "1 fewer dimension thaninput",
        "question": "If keepdimisTrue, the output tensors have what?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the value of the output tensors if keepdimisTrue?": {
        "answer": "Note",
        "question": "What is the value of the output tensors if keepdimisTrue?",
        "context": "Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "If the output tensors are of the same size asinput, what is the default?": {
        "answer": "IfkeepdimisTrue",
        "question": "If the output tensors are of the same size asinput, what is the default?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What returns the maximum value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What returns the maximum value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "If output tensors are of the same size as input, what is the default?": {
        "answer": "IfkeepdimisTrue",
        "question": "If output tensors are of the same size as input, what is the default?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the value of the maximum value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "Note",
        "question": "What is the value of the maximum value of each row of the inputtensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "If keepdimisTrue, the output tensors are of what size?": {
        "answer": "size 1",
        "question": "If keepdimisTrue, the output tensors are of what size?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What happens if the output tensors are squeezed?": {
        "answer": "1 fewer dimension thaninput",
        "question": "What happens if the output tensors are squeezed?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What happens if there are multiple maximal values in a reduced row?": {
        "answer": "the indices of the first maximal value are returned",
        "question": "What happens if there are multiple maximal values in a reduced row?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the dimension to reduce?": {
        "answer": "dim(int)",
        "question": "What is the dimension to reduce?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What default value indicates that the output tensor hasdimretained?": {
        "answer": "False",
        "question": "What default value indicates that the output tensor hasdimretained?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What are returned if there are multiple maximal values in a reduced row?": {
        "answer": "indices of the first maximal value",
        "question": "What are returned if there are multiple maximal values in a reduced row?",
        "context": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the input tensor?": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is the name of the function that determines whether the output tensor hasdimretained or not?": {
        "answer": "keepdim",
        "question": "What is the name of the function that determines whether the output tensor hasdimretained or not?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the default value of the output tensor hasdimretained?": {
        "answer": "False",
        "question": "What is the default value of the output tensor hasdimretained?",
        "context": "Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is out(tuple,optional)?": {
        "answer": "the result tuple of two output tensors",
        "question": "What is out(tuple,optional)?",
        "context": "Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    "What is the name of the file that loads the Torch serialized object at the given URL?": {
        "answer": "Moved totorch.hub",
        "question": "What is the name of the file that loads the Torch serialized object at the given URL?",
        "context": "Moved totorch.hub. Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What does move totorch.hub do?": {
        "answer": "Loads the Torch serialized object at the given URL",
        "question": "What does move totorch.hub do?",
        "context": "Moved totorch.hub. Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If downloaded file is a zip file, it will be automatically what?": {
        "answer": "decompressed",
        "question": "If downloaded file is a zip file, it will be automatically what?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If the object is already present in what, it's deserialized and returned?": {
        "answer": "model_dir",
        "question": "If the object is already present in what, it's deserialized and returned?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "How is the default value of model_diris returned?": {
        "answer": "byget_dir()",
        "question": "How is the default value of model_diris returned?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the URL of the object to download model_dir?": {
        "answer": "URL",
        "question": "What is the URL of the object to download model_dir?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What does the Torch serialized object do?": {
        "answer": "Loads the Torch serialized object at the given URL",
        "question": "What does the Torch serialized object do?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If downloaded file is a zip file, what happens to it?": {
        "answer": "automatically decompressed",
        "question": "If downloaded file is a zip file, what happens to it?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What happens if the object is already present inmodel_dir?": {
        "answer": "it\u2019s deserialized and returned",
        "question": "What happens if the object is already present inmodel_dir?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "If the object is already present inmodel_dir, it's deserialized and returned.": {
        "answer": "the object is already present inmodel_dir",
        "question": "If the object is already present inmodel_dir, it's deserialized and returned.",
        "context": "If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "How is the default value ofmodel_diris returned?": {
        "answer": "byget_dir()",
        "question": "How is the default value ofmodel_diris returned?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What can map_location be used to specify how to remap storage locations?": {
        "answer": "a function or a dict",
        "question": "What can map_location be used to specify how to remap storage locations?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "When is the object deserialized and returned?": {
        "answer": "If the object is already present inmodel_dir",
        "question": "When is the object deserialized and returned?",
        "context": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is map_location?": {
        "answer": "a function or a dict",
        "question": "What is map_location?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default value of model_dirishub_dir>/checkpointswherehub_diris?": {
        "answer": "True",
        "question": "What is the default value of model_dirishub_dir>/checkpointswherehub_diris?",
        "context": "If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What can be used to specify how to remap storage locations?": {
        "answer": "a function or a dict",
        "question": "What can be used to specify how to remap storage locations?",
        "context": "url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What does progress(bool,optional) display to stderr?": {
        "answer": "a progress bar",
        "question": "What does progress(bool,optional) display to stderr?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "Progress(bool,optional) \u2013 whether or not to display a progress bar to what?": {
        "answer": "stderr",
        "question": "Progress(bool,optional) \u2013 whether or not to display a progress bar to what?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default setting to display a progress bar to stderr?": {
        "answer": "True",
        "question": "What is the default setting to display a progress bar to stderr?",
        "context": "url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What default value is used to display a progress bar to stderr?": {
        "answer": "True",
        "question": "What default value is used to display a progress bar to stderr?",
        "context": "model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is a function or dict specifying how to remap storage locations?": {
        "answer": "map_location",
        "question": "What is a function or dict specifying how to remap storage locations?",
        "context": "map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default value of check_hash?": {
        "answer": "False",
        "question": "What is the default value of check_hash?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the hash used to ensure?": {
        "answer": "unique names",
        "question": "What is the hash used to ensure?",
        "context": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "Whether or not to display a progress bar to stderr?": {
        "answer": "progress",
        "question": "Whether or not to display a progress bar to stderr?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the hash used for?": {
        "answer": "ensure unique names and to verify the contents of the file",
        "question": "What is the hash used for?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What is the default name for the downloaded file?": {
        "answer": "False file_name(string,optional)",
        "question": "What is the default name for the downloaded file?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "What will be used if not set?": {
        "answer": "fromurl",
        "question": "What will be used if not set?",
        "context": "progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False file_name(string,optional) \u2013 name for the downloaded file. Filename fromurlwill be used if not set. ",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    "Returns the mean value of all elements in what?": {
        "answer": "theinputtensor",
        "question": "Returns the mean value of all elements in what?",
        "context": "Returns the mean value of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What does input(Tensor) return?": {
        "answer": "the mean value of each row of theinputtensor in the given dimensiondim",
        "question": "What does input(Tensor) return?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "Ifdimis a list of dimensions, what do you do over all of them?": {
        "answer": "reduce",
        "question": "Ifdimis a list of dimensions, what do you do over all of them?",
        "context": "Returns the mean value of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "IfkeepdimisTrue, the output tensor is of what size?": {
        "answer": "size 1",
        "question": "IfkeepdimisTrue, the output tensor is of what size?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "How many dimension(s) smaller is the output tensor?": {
        "answer": "1",
        "question": "How many dimension(s) smaller is the output tensor?",
        "context": "Returns the mean value of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What do you do ifdimis a list of dimensions?": {
        "answer": "reduce",
        "question": "What do you do ifdimis a list of dimensions?",
        "context": "Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "How many dimension(s) fewer dimension(s) does the output tensor have?": {
        "answer": "1",
        "question": "How many dimension(s) fewer dimension(s) does the output tensor have?",
        "context": "input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What does the dimensiondim return?": {
        "answer": "the mean value of each row of theinputtensor",
        "question": "What does the dimensiondim return?",
        "context": "Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "The output tensor has how many fewer dimension(s)?": {
        "answer": "1",
        "question": "The output tensor has how many fewer dimension(s)?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What does return in the given dimensiondim?": {
        "answer": "the mean value of each row of theinputtensor",
        "question": "What does return in the given dimensiondim?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What do you do ifdimis a list of dimensions over all of them?": {
        "answer": "reduce",
        "question": "What do you do ifdimis a list of dimensions over all of them?",
        "context": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    "What are the quantiles of each row of the inputtensor along the dimensiondim?": {
        "answer": "q-th",
        "question": "What are the quantiles of each row of the inputtensor along the dimensiondim?",
        "context": "Computes the q-th quantiles of each row of theinputtensor\nalong the dimensiondim. To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "How is the result computed if the quantile lies between two data points?": {
        "answer": "linear interpolation",
        "question": "How is the result computed if the quantile lies between two data points?",
        "context": "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What do we map to compute the quantile?": {
        "answer": "q in [0, 1] to the range of indices [0, n]",
        "question": "What do we map to compute the quantile?",
        "context": "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What is fraction?": {
        "answer": "fractional part of the computed quantile index",
        "question": "What is fraction?",
        "context": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "If the first dimension of the output represents quantiles and has size equal to the size ofq, what is the size of the remaining dimensions?": {
        "answer": "Ifqis a 1D tensor",
        "question": "If the first dimension of the output represents quantiles and has size equal to the size ofq, what is the size of the remaining dimensions?",
        "context": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What resulted in the inputtensor being flattened before computation?": {
        "answer": "defaultdimisNone",
        "question": "What resulted in the inputtensor being flattened before computation?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What is q(floatorTensor)?": {
        "answer": "a scalar or 1D tensor of values in the range",
        "question": "What is q(floatorTensor)?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What does the first dimension of the output represent?": {
        "answer": "the first dimension of the output represents the quantiles",
        "question": "What does the first dimension of the output represent?",
        "context": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    "What does torch.vdot do?": {
        "answer": "Computes the dot product of two 1D tensors",
        "question": "What does torch.vdot do?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What function handles complex numbers differently than dot(a, b)?": {
        "answer": "vdot(a, b)",
        "question": "What function handles complex numbers differently than dot(a, b)?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What is used for the calculation of the dot product if the first argument is complex?": {
        "answer": "complex conjugate",
        "question": "What is used for the calculation of the dot product if the first argument is complex?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What intentionally only supports computing the dot product of two 1D tensors with the same number of elements?": {
        "answer": "torch.vdot",
        "question": "What intentionally only supports computing the dot product of two 1D tensors with the same number of elements?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What is the name of the first tensor in the dot product?": {
        "answer": "input(Tensor)",
        "question": "What is the name of the first tensor in the dot product?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "When is the conjugate of input(Tensor) used?": {
        "answer": "if it\u2019s complex",
        "question": "When is the conjugate of input(Tensor) used?",
        "context": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    "What is the name of the window function?": {
        "answer": "Hann",
        "question": "What is the name of the window function?",
        "context": "Hann window function. whereNNNis the full window size. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is NNN?": {
        "answer": "full window size",
        "question": "What is NNN?",
        "context": "whereNNNis the full window size. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is a positive integer controlling the returned window size?": {
        "answer": "inputwindow_lengthis",
        "question": "What is a positive integer controlling the returned window size?",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is theNNNin above formula in factwindow_length+1textwindow_length + 1window_length+1": {
        "answer": "ifperiodicis true",
        "question": "What is theNNNin above formula in factwindow_length+1textwindow_length + 1window_length+1",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the same as totorch.hann_window(L+1,periodic=False)?": {
        "answer": "havetorch.hann_window",
        "question": "What is the same as totorch.hann_window(L+1,periodic=False)?",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the name of the formula that determines if the returned window is true or false?": {
        "answer": "Note",
        "question": "What is the name of the formula that determines if the returned window is true or false?",
        "context": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the size of the returned window periodic?": {
        "answer": "window_length(int)",
        "question": "What is the size of the returned window periodic?",
        "context": "Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "If True, return a symmetric window.": {
        "answer": "If False",
        "question": "If True, return a symmetric window.",
        "context": "Note Ifwindow_length=1=1=1, the returned window contains a single value 1. window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the desired data type of returned tensor?": {
        "answer": "dtype",
        "question": "What is the desired data type of returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What uses a global default?": {
        "answer": "ifNone",
        "question": "What uses a global default?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What type of windows are supported?": {
        "answer": "floating point types",
        "question": "What type of windows are supported?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What returns a symmetric window?": {
        "answer": "If False",
        "question": "What returns a symmetric window?",
        "context": "periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the size of returned window periodic?": {
        "answer": "window_length",
        "question": "What is the size of returned window periodic?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "If True, return a symmetric window?": {
        "answer": "If False",
        "question": "If True, return a symmetric window?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What type of window types are supported?": {
        "answer": "floating point types",
        "question": "What type of window types are supported?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the desired layout of returned window tensor?": {
        "answer": "layout",
        "question": "What is the desired layout of returned window tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What type of layout is supported?": {
        "answer": "Onlytorch.strided",
        "question": "What type of layout is supported?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What returns a window to be used as periodic function?": {
        "answer": "If True",
        "question": "What returns a window to be used as periodic function?",
        "context": "periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What types are supported only?": {
        "answer": "floating point types",
        "question": "What types are supported only?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is supported for dense layout?": {
        "answer": "Onlytorch.strided",
        "question": "What is supported for dense layout?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the desired device of returned tensor?": {
        "answer": "device(torch.device, optional)",
        "question": "What is the desired device of returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What default uses the current device for the default tensor type?": {
        "answer": "ifNone",
        "question": "What default uses the current device for the default tensor type?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What should record operations on the returned tensor?": {
        "answer": "autograd",
        "question": "What should record operations on the returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is the default value for autograd to record operations on the returned tensor?": {
        "answer": "Default:False",
        "question": "What is the default value for autograd to record operations on the returned tensor?",
        "context": "window_length(int) \u2013 the size of returned window periodic(bool,optional) \u2013 If True, returns a window to be used as periodic\nfunction. If False, return a symmetric window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). Only floating point types are supported. layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "A 1-D tensor of size(window_length,)(textwindow_length,)(window_": {
        "answer": "window Tensor",
        "question": "A 1-D tensor of size(window_length,)(textwindow_length,)(window_",
        "context": "device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 1-D tensor of size(window_length,)(\\text{window\\_length},)(window_length,)containing the window Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    "What is deprecated in favor oftorch.linalg.lstsq()?": {
        "answer": "torch.lstsq()",
        "question": "What is deprecated in favor oftorch.linalg.lstsq()?",
        "context": "torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    "What does the returnedsolutionintorch.lstsq()store the residuals of the solution in?": {
        "answer": "ncolumns",
        "question": "What does the returnedsolutionintorch.lstsq()store the residuals of the solution in?",
        "context": "Warning torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    "The returnedsolutionintorch.lstsq()stores the residuals of the solution in the lastm - what in the": {
        "answer": "ncolumns",
        "question": "The returnedsolutionintorch.lstsq()stores the residuals of the solution in the lastm - what in the",
        "context": "torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    "What happens if the global deterministic flag is turned on?": {
        "answer": "Returns True",
        "question": "What happens if the global deterministic flag is turned on?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "What documentation provides more details about the global deterministic flag?": {
        "answer": "totorch.use_deterministic_algorithms()",
        "question": "What documentation provides more details about the global deterministic flag?",
        "context": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    "Returns a new tensor with what representation if each element isfiniteor not?": {
        "answer": "boolean elements",
        "question": "Returns a new tensor with what representation if each element isfiniteor not?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "What are finite when they are not NaN, negative infinity, or infinity?": {
        "answer": "Real values",
        "question": "What are finite when they are not NaN, negative infinity, or infinity?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "Complex values are what when both their real and imaginary parts are finite?": {
        "answer": "finite",
        "question": "Complex values are what when both their real and imaginary parts are finite?",
        "context": "Returns a new tensor with boolean elements representing if each element isfiniteor not. Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. input(Tensor) \u2013 the input tensor. A boolean tensor that is True whereinputis finite and False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    "A boolean tensor that is what?": {
        "answer": "True",
        "question": "A boolean tensor that is what?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What kind of distribution is the fillsselftensor drawn from?": {
        "answer": "geometric distribution",
        "question": "What kind of distribution is the fillsselftensor drawn from?",
        "context": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    "What is filled with elements drawn from the geometric distribution?": {
        "answer": "Fillsselftensor",
        "question": "What is filled with elements drawn from the geometric distribution?",
        "context": "Fillsselftensor with elements drawn from the geometric distribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    "What kind of distribution is the fillsselftensor?": {
        "answer": "uniform",
        "question": "What kind of distribution is the fillsselftensor?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What returns a new tensor?": {
        "answer": "Flip tensor",
        "question": "What returns a new tensor?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What does torch.flipudis do to return a new tensor?": {
        "answer": "Flip the entries in each column in the up/down direction",
        "question": "What does torch.flipudis do to return a new tensor?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What are preserved, but appear in a different order than before?": {
        "answer": "Rows",
        "question": "What are preserved, but appear in a different order than before?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "Note Requires the tensor to be at least what?": {
        "answer": "1-D",
        "question": "Note Requires the tensor to be at least what?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What makes a copy of input's data?": {
        "answer": "Note torch.flip",
        "question": "What makes a copy of input's data?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What program returns a view in constant time?": {
        "answer": "NumPy",
        "question": "What program returns a view in constant time?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is torch.flipudis expected to be?": {
        "answer": "slower thannp.flipud",
        "question": "What is torch.flipudis expected to be?",
        "context": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    "What is the name of the tensor created?": {
        "answer": "sizesizefilled withfill_value",
        "question": "What is the name of the tensor created?",
        "context": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "Where is the tensor's dtype inferred?": {
        "answer": "fromfill_value",
        "question": "Where is the tensor's dtype inferred?",
        "context": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "What defines the shape of the output tensor?": {
        "answer": "size",
        "question": "What defines the shape of the output tensor?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the value to fill the output tensor with?": {
        "answer": "fill_value",
        "question": "What is the value to fill the output tensor with?",
        "context": "fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    "Out(Tensor,optional) - what does out(Tensor,optional) refer to?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what does out(Tensor,optional) refer to?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the desired layout of returned Tensor?": {
        "answer": "layout",
        "question": "What is the desired layout of returned Tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the default for the layout of the returned Tensor?": {
        "answer": "Default:torch.strided",
        "question": "What is the default for the layout of the returned Tensor?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "Out(Tensor,optional) - what does out(Tensor,optional) contain?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what does out(Tensor,optional) contain?",
        "context": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the default layout of the returned Tensor?": {
        "answer": "Default:torch.strided",
        "question": "What is the default layout of the returned Tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the default layout of returned Tensor?": {
        "answer": "Default:torch.strided",
        "question": "What is the default layout of returned Tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the name of the function that uses numbers from the continuous uniform distribution?": {
        "answer": "Fillsselftensor",
        "question": "What is the name of the function that uses numbers from the continuous uniform distribution?",
        "context": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    "What does this function do?": {
        "answer": "Gets the current device of the generator",
        "question": "What does this function do?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the generator state?": {
        "answer": "atorch.ByteTensor",
        "question": "What is the name of the generator state?",
        "context": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What contains all the necessary bits to restore a Generator to a specific point in time?": {
        "answer": "Atorch.ByteTensor",
        "question": "What contains all the necessary bits to restore a Generator to a specific point in time?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What returns the initial seed for generating random numbers?": {
        "answer": "atorch.Generatorobject",
        "question": "What returns the initial seed for generating random numbers?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does the Tensor Example do?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does the Tensor Example do?",
        "context": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the object that returns the initial seed for generating random numbers?": {
        "answer": "atorch.Generatorobject",
        "question": "What is the name of the object that returns the initial seed for generating random numbers?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a number that has a good balance of 0 and 1 bits?": {
        "answer": "a large seed",
        "question": "What is a number that has a good balance of 0 and 1 bits?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a good balance for a large seed?": {
        "answer": "0 and 1 bits",
        "question": "What is a good balance for a large seed?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is a good balance of in a seed?": {
        "answer": "0 bits",
        "question": "What is a good balance of in a seed?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the Generator state?": {
        "answer": "atorch.ByteTensor",
        "question": "What is the name of the Generator state?",
        "context": "Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does Atorch.ByteTensor contain?": {
        "answer": "Tensor",
        "question": "What does Atorch.ByteTensor contain?",
        "context": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "Atorch.ByteTensor contains all the necessary bits to restore a Generator to a specific point in time.": {
        "answer": "Tensor",
        "question": "Atorch.ByteTensor contains all the necessary bits to restore a Generator to a specific point in time.",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does Atorch.ByteTensor return?": {
        "answer": "atorch.Generatorobject",
        "question": "What does Atorch.ByteTensor return?",
        "context": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the variable that returns the initial seed for generating random numbers?": {
        "answer": "Tensor",
        "question": "What is the name of the variable that returns the initial seed for generating random numbers?",
        "context": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does the Tensor return?": {
        "answer": "atorch.Generatorobject",
        "question": "What does the Tensor return?",
        "context": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What is the name of the function that returns the initial seed for generating random numbers?": {
        "answer": "Returns the initial seed for generating random numbers",
        "question": "What is the name of the function that returns the initial seed for generating random numbers?",
        "context": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does it do?": {
        "answer": "Sets the seed for generating random numbers",
        "question": "What does it do?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "What does it return?": {
        "answer": "Returns the initial seed for generating random numbers",
        "question": "What does it return?",
        "context": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    "Returns what if inference mode is currently enabled?": {
        "answer": "True",
        "question": "Returns what if inference mode is currently enabled?",
        "context": "Returns True if inference mode is currently enabled. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled"
    },
    "What will be used to calculate the variance?": {
        "answer": "IfunbiasedisTrue",
        "question": "What will be used to calculate the variance?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is calculated if unbiasedisTrue?": {
        "answer": "the sample deviation",
        "question": "What is calculated if unbiasedisTrue?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the dimension or dimensions to reduce?": {
        "answer": "dim",
        "question": "What is the dimension or dimensions to reduce?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What bool indicates whether to use Bessel's correction?": {
        "answer": "unbiased",
        "question": "What bool indicates whether to use Bessel's correction?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "Out(Tensor,optional) - what is the output tensor?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what is the output tensor?",
        "context": "This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What contains the variance and mean?": {
        "answer": "tuple",
        "question": "What contains the variance and mean?",
        "context": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is an intortuple of python:ints?": {
        "answer": "dim",
        "question": "What is an intortuple of python:ints?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the name of the function that determines whether the output tensor is hasdimretained or not?": {
        "answer": "keepdim",
        "question": "What is the name of the function that determines whether the output tensor is hasdimretained or not?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What does a tuple contain?": {
        "answer": "variance and mean",
        "question": "What does a tuple contain?",
        "context": "dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the function that calculates the variance and mean of all elements in the inputtensor?": {
        "answer": "Calculates the variance and mean of all elements in theinputtensor",
        "question": "What is the function that calculates the variance and mean of all elements in the inputtensor?",
        "context": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "Calculates the variance and mean of all elements in what?": {
        "answer": "theinputtensor",
        "question": "Calculates the variance and mean of all elements in what?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "When will Bessel's correction be used?": {
        "answer": "IfunbiasedisTrue",
        "question": "When will Bessel's correction be used?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is calculated without any correction if unbiasedisTrue?": {
        "answer": "the sample deviation",
        "question": "What is calculated without any correction if unbiasedisTrue?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the name of Bessel's correction?": {
        "answer": "unbiased",
        "question": "What is the name of Bessel's correction?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "A tuple (var, mean) contains what?": {
        "answer": "variance and mean",
        "question": "A tuple (var, mean) contains what?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What indicates that Bessel's correction will be used?": {
        "answer": "IfunbiasedisTrue",
        "question": "What indicates that Bessel's correction will be used?",
        "context": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is the name of the output tensor?": {
        "answer": "keepdim",
        "question": "What is the name of the output tensor?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What does the function do?": {
        "answer": "Calculates the variance and mean of all elements in theinputtensor",
        "question": "What does the function do?",
        "context": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    "What is a contiguous, one-dimensional array of a single data type?": {
        "answer": "Atorch.Storageis",
        "question": "What is a contiguous, one-dimensional array of a single data type?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has a corresponding storage of the same data type?": {
        "answer": "Everytorch.Tensor",
        "question": "What has a corresponding storage of the same data type?",
        "context": "Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Everytorch.Tensorcast to?": {
        "answer": "bfloat16",
        "question": "What type of storage does Everytorch.Tensorcast to?",
        "context": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Casts this storage to Casts this storage to bool type Casts this storage to byte type Casts this storage": {
        "answer": "bfloat16",
        "question": "What type of storage does Casts this storage to Casts this storage to bool type Casts this storage to byte type Casts this storage",
        "context": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the object is already in CUDA memory and on the correct device, what is performed and the original object is returned?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Casts this storage to Casts this storage to byte type Casts this storage to char type Casts this storage to": {
        "answer": "bool",
        "question": "What type of storage does Casts this storage to Casts this storage to byte type Casts this storage to char type Casts this storage to",
        "context": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Casts this storage to what type of type?": {
        "answer": "bfloat16",
        "question": "Casts this storage to what type of type?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does device(int) contain?": {
        "answer": "destination GPU id",
        "question": "What does device(int) contain?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does device(int) default to?": {
        "answer": "current device",
        "question": "What does device(int) default to?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Casts this storage to Returns a copy of?": {
        "answer": "char",
        "question": "What type of storage does Casts this storage to Returns a copy of?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "device(int) \u2013 The what?": {
        "answer": "destination GPU id",
        "question": "device(int) \u2013 The what?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does Casts this storage to complex double type Casts this storage to complex float type Returns if it's not already on the CPU": {
        "answer": "a CPU copy",
        "question": "What does Casts this storage to complex double type Casts this storage to complex float type Returns if it's not already on the CPU",
        "context": "Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Casts this storage to complex what type of type?": {
        "answer": "float",
        "question": "Casts this storage to complex what type of type?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does cast this storage to complex float type return if it's not already on the CPU?": {
        "answer": "a CPU copy",
        "question": "What does cast this storage to complex float type return if it's not already on the CPU?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the storage is already on the CPU, where is the CPU copy of the storage returned?": {
        "answer": "if it\u2019s not already on the CPU",
        "question": "If the storage is already on the CPU, where is the CPU copy of the storage returned?",
        "context": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the destination GPU id?": {
        "answer": "device(int)",
        "question": "What is the destination GPU id?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory, the copy will be what with respect to the host?": {
        "answer": "asynchronous",
        "question": "If the source is in pinned memory, the copy will be what with respect to the host?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has no effect if the source is in pinned memory?": {
        "answer": "argument",
        "question": "What has no effect if the source is in pinned memory?",
        "context": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Where is the original object stored?": {
        "answer": "CUDA memory",
        "question": "Where is the original object stored?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory, the argument has what effect?": {
        "answer": "no effect",
        "question": "If the source is in pinned memory, the argument has what effect?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does kwargs contain for compatibility?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "What does kwargs contain for compatibility?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already in what, then no copy is performed and the original object is returned?": {
        "answer": "CUDA memory",
        "question": "If the object is already in what, then no copy is performed and the original object is returned?",
        "context": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what happens?": {
        "answer": "the argument has no effect",
        "question": "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what happens?",
        "context": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the keyasyncin cast to?": {
        "answer": "double type",
        "question": "What type of storage does the keyasyncin cast to?",
        "context": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what effect does the argument have?": {
        "answer": "no effect",
        "question": "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what effect does the argument have?",
        "context": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is used to share memory between all processes?": {
        "answer": "IfsharedisTrue",
        "question": "What type of storage is used to share memory between all processes?",
        "context": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if the storage is cast to double type?": {
        "answer": "All changes are written to the file",
        "question": "What happens if the storage is cast to double type?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the changes on the storage do not affect the file, what is the default?": {
        "answer": "IfsharedisFalse",
        "question": "If the changes on the storage do not affect the file, what is the default?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What may kwargs contain for compatibility?": {
        "answer": "keyasyncin place of thenon_blockingargument",
        "question": "What may kwargs contain for compatibility?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the size of the storage?": {
        "answer": "sizeis the number of elements in the storage",
        "question": "What is the size of the storage?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfsharedisFalse, the file must contain what?": {
        "answer": "at leastsize * sizeof(Type)bytes",
        "question": "IfsharedisFalse, the file must contain what?",
        "context": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What will be created if needed?": {
        "answer": "IfsharedisTruethe file",
        "question": "What will be created if needed?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the file name to map?": {
        "answer": "filename(str)",
        "question": "What is the file name to map?",
        "context": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if the storage is cast to float type IfsharedisTrue?": {
        "answer": "All changes are written to the file",
        "question": "What happens if the storage is cast to float type IfsharedisTrue?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "IfsharedisTrue, the file must contain what?": {
        "answer": "at leastsize * sizeof(Type)bytes",
        "question": "IfsharedisTrue, the file must contain what?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the file name to map shared?": {
        "answer": "filename(str)",
        "question": "What is the file name to map shared?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What means that memory is shared between all processes?": {
        "answer": "IfsharedisTrue",
        "question": "What means that memory is shared between all processes?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if memory is shared between all processes?": {
        "answer": "All changes are written to the file",
        "question": "What happens if memory is shared between all processes?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What means that the changes on the storage do not affect the file?": {
        "answer": "IfsharedisFalse",
        "question": "What means that the changes on the storage do not affect the file?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the size of a file?": {
        "answer": "sizeis the number of elements in the storage",
        "question": "What is the size of a file?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does size mean?": {
        "answer": "sizeis the number of elements in the storage",
        "question": "What does size mean?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the file must contain at leastsize * sizeof(Type)bytes, what is it called?": {
        "answer": "IfsharedisFalse",
        "question": "If the file must contain at leastsize * sizeof(Type)bytes, what is it called?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What file will be created if needed?": {
        "answer": "IfsharedisTruethe",
        "question": "What file will be created if needed?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does Casts this storage to if it's not already pinned?": {
        "answer": "Copies the storage to pinned memory",
        "question": "What does Casts this storage to if it's not already pinned?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory?": {
        "answer": "Storages in shared memory cannot be resized",
        "question": "What is a no-op for storages already in shared memory?",
        "context": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Which storage does not need to be moved for sharing across processes?": {
        "answer": "CUDA storages",
        "question": "Which storage does not need to be moved for sharing across processes?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is a no-op for storages already in shared memory and for CUDA storages?": {
        "answer": "Storages in shared memory cannot be resized",
        "question": "What is a no-op for storages already in shared memory and for CUDA storages?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What returns the storage to shared memory?": {
        "answer": "self",
        "question": "What returns the storage to shared memory?",
        "context": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does self return?": {
        "answer": "self Casts this storage to short type Returns a list containing the elements of this storage",
        "question": "What does self return?",
        "context": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What storage does not need to be moved for sharing across processes?": {
        "answer": "CUDA storages",
        "question": "What storage does not need to be moved for sharing across processes?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Is shared memory able to be resized?": {
        "answer": "Storages in shared memory cannot be resized",
        "question": "Is shared memory able to be resized?",
        "context": "Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does self do to a storage?": {
        "answer": "self Casts this storage to short type",
        "question": "What does self do to a storage?",
        "context": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is cast to pinned memory if it's not already pinned?": {
        "answer": "Copies",
        "question": "What type of storage is cast to pinned memory if it's not already pinned?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does not need to be moved for sharing across processes?": {
        "answer": "CUDA storages",
        "question": "What type of storage does not need to be moved for sharing across processes?",
        "context": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the storage that casts it to short type?": {
        "answer": "self",
        "question": "What is the name of the storage that casts it to short type?",
        "context": "Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage is cast to pinned memory?": {
        "answer": "Copies",
        "question": "What type of storage is cast to pinned memory?",
        "context": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type does self cast the storage to?": {
        "answer": "ifdtypeis not provided",
        "question": "What type does self cast the storage to?",
        "context": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What does the storage do if it's not already pinned?": {
        "answer": "Copies",
        "question": "What does the storage do if it's not already pinned?",
        "context": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is performed if the storage is already of the correct type?": {
        "answer": "no copy",
        "question": "What is performed if the storage is already of the correct type?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the storage is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the storage is already of the correct type, what is performed and the original object is returned?",
        "context": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the desired type of a storage?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the desired type of a storage?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What returns the type ifdtypeis not provided?": {
        "answer": "self Casts this storage to short type Returns a list containing the elements of this storage",
        "question": "What returns the type ifdtypeis not provided?",
        "context": "Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If this is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If this is already of the correct type, what is performed and the original object is returned?",
        "context": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the desired type of the storage?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the desired type of the storage?",
        "context": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if the type of the storage is not provided?": {
        "answer": "ifdtypeis not provided",
        "question": "What happens if the type of the storage is not provided?",
        "context": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What returns the type of the storage?": {
        "answer": "ifdtypeis not provided",
        "question": "What returns the type of the storage?",
        "context": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the object is already of the correct type, what is performed and the original object is returned?": {
        "answer": "no copy",
        "question": "If the object is already of the correct type, what is performed and the original object is returned?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the name of the type non_blocking(bool)?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the name of the type non_blocking(bool)?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What has no effect if the source is in pinned memory and destination is on the GPU?": {
        "answer": "argument",
        "question": "What has no effect if the source is in pinned memory and destination is on the GPU?",
        "context": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type does return the type of the object?": {
        "answer": "ifdtypeis not provided",
        "question": "What type does return the type of the object?",
        "context": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens if the object is already of the correct type?": {
        "answer": "no copy is performed and the original object is returned",
        "question": "What happens if the object is already of the correct type?",
        "context": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What happens to the argument if the source is in pinned memory and destination is on the GPU?": {
        "answer": "no effect",
        "question": "What happens to the argument if the source is in pinned memory and destination is on the GPU?",
        "context": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is the desired type non_blocking(bool)?": {
        "answer": "dtype(typeorstring)",
        "question": "What is the desired type non_blocking(bool)?",
        "context": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory and destination is on the GPU, the copy is performed asynchronously with respect to the host, what happens": {
        "answer": "argument has no effect",
        "question": "If the source is in pinned memory and destination is on the GPU, the copy is performed asynchronously with respect to the host, what happens",
        "context": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What is deprecated?": {
        "answer": "Theasyncarg",
        "question": "What is deprecated?",
        "context": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does Theasyncarg cast to?": {
        "answer": "bfloat16",
        "question": "What type of storage does Theasyncarg cast to?",
        "context": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the argument is true, the copy is performed asynchronously with respect to the host, and the source is in pinned memory and destination is": {
        "answer": "no effect",
        "question": "If the argument is true, the copy is performed asynchronously with respect to the host, and the source is in pinned memory and destination is",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "What type of storage does the bool type cast to?": {
        "answer": "byte type",
        "question": "What type of storage does the bool type cast to?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "If the source is in pinned memory and destination is on the GPU, how is the copy performed with respect to the host?": {
        "answer": "asynchronously",
        "question": "If the source is in pinned memory and destination is on the GPU, how is the copy performed with respect to the host?",
        "context": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. ",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    "Solves a system of equations with what?": {
        "answer": "triangular coefficient",
        "question": "Solves a system of equations with what?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does solvesAX=bAX = bAX=band assumesAAAis?": {
        "answer": "upper-triangular",
        "question": "What does solvesAX=bAX = bAX=band assumesAAAis?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What can torch.triangular_solve(b, A) take in?": {
        "answer": "2D inputsb",
        "question": "What can torch.triangular_solve(b, A) take in?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does torch.triangular_solve(b, A) return if the inputs are batches?": {
        "answer": "batched outputs",
        "question": "What does torch.triangular_solve(b, A) return if the inputs are batches?",
        "context": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does solvesAAA assume is?": {
        "answer": "upper-triangular",
        "question": "What does solvesAAA assume is?",
        "context": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What does torch.triangular_solve return if the inputs are batches?": {
        "answer": "batched outputsX",
        "question": "What does torch.triangular_solve return if the inputs are batches?",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the value of * of more batch dimensions?": {
        "answer": "zero",
        "question": "What is the value of * of more batch dimensions?",
        "context": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(": {
        "answer": "b(Tensor)",
        "question": "What is the name of the multiple right-hand sides of size(,m,k)(*, m, k)(",
        "context": "torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "Supports input of what data types?": {
        "answer": "float, double, cfloat and cdouble",
        "question": "Supports input of what data types?",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What supports multiple right-hand sides of size(,m,k)(*, m, k)(,m,k": {
        "answer": "b(Tensor)",
        "question": "What supports multiple right-hand sides of size(,m,k)(*, m, k)(,m,k",
        "context": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value of the upper-triangular system of equations?": {
        "answer": "True",
        "question": "What is the default value of the upper-triangular system of equations?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the name of the multiple right-hand sides of size(,m,k)?": {
        "answer": "b(Tensor)",
        "question": "What is the name of the multiple right-hand sides of size(,m,k)?",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is transpose(bool,optional)?": {
        "answer": "whetherAAAshould be transposed before being sent into the solver",
        "question": "What is transpose(bool,optional)?",
        "context": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value of the transpose(bool,optional)?": {
        "answer": "False",
        "question": "What is the default value of the transpose(bool,optional)?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the input triangular coefficient matrix of size?": {
        "answer": "A(Tensor)",
        "question": "What is the input triangular coefficient matrix of size?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the default value for whether the AAA should be transposed before being sent into the solver?": {
        "answer": "transpose",
        "question": "What is the default value for whether the AAA should be transposed before being sent into the solver?",
        "context": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    "What is the tensor filled with?": {
        "answer": "scalar value1,",
        "question": "What is the tensor filled with?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is a sequence of integers defining the shape of the output tensor?": {
        "answer": "size(int...)",
        "question": "What is a sequence of integers defining the shape of the output tensor?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "Can be a variable number of arguments or what?": {
        "answer": "a collection like a list or tuple",
        "question": "Can be a variable number of arguments or what?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "Out(Tensor,optional) - what does out(Tensor,optional) return?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what does out(Tensor,optional) return?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is the default value of seetorch.set_default_tensor_type()?": {
        "answer": "ifNone",
        "question": "What is the default value of seetorch.set_default_tensor_type()?",
        "context": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What can size(int) be?": {
        "answer": "a variable number of arguments",
        "question": "What can size(int) be?",
        "context": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    "What is modeled after SciPy'sspecialmodule?": {
        "answer": "The torch.special module",
        "question": "What is modeled after SciPy'sspecialmodule?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the torch.special module modeled after?": {
        "answer": "SciPy",
        "question": "What is the torch.special module modeled after?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What version of PyTorch is the torch.special module in?": {
        "answer": "BETA",
        "question": "What version of PyTorch is the torch.special module in?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the status of the torch.special module?": {
        "answer": "New functions are still being added",
        "question": "What is the status of the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Where can you find more information about the torch.special module?": {
        "answer": "the documentation of each function for details",
        "question": "Where can you find more information about the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Computes the entropy oninput, what is it?": {
        "answer": "elementwise",
        "question": "Computes the entropy oninput, what is it?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Out(Tensor,optional) - what does out(Tensor,optional) represent?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what does out(Tensor,optional) represent?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does the torch.special module compute?": {
        "answer": "the error function ofinput",
        "question": "What does the torch.special module compute?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the error function of input defined as?": {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the error function of input defined as?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the error function of input?": {
        "answer": "complementary error function ofinput",
        "question": "What is the error function of input?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is out(Tensor,optional) defined as?": {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional) defined as?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the inverse error function of input?": {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is the inverse error function of input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the complementary error function of input?": {
        "answer": "inverse error function ofinput",
        "question": "What is the complementary error function of input?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is another name for the logistic sigmoid function?": {
        "answer": "expit",
        "question": "What is another name for the logistic sigmoid function?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Out(Tensor,optional) \u2013 what?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) \u2013 what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Computes the exponential of the elements minus what ofinput?": {
        "answer": "1",
        "question": "Computes the exponential of the elements minus what ofinput?",
        "context": "Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the name of the function that computes the exponential of the elements minus 1 ofinput?": {
        "answer": "Note",
        "question": "What is the name of the function that computes the exponential of the elements minus 1 ofinput?",
        "context": "Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Out(Tensor,optional) is what?": {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) is what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the name of the function that computes the exponential of the elements minus 1 of input?": {
        "answer": "Note",
        "question": "What is the name of the function that computes the exponential of the elements minus 1 of input?",
        "context": "Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is defined in the range(1,1)(-1, 1)(1,1)?": {
        "answer": "inverse error function ofinput",
        "question": "What is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the expit also known as?": {
        "answer": "the logistic sigmoid function",
        "question": "What is the expit also known as?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What function provides greater precision than the exponential of the elements minus 1 ofinput?": {
        "answer": "exp(x) - 1",
        "question": "What function provides greater precision than the exponential of the elements minus 1 ofinput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the base of the exponential function of input?": {
        "answer": "base two",
        "question": "What is the base of the exponential function of input?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the input(Tensor)?": {
        "answer": "input tensor",
        "question": "What is the input(Tensor)?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "The exponential of the elements minus 1 ofinput provides greater precision than what for small values of x?": {
        "answer": "exp(x) - 1",
        "question": "The exponential of the elements minus 1 ofinput provides greater precision than what for small values of x?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is out(Tensor,optional)?": {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional)?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the function that computes the exponential of the elements minus 1 ofinput?": {
        "answer": "base two exponential function ofinput",
        "question": "What is the function that computes the exponential of the elements minus 1 ofinput?",
        "context": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does this function compute?": {
        "answer": "the base two exponential function ofinput",
        "question": "What does this function compute?",
        "context": "Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "This function provides greater precision than what for small values of x?": {
        "answer": "exp(x) - 1",
        "question": "This function provides greater precision than what for small values of x?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the output tensor?": {
        "answer": "out(Tensor,optional) \u2013 the output tensor",
        "question": "What is the output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Computes the natural what of the absolute value of the gamma function oninput?": {
        "answer": "logarithm",
        "question": "Computes the natural what of the absolute value of the gamma function oninput?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What type of function is computed for each element of input?": {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What type of function is computed for each element of input?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is an example of a function that computes the base two exponential function of input?": {
        "answer": "Computes the base two exponential function ofinput",
        "question": "What is an example of a function that computes the base two exponential function of input?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the absolute value of the gamma function oninput?": {
        "answer": "natural logarithm",
        "question": "What is the absolute value of the gamma function oninput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What order modified Bessel function is computed for each element of input?": {
        "answer": "zeroth",
        "question": "What order modified Bessel function is computed for each element of input?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the base two exponential function of input?": {
        "answer": "Computes the base two exponential function ofinput",
        "question": "What is the base two exponential function of input?",
        "context": "Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Input is clamped to what when eps is not None?": {
        "answer": "eps",
        "question": "Input is clamped to what when eps is not None?",
        "context": "Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "When eps is None andinput 0 orinput> 1, the function will yield what?": {
        "answer": "NaN",
        "question": "When eps is None andinput 0 orinput> 1, the function will yield what?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "Inputis clamped to what when eps is not None?": {
        "answer": "eps",
        "question": "Inputis clamped to what when eps is not None?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is the epsilon for input clamp bound?": {
        "answer": "float",
        "question": "What is the epsilon for input clamp bound?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What does out(Tensor,optional) return?": {
        "answer": "output tensor",
        "question": "What does out(Tensor,optional) return?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is an example of a function that computes input*log1p?": {
        "answer": "Computesinput*log1p(other)with the following cases",
        "question": "What is an example of a function that computes input*log1p?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is'sscipy.special.xlog1py' similar to?": {
        "answer": "SciPy",
        "question": "What is'sscipy.special.xlog1py' similar to?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is an example of an output tensor?": {
        "answer": "Computesinput*log1p(other)with the following cases",
        "question": "What is an example of an output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfc"
    },
    "What is a boolean tensor that is True whereinputis greater thanother?": {
        "answer": "Computesinput",
        "question": "What is a boolean tensor that is True whereinputis greater thanother?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What can the second argument be?": {
        "answer": "a number or a tensor whose shape isbroadcastablewith the first argument",
        "question": "What can the second argument be?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What is the tensor to compare out(Tensor,optional)?": {
        "answer": "output tensor",
        "question": "What is the tensor to compare out(Tensor,optional)?",
        "context": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    "What does PCA stand for?": {
        "answer": "linear Principal Component Analysis",
        "question": "What does PCA stand for?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the nearly optimal approximation of a singular value decomposition of a centered matrixAAA?": {
        "answer": "namedtuple",
        "question": "What is the nearly optimal approximation of a singular value decomposition of a centered matrixAAA?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represents the principal directions of AAA?": {
        "answer": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "question": "What represents the principal directions of AAA?",
        "context": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions?": {
        "answer": "AAAis",
        "question": "What is a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions?",
        "context": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the relation of(U,S,V)to PCA?": {
        "answer": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "question": "What is the relation of(U,S,V)to PCA?",
        "context": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "The size of returned matrices depend on the specified rank and what other value?": {
        "answer": "q",
        "question": "The size of returned matrices depend on the specified rank and what other value?",
        "context": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What represent the principal directions?": {
        "answer": "theVVVcolumns",
        "question": "What represent the principal directions?",
        "context": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the input tensor of size(,m,n)?": {
        "answer": "A(Tensor)",
        "question": "What is the input tensor of size(,m,n)?",
        "context": "Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the default q value?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "What is the default q value?",
        "context": "Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the default setting to center the input tensor?": {
        "answer": "if True",
        "question": "What is the default setting to center the input tensor?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the input tensor of size(,m,n)(*, m, n)(,m,": {
        "answer": "A(Tensor)",
        "question": "What is the input tensor of size(,m,n)(*, m, n)(,m,",
        "context": "UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the default value of q?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "What is the default value of q?",
        "context": "UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the number of subspace iterations to conduct?": {
        "answer": "niter",
        "question": "What is the number of subspace iterations to conduct?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the input tensor of size?": {
        "answer": "A(Tensor)",
        "question": "What is the input tensor of size?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the default value of the q(int,optional)?": {
        "answer": "By default,q=min(6,m,n)",
        "question": "What is the default value of the q(int,optional)?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is a reference to the number of subspace iterations to conduct?": {
        "answer": "References",
        "question": "What is a reference to the number of subspace iterations to conduct?",
        "context": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    "What is the new tensor that returns a new tensor that is?": {
        "answer": "a narrowed version",
        "question": "What is the new tensor that returns a new tensor that is?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What is the dimensiondimis input?": {
        "answer": "fromstarttostart+length",
        "question": "What is the dimensiondimis input?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What do the returned tensor andinputtensor share?": {
        "answer": "underlying storage",
        "question": "What do the returned tensor andinputtensor share?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "What is the starting dimension length?": {
        "answer": "the distance to the ending dimension",
        "question": "What is the starting dimension length?",
        "context": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    "Returns the minimum value of all elements in what?": {
        "answer": "theinputtensor",
        "question": "Returns the minimum value of all elements in what?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the minimum value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What is the minimum value of each row of the inputtensor in the given dimensiondim?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "Andindicesis what of each minimum value found (argmin)?": {
        "answer": "index location",
        "question": "Andindicesis what of each minimum value found (argmin)?",
        "context": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the index location of each minimum value found?": {
        "answer": "Andindicesis the index location of each minimum value found",
        "question": "What is the index location of each minimum value found?",
        "context": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the value of the minimum value of each row of the inputtensor in the given dimensiondim?": {
        "answer": "Note",
        "question": "What is the value of the minimum value of each row of the inputtensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What returns the minimum value of each row of theinputtensor in the given dimensiondim?": {
        "answer": "a namedtuple",
        "question": "What returns the minimum value of each row of theinputtensor in the given dimensiondim?",
        "context": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "When are the output tensors of the same size as input?": {
        "answer": "IfkeepdimisTrue",
        "question": "When are the output tensors of the same size as input?",
        "context": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What happens if there are multiple minimal values in a reduced row?": {
        "answer": "the indices of the first minimal value are returned",
        "question": "What happens if there are multiple minimal values in a reduced row?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What is the tuple of two output tensors?": {
        "answer": "out",
        "question": "What is the tuple of two output tensors?",
        "context": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    "What kind of function is the call to geqrf?": {
        "answer": "low-level",
        "question": "What kind of function is the call to geqrf?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the return value of the low-level function for calling LAPACK's geqrf?": {
        "answer": "namedtuple",
        "question": "What is the return value of the low-level function for calling LAPACK's geqrf?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What does the function compute?": {
        "answer": "QR decomposition ofinput",
        "question": "What does the function compute?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What are stored in the same output tensora?": {
        "answer": "BothQandRmatrices",
        "question": "What are stored in the same output tensora?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where are the elements ofRare stored?": {
        "answer": "on and above the diagonal",
        "question": "Where are the elements ofRare stored?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Where is the matrix stored?": {
        "answer": "below the diagonal",
        "question": "Where is the matrix stored?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the result of withtorch.ormqr()?": {
        "answer": "matrix-matrix multiplication",
        "question": "What is the result of withtorch.ormqr()?",
        "context": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the name of the LAPACK function that computes Q and R matrices?": {
        "answer": "geqrf",
        "question": "What is the name of the LAPACK function that computes Q and R matrices?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What can solve matrix equations using?": {
        "answer": "a QR decomposition",
        "question": "What can solve matrix equations using?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the input matrix out(tuple,optional)?": {
        "answer": "input(Tensor)",
        "question": "What is the input matrix out(tuple,optional)?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "Ignored what?": {
        "answer": "ifNone",
        "question": "Ignored what?",
        "context": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What is the default value for the input matrix out(tuple,optional)?": {
        "answer": "Default:None",
        "question": "What is the default value for the input matrix out(tuple,optional)?",
        "context": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    "What returns a number of samples from the multinomial probability distribution located in the corresponding row of tensorinput?": {
        "answer": "tensor",
        "question": "What returns a number of samples from the multinomial probability distribution located in the corresponding row of tensorinput?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What must the rows ofinputdo be?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What must the rows ofinputdo be?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is ordered from left to right according to when each was sampled?": {
        "answer": "Indices",
        "question": "What is ordered from left to right according to when each was sampled?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinput is a vector, what is it?": {
        "answer": "Ifinputis a vector",
        "question": "Ifinput is a vector, what is it?",
        "context": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What must the rows of inputdo be?": {
        "answer": "non-negative, finite and have a non-zero sum",
        "question": "What must the rows of inputdo be?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What are ordered from left to right according to when each was sampled?": {
        "answer": "Indices",
        "question": "What are ordered from left to right according to when each was sampled?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a vector,outis a vector of what?": {
        "answer": "sizenum_samples",
        "question": "Ifinputis a vector,outis a vector of what?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix, outis a matrix of shape(m times textnum_samples": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a matrix, outis a matrix of shape(m times textnum_samples",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what isTrue, samples are drawn with what?": {
        "answer": "replacement",
        "question": "If what isTrue, samples are drawn with what?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix,outis a matrix of what?": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a matrix,outis a matrix of what?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix,outis a matrix of shape(mnum_samples)?": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a matrix,outis a matrix of shape(mnum_samples)?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If what isTrue, samples are drawn with replacement. If not, samples are drawn without replacement.": {
        "answer": "replacement",
        "question": "If what isTrue, samples are drawn with replacement. If not, samples are drawn without replacement.",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If replacement isTrue, samples are drawn with replacement. If not, what happens to them?": {
        "answer": "they are drawn without replacement",
        "question": "If replacement isTrue, samples are drawn with replacement. If not, what happens to them?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the name of the variable that is used when a sample index is drawn for a row?": {
        "answer": "Note",
        "question": "What is the name of the variable that is used when a sample index is drawn for a row?",
        "context": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "Ifinputis a matrix withmrows, outis a matrix of shape(mnum_samples)?": {
        "answer": "matrix withmrows",
        "question": "Ifinputis a matrix withmrows, outis a matrix of shape(mnum_samples)?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the name of a vector that is a matrix of sizenum_samples?": {
        "answer": "Note",
        "question": "What is the name of a vector that is a matrix of sizenum_samples?",
        "context": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "If a sample index is drawn for a row, it cannot be drawn again for that row?": {
        "answer": "If replacement isTrue",
        "question": "If a sample index is drawn for a row, it cannot be drawn again for that row?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What must num_samples be lower than when drawn without replacement?": {
        "answer": "number of non-zero elements ininput",
        "question": "What must num_samples be lower than when drawn without replacement?",
        "context": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What does input(Tensor) contain probabilities?": {
        "answer": "input tensor",
        "question": "What does input(Tensor) contain probabilities?",
        "context": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is drawn for a row, it cannot be drawn again for that row?": {
        "answer": "a sample index",
        "question": "What is drawn for a row, it cannot be drawn again for that row?",
        "context": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What is the input tensor containing probabilities?": {
        "answer": "input(Tensor)",
        "question": "What is the input tensor containing probabilities?",
        "context": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    "What do modules make it easy to specify for PyTorch's Optimizers to update?": {
        "answer": "learnable parameters",
        "question": "What do modules make it easy to specify for PyTorch's Optimizers to update?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How do modules work with PyTorch's autogradsystem?": {
        "answer": "Easy to work with and transform",
        "question": "How do modules work with PyTorch's autogradsystem?",
        "context": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is one of the advantages of using modules?": {
        "answer": "Easy to work with and transform",
        "question": "What is one of the advantages of using modules?",
        "context": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Who is this note intended for?": {
        "answer": "all PyTorch users",
        "question": "Who is this note intended for?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Why are many topics in this note elaborated on in other notes or tutorials?": {
        "answer": "modules are so fundamental to PyTorch",
        "question": "Why are many topics in this note elaborated on in other notes or tutorials?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a Simple Custom Module Modules a part of?": {
        "answer": "Building Blocks Neural Network Training",
        "question": "What is a Simple Custom Module Modules a part of?",
        "context": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does this note describe?": {
        "answer": "modules",
        "question": "What does this note describe?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple custom module?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What is a simple custom module?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a module that can be used for Neural Network Training?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What is a module that can be used for Neural Network Training?",
        "context": "Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does this module apply to its input?": {
        "answer": "an affine transformation",
        "question": "What does this module apply to its input?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the basic feature of the Linearmodule module?": {
        "answer": "module has the following fundamental characteristics of modules",
        "question": "What is the basic feature of the Linearmodule module?",
        "context": "Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an example of a module that can be used to train a neural network?": {
        "answer": "Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "question": "What is an example of a module that can be used to train a neural network?",
        "context": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the Linearmodule module apply to its input?": {
        "answer": "an affine transformation",
        "question": "What does the Linearmodule module apply to its input?",
        "context": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the basic feature of the Linearmodule?": {
        "answer": "module has the following fundamental characteristics of modules",
        "question": "What is the basic feature of the Linearmodule?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the module that lets us look at a custom version of PyTorch'sLinearmodule?": {
        "answer": "Module State Module Hooks Advanced Features",
        "question": "What is the name of the module that lets us look at a custom version of PyTorch'sLinearmodule?",
        "context": "Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the feature that lets us look at a custom version of PyTorch'sLinearmodule?": {
        "answer": "Module Hooks Advanced Features",
        "question": "What is the name of the feature that lets us look at a custom version of PyTorch'sLinearmodule?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the simple module inherit from the base Module class?": {
        "answer": "module has the following fundamental characteristics of modules",
        "question": "What does the simple module inherit from the base Module class?",
        "context": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a custom version of PyTorch'sLinearmodule called?": {
        "answer": "Advanced Features",
        "question": "What is a custom version of PyTorch'sLinearmodule called?",
        "context": "Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the module that applies an affine transformation to its input?": {
        "answer": "a simpler, custom version of PyTorch\u2019sLinearmodule",
        "question": "What is the name of the module that applies an affine transformation to its input?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What defines some that is used in computation?": {
        "answer": "state",
        "question": "What defines some that is used in computation?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is each of the random-initializedweightandbiastensors defined as?": {
        "answer": "aParameter",
        "question": "What is each of the random-initializedweightandbiastensors defined as?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be considered the \"learnable\" aspects of the module's computation?": {
        "answer": "Parameters",
        "question": "What can be considered the \"learnable\" aspects of the module's computation?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Modules are not required to have what?": {
        "answer": "state",
        "question": "Modules are not required to have what?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What function performs the computation?": {
        "answer": "forward()",
        "question": "What function performs the computation?",
        "context": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What performs arbitrary computation involving any number of inputs and outputs?": {
        "answer": "theforward()implementation",
        "question": "What performs arbitrary computation involving any number of inputs and outputs?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What demonstrates how modules package state and computation together?": {
        "answer": "module",
        "question": "What demonstrates how modules package state and computation together?",
        "context": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can be constructed and called in this simple module?": {
        "answer": "Instances",
        "question": "What can be constructed and called in this simple module?",
        "context": "This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are aspects of a module's computation that should be \"learned\"?": {
        "answer": "the parameters registered by a module",
        "question": "What are aspects of a module's computation that should be \"learned\"?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What program can be used to update the parameters of a module?": {
        "answer": "PyTorch",
        "question": "What program can be used to update the parameters of a module?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What should we examine first?": {
        "answer": "how modules can be composed with one another",
        "question": "What should we examine first?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Modules can contain other what?": {
        "answer": "modules",
        "question": "Modules can contain other what?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the simplest way to do this?": {
        "answer": "using theSequentialmodule",
        "question": "What is the simplest way to do this?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does Sequential feed the output of the first MyLinearmodule into?": {
        "answer": "theReLU",
        "question": "What does Sequential feed the output of the first MyLinearmodule into?",
        "context": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is theSequentialmodule limited to?": {
        "answer": "in-order chaining of modules",
        "question": "What is theSequentialmodule limited to?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does Sequential automatically feed the output of the first MyLinearmodule as?": {
        "answer": "input into theReLU",
        "question": "What does Sequential automatically feed the output of the first MyLinearmodule as?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Sequentialautomatically feeds the output of the firstMyLinearmodule as input into the ReLU, and the output of that as input": {
        "answer": "in-order chaining of modules",
        "question": "Sequentialautomatically feeds the output of the firstMyLinearmodule as input into the ReLU, and the output of that as input",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is recommended for anything beyond the simplest use cases?": {
        "answer": "a custom module",
        "question": "What is recommended for anything beyond the simplest use cases?",
        "context": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a simple neural network implemented as?": {
        "answer": "a custom module",
        "question": "What is a simple neural network implemented as?",
        "context": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is recommended to define for anything beyond the simplest use cases?": {
        "answer": "a custom module",
        "question": "What is recommended to define for anything beyond the simplest use cases?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is implemented as a custom module?": {
        "answer": "a simple neural network",
        "question": "What is implemented as a custom module?",
        "context": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does s() andnamed_modules()recursivelyiterate through a module and its child modules?": {
        "answer": "module",
        "question": "What does s() andnamed_modules()recursivelyiterate through a module and its child modules?",
        "context": "To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What register submodules from a list or dict?": {
        "answer": "TheModuleListandModuleDictmodules",
        "question": "What register submodules from a list or dict?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is it sometimes necessary for a module to do?": {
        "answer": "dynamically define submodules",
        "question": "What is it sometimes necessary for a module to do?",
        "context": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do calls toparameters() andnamed_parameters() recursively include?": {
        "answer": "child parameters",
        "question": "What do calls toparameters() andnamed_parameters() recursively include?",
        "context": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can elaborate neural networks be formed?": {
        "answer": "module composition",
        "question": "How can elaborate neural networks be formed?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where can PyTorch provide a large library of performant modules?": {
        "answer": "thetorch.nnnamespace",
        "question": "Where can PyTorch provide a large library of performant modules?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a full example of?": {
        "answer": "training a neural network",
        "question": "What is a full example of?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do you want to know about PyTorch?": {
        "answer": "more information",
        "question": "What do you want to know about PyTorch?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an example of in the next section?": {
        "answer": "training a neural network",
        "question": "What is an example of in the next section?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an example of how to train a neural network?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is an example of how to train a neural network?",
        "context": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an example of a function that can be added to a module and its submodules?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is an example of a function that can be added to a module and its submodules?",
        "context": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a function that can be added to a module and its submodules?": {
        "answer": "Recursivelyapply()a function",
        "question": "What is a function that can be added to a module and its submodules?",
        "context": "Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is one of PyTorch's Optimizers?": {
        "answer": "Defining neural net modules",
        "question": "What is one of PyTorch's Optimizers?",
        "context": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are present in this simplified example?": {
        "answer": "key parts of training",
        "question": "What are present in this simplified example?",
        "context": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What happens when a network is built?": {
        "answer": "it has to be trained",
        "question": "What happens when a network is built?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the key part of training?": {
        "answer": "A network is created",
        "question": "What is the key part of training?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the network learn to output in this simplified example?": {
        "answer": "zero",
        "question": "What does the network learn to output in this simplified example?",
        "context": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the optimizer in this example?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is the optimizer in this example?",
        "context": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the optimizer do?": {
        "answer": "computes a loss",
        "question": "What does the optimizer do?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is created?": {
        "answer": "A network",
        "question": "What is created?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the optimizer that is created when a network is created?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is the name of the optimizer that is created when a network is created?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does the optimizer do to the network's gradients?": {
        "answer": "zeros",
        "question": "What does the optimizer do to the network's gradients?",
        "context": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an optimizer in this case?": {
        "answer": "stochastic gradient descent optimizer",
        "question": "What is an optimizer in this case?",
        "context": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does loss.backward() do to update the parameters' gradients?": {
        "answer": "zeros",
        "question": "What does loss.backward() do to update the parameters' gradients?",
        "context": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What happens after the above snippet is run?": {
        "answer": "the network\u2019s parameters have changed",
        "question": "What happens after the above snippet is run?",
        "context": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "The value ofl1'sweightparameter is closer to what value?": {
        "answer": "0",
        "question": "The value ofl1'sweightparameter is closer to what value?",
        "context": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is an example of?": {
        "answer": "training a neural network",
        "question": "What is an example of?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What happens after the above snippet has been run?": {
        "answer": "the network\u2019s parameters have changed",
        "question": "What happens after the above snippet has been run?",
        "context": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d): ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What shows that the network's values are closer to 0?": {
        "answer": "examining the value ofl1\u2019sweightparameter",
        "question": "What shows that the network's values are closer to 0?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the article that explains how to train neural networks?": {
        "answer": "Using Optimizers",
        "question": "What is the name of the article that explains how to train neural networks?",
        "context": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the term for training neural networks?": {
        "answer": "Neural network training",
        "question": "What is the term for training neural networks?",
        "context": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of a module's dict that affects its computation?": {
        "answer": "state",
        "question": "What is the name of a module's dict that affects its computation?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is included in a module'sstate_dict?": {
        "answer": "module\u2019s parameters",
        "question": "What is included in a module'sstate_dict?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "For some modules, it may be useful to have state that affects module computation but is not learnable?": {
        "answer": "beyond parameters",
        "question": "For some modules, it may be useful to have state that affects module computation but is not learnable?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides the concept of buffers?": {
        "answer": "PyTorch",
        "question": "What provides the concept of buffers?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Parameters are what type of state a module can have?": {
        "answer": "learnable aspects of computation",
        "question": "Parameters are what type of state a module can have?",
        "context": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a motivating example for the use of buffers?": {
        "answer": "a simple module that maintains a running mean",
        "question": "What is a motivating example for the use of buffers?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What does this snippet show how to use to accomplish this?": {
        "answer": "register_buffer()",
        "question": "What does this snippet show how to use to accomplish this?",
        "context": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019sstate_dictso that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to useregister_buffer()to accomplish this: Now, the current value of the running mean is considered part of the module\u2019sstate_dictand will be properly restored when loading the module from disk: As mentioned previously, buffers can be left out of the module\u2019sstate_dictby marking them as non-persistent: Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied withto(): Buffers of a module can be iterated over usingbuffers()ornamed_buffers(). For more information, check out: Saving and loading:https://pytorch.org/tutorials/beginner/saving_loading_models.html Serialization semantics:https://pytorch.org/docs/master/notes/serialization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are called during the backward pass?": {
        "answer": "Backward hooks",
        "question": "What are called during the backward pass?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can backward hooks be installed?": {
        "answer": "withregister_full_backward_hook()",
        "question": "How can backward hooks be installed?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "When will backward hooks be called?": {
        "answer": "when the backward for this Module has been computed",
        "question": "When will backward hooks be called?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "How can backward hooks be installed for all modules?": {
        "answer": "globally",
        "question": "How can backward hooks be installed for all modules?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What do all hooks allow the user to return that will be used throughout the remaining computation?": {
        "answer": "updated value",
        "question": "What do all hooks allow the user to return that will be used throughout the remaining computation?",
        "context": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What can hooks be used to do without having to change the module'sforward()function?": {
        "answer": "modify some inputs/outputs",
        "question": "What can hooks be used to do without having to change the module'sforward()function?",
        "context": "All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function. ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What provides advanced features that are designed to work with modules?": {
        "answer": "PyTorch",
        "question": "What provides advanced features that are designed to work with modules?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What are the functionalities of PyTorch's advanced features called when writing a new module?": {
        "answer": "inherited",
        "question": "What are the functionalities of PyTorch's advanced features called when writing a new module?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Where can an in-depth discussion of PyTorch's advanced features be found?": {
        "answer": "links below",
        "question": "Where can an in-depth discussion of PyTorch's advanced features be found?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is the name of the feature that is inherited when writing a new module?": {
        "answer": "Profiling",
        "question": "What is the name of the feature that is inherited when writing a new module?",
        "context": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "Exporting modules to what is a good way to use C++?": {
        "answer": "TorchScript",
        "question": "Exporting modules to what is a good way to use C++?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What language is a good example of a language that can be exported to TorchScript?": {
        "answer": "C++",
        "question": "What language is a good example of a language that can be exported to TorchScript?",
        "context": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html ",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    "What is a multi-dimensional matrix containing elements of a single data type?": {
        "answer": "Atorch.Tensoris",
        "question": "What is a multi-dimensional matrix containing elements of a single data type?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many tensor types does Torch define?": {
        "answer": "10 tensor types with CPU and GPU variants",
        "question": "How many tensor types does Torch define?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?": {
        "answer": "Data type dtype",
        "question": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a dtype CPU tensor GPU tensor?": {
        "answer": "32-bit floating point torch",
        "question": "What type of torch is a dtype CPU tensor GPU tensor?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a GPU tensor?": {
        "answer": "32-bit floating point torch",
        "question": "What type of torch is a GPU tensor?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is included in a BFloat16Tensor 32-bit complex torch?": {
        "answer": "8-bit",
        "question": "What type of complex torch is included in a BFloat16Tensor 32-bit complex torch?",
        "context": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is float32ortorch?": {
        "answer": "torch",
        "question": "What type of torch is float32ortorch?",
        "context": "torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits does a complex torch have?": {
        "answer": "8",
        "question": "How many bits does a complex torch have?",
        "context": "torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a double torch?": {
        "answer": "64-bit floating point torch",
        "question": "What type of torch is a double torch?",
        "context": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a double torch?": {
        "answer": "torch.float64ortorch",
        "question": "What is a double torch?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the floating point of a double-tensor torch?": {
        "answer": "16-bit",
        "question": "What is the floating point of a double-tensor torch?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is float16ortorch?": {
        "answer": "16-bit floating point1 torch",
        "question": "What type of torch is float16ortorch?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a half torch?": {
        "answer": "16-bit floating point1 torch",
        "question": "What is a half torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a HalfTensor torch?": {
        "answer": "half torch",
        "question": "What type of torch is a HalfTensor torch?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits does a BFloat16Tensor have?": {
        "answer": "32",
        "question": "How many bits does a BFloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a HalfTensor?": {
        "answer": "16-bit floating point2 torch",
        "question": "What type of torch is a HalfTensor?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a bfloat16 torch?": {
        "answer": "16-bit floating point2 torch",
        "question": "What type of torch is a bfloat16 torch?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is the BFloat16Tensor 32-bit complex torch?": {
        "answer": "64-bit",
        "question": "What type of complex torch is the BFloat16Tensor 32-bit complex torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits are in a complex torch?": {
        "answer": "64",
        "question": "How many bits are in a complex torch?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the most common type of complex torch?": {
        "answer": "32-bit",
        "question": "What is the most common type of complex torch?",
        "context": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of complex torch is the complex64 128-bit complex torch?": {
        "answer": "64-bit",
        "question": "What type of complex torch is the complex64 128-bit complex torch?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a bool torch?": {
        "answer": "Boolean torch",
        "question": "What type of torch is a bool torch?",
        "context": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits is a complex torch?": {
        "answer": "128",
        "question": "How many bits is a complex torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of integer is a torch?": {
        "answer": "8-bit",
        "question": "What type of integer is a torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a ByteTensor?": {
        "answer": "8-bit integer",
        "question": "What is a ByteTensor?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a ByteTensor?": {
        "answer": "8-bit integer",
        "question": "What type of torch is a ByteTensor?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a CharTensor torch?": {
        "answer": "8-bit integer",
        "question": "What type of torch is a CharTensor torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bits is a CharTensor?": {
        "answer": "16",
        "question": "How many bits is a CharTensor?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of integer is a shortTensor?": {
        "answer": "32-bit",
        "question": "What type of integer is a shortTensor?",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of integer is a short torch?": {
        "answer": "16-bit",
        "question": "What type of integer is a short torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a shorttensor?": {
        "answer": "32-bit integer",
        "question": "What is a shorttensor?",
        "context": "torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the sign of the IntTensor 6?": {
        "answer": "4-bit integer",
        "question": "What is the sign of the IntTensor 6?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many bit integers are in an IntTensor?": {
        "answer": "4",
        "question": "How many bit integers are in an IntTensor?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of torch is a torch?": {
        "answer": "32-bit integer",
        "question": "What type of torch is a torch?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What programming language can be used to access and modify the contents of a tensor?": {
        "answer": "Python",
        "question": "What programming language can be used to access and modify the contents of a tensor?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to get a Python number from a tensor containing a single value?": {
        "answer": "Usetorch.Tensor.item()",
        "question": "What is used to get a Python number from a tensor containing a single value?",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How can a tensor be created?": {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation",
        "question": "How can a tensor be created?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What holds each tensor's data?": {
        "answer": "associatedtorch.Storage",
        "question": "What holds each tensor's data?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the tensor class provide?": {
        "answer": "multi-dimensional,stridedview of a storage",
        "question": "What does the tensor class provide?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?": {
        "answer": "Note",
        "question": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "For more information on tensor views, see what?": {
        "answer": "Views",
        "question": "For more information on tensor views, see what?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is another name for tensor views?": {
        "answer": "Note",
        "question": "What is another name for tensor views?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you do when creating a tensor?": {
        "answer": "Note",
        "question": "What do you do when creating a tensor?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor class that holds its data?": {
        "answer": "Note",
        "question": "What is the name of the tensor class that holds its data?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Methods that mutate a tensor are marked with what?": {
        "answer": "underscore suffix",
        "question": "Methods that mutate a tensor are marked with what?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the result oftorch.FloatTensor.abs()?": {
        "answer": "a new tensor",
        "question": "What is the result oftorch.FloatTensor.abs()?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a suffix for a method that mutates a tensor?": {
        "answer": "Note",
        "question": "What is a suffix for a method that mutates a tensor?",
        "context": "For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Methods which mutate a tensor are marked with what?": {
        "answer": "underscore suffix",
        "question": "Methods which mutate a tensor are marked with what?",
        "context": "For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What doestorch.FloatTensor.abs() return?": {
        "answer": "a new tensor",
        "question": "What doestorch.FloatTensor.abs() return?",
        "context": "For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What are some of the attributes of atorch.Tensor?": {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What are some of the attributes of atorch.Tensor?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What suffix is used to mark methods that mutate a tensor?": {
        "answer": "underscore",
        "question": "What suffix is used to mark methods that mutate a tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What doestorch.FloatTensor.abs()compute the result in?": {
        "answer": "a new tensor",
        "question": "What doestorch.FloatTensor.abs()compute the result in?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What method is used to change an existing tensor'storch.deviceand/ortorch.dtype?": {
        "answer": "to()method",
        "question": "What method is used to change an existing tensor'storch.deviceand/ortorch.dtype?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a warning about usingto() method on a tensor?": {
        "answer": "Warning",
        "question": "What is a warning about usingto() method on a tensor?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a warning about a method that mutates a tensor?": {
        "answer": "Warning",
        "question": "What is a warning about a method that mutates a tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Methods that mutate a tensor are marked with what suffix?": {
        "answer": "underscore",
        "question": "Methods that mutate a tensor are marked with what suffix?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is a warning?": {
        "answer": "Warning",
        "question": "What is a warning?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What kind of memory usage could be caused by the current implementation oftorch.Tensor?": {
        "answer": "unexpectedly high",
        "question": "What kind of memory usage could be caused by the current implementation oftorch.Tensor?",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What should you use if you have a lot of tiny tensors?": {
        "answer": "one large structure",
        "question": "What should you use if you have a lot of tiny tensors?",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "How many ways to create a tensor?": {
        "answer": "a few main ways to create a tensor",
        "question": "How many ways to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch.tensor() do to create a tensor with?": {
        "answer": "pre-existing data",
        "question": "What does usetorch.tensor() do to create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you need to create a tensor with?": {
        "answer": "specific size",
        "question": "What do you need to create a tensor with?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does usetorch to create a tensor with specific size?": {
        "answer": "*tensor creation ops",
        "question": "What does usetorch to create a tensor with specific size?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the best way to create a tensor with the same size as another tensor?": {
        "answer": "usetorch",
        "question": "What is the best way to create a tensor with the same size as another tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What do you use to create a tensor with the same size as another tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What do you use to create a tensor with the same size as another tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with similar type but different size as another tensor?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is used to create a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is reversed in a tensor?": {
        "answer": "its dimensions",
        "question": "What is reversed in a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with pre-existing data?": {
        "answer": "usetorch.tensor()",
        "question": "What is used to create a tensor with pre-existing data?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with specific size?": {
        "answer": "*tensor creation ops",
        "question": "What is used to create a tensor with specific size?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with the same size as another tensor?": {
        "answer": "*_liketensor creation ops",
        "question": "What is used to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Is this Tensor with what?": {
        "answer": "its dimensions reversed",
        "question": "Is this Tensor with what?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of dimensions inx,x.Tis equivalent to?": {
        "answer": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute",
        "question": "What is the number of dimensions inx,x.Tis equivalent to?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Is this Tensor with its dimensions reversed or reversed?": {
        "answer": "reversed",
        "question": "Is this Tensor with its dimensions reversed or reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with similar type but different size?": {
        "answer": "Tensor.new_tensor",
        "question": "What is used to create a tensor with similar type but different size?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is used to create a tensor with the same type but different size as another tensor?": {
        "answer": "usetensor.new_*creation ops",
        "question": "What is used to create a tensor with the same type but different size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of dimensions inx,x.Tis equivalent to tox.permute(n-1,n-2,...,0": {
        "answer": "Ifnis",
        "question": "What is the number of dimensions inx,x.Tis equivalent to tox.permute(n-1,n-2,...,0",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_tensor return as the tensor data?": {
        "answer": "data",
        "question": "What does Tensor.new_tensor return as the tensor data?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_full return?": {
        "answer": "Tensor of sizesizefilled withfill_value",
        "question": "What does Tensor.new_full return?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a Tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor",
        "question": "What returns a Tensor of sizesizefilled withfill_value?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Is the tensor with its dimensions reversed or reversed?": {
        "answer": "reversed",
        "question": "Is the tensor with its dimensions reversed or reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_empty return a Tensor of sizesizefilled with?": {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_empty return a Tensor of sizesizefilled with?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_ones Returns a Tensor of sizesizefilled with what value?": {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled with what value?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is returned as the tensor data?": {
        "answer": "data",
        "question": "What is returned as the tensor data?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_zeros return?": {
        "answer": "a Tensor of sizesizefilled with0",
        "question": "What does Tensor.new_zeros return?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Tensor with its dimensions reversed?": {
        "answer": "Tensor.is_cuda",
        "question": "What is the name of the Tensor with its dimensions reversed?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the number of dimensions inx,x?": {
        "answer": "Ifnis",
        "question": "What is the number of dimensions inx,x?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What data does the new Tensor return?": {
        "answer": "tensor data",
        "question": "What data does the new Tensor return?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor stored?": {
        "answer": "GPU",
        "question": "Where is the Tensor stored?",
        "context": "IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is true if the Tensor is stored on the GPU?": {
        "answer": "quantized",
        "question": "What is true if the Tensor is stored on the GPU?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the new Tensor return withdataas?": {
        "answer": "tensor data",
        "question": "What does the new Tensor return withdataas?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_full return a Tensor of?": {
        "answer": "sizesizefilled withfill_value",
        "question": "What does Tensor.new_full return a Tensor of?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros Returns a Tensor of sizesizefilled with what value?": {
        "answer": "0",
        "question": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what value?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Returns a new Tensor withdataas what?": {
        "answer": "tensor data",
        "question": "Returns a new Tensor withdataas what?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.is_what IsTrueif the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "Tensor.is_what IsTrueif the Tensor is a meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the device that returns a tensor of sizesizefilled withfill_value?": {
        "answer": "Tensor.device",
        "question": "What is the name of the device that returns a tensor of sizesizefilled withfill_value?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Returns a Tensor of what?": {
        "answer": "sizesizefilled withfill_value",
        "question": "Returns a Tensor of what?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is true if the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What is true if the Tensor is a meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_ones Returns a Tensor of sizesizefilled with what?": {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is thetorch.devicewhere the Tensor is?": {
        "answer": "Tensor.grad",
        "question": "What is thetorch.devicewhere the Tensor is?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does Tensor.new_ones return a Tensor of sizesizefilled with?": {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_ones return a Tensor of sizesizefilled with?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is thetorch.devicewhere this Tensor is?": {
        "answer": "Tensor.grad",
        "question": "What is thetorch.devicewhere this Tensor is?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the size of the Tensor of sizesizefilled with?": {
        "answer": "0",
        "question": "What is the size of the Tensor of sizesizefilled with?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "When a Tensor.grad isNoneby default, it becomes a Tensor the first time a call to which function compute": {
        "answer": "tobackward()",
        "question": "When a Tensor.grad isNoneby default, it becomes a Tensor the first time a call to which function compute",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?": {
        "answer": "0",
        "question": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor?": {
        "answer": "thetorch.device",
        "question": "Where is the Tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the attribute that becomes a Tensor the first time a call tobackward()computes gradients forself": {
        "answer": "Tensor.ndim",
        "question": "What is the name of the attribute that becomes a Tensor the first time a call tobackward()computes gradients forself",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the size of the Tensor returned by Tensor.new_zeros?": {
        "answer": "sizesizefilled with0",
        "question": "What is the size of the Tensor returned by Tensor.new_zeros?",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the first time a Tensor becomes a Tensor?": {
        "answer": "tobackward()computes gradients forself",
        "question": "What is the first time a Tensor becomes a Tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the Alias fordim?": {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What is the name of the Alias fordim?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the size of the Tensor?": {
        "answer": "sizesizefilled with0",
        "question": "What is the size of the Tensor?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is true if the Tensor is quantized?": {
        "answer": "Tensor.is",
        "question": "What is true if the Tensor is quantized?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a new tensor containing real values of theselftensor?": {
        "answer": "Tensor",
        "question": "What returns a new tensor containing real values of theselftensor?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.real Returns a new tensor containing what?": {
        "answer": "real values of theselftensor",
        "question": "Tensor.real Returns a new tensor containing what?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Tensor.imag Returns a new tensor containing what of theselftensor?": {
        "answer": "imaginary values",
        "question": "Tensor.imag Returns a new tensor containing what of theselftensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What returns a new tensor containing imaginary values of theselftensor?": {
        "answer": "Tensor.abs",
        "question": "What returns a new tensor containing imaginary values of theselftensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "IsTrueif the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "IsTrueif the Tensor is a meta tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does imag return a new tensor containing?": {
        "answer": "imaginary values",
        "question": "What does imag return a new tensor containing?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What function returns a new tensor containing imaginary values of theselftensor?": {
        "answer": "Seetorch.abs()",
        "question": "What function returns a new tensor containing imaginary values of theselftensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor is true if the Tensor is a meta tensor?": {
        "answer": "meta",
        "question": "What type of tensor is true if the Tensor is a meta tensor?",
        "context": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What Returns a new tensor containing real values of theselftensor?": {
        "answer": "real",
        "question": "What Returns a new tensor containing real values of theselftensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is olute?": {
        "answer": "In-place version ofabs() Tensor.abs",
        "question": "What is olute?",
        "context": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "IsTrue if the Tensor is a what?": {
        "answer": "meta tensor",
        "question": "IsTrue if the Tensor is a what?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the function that returns the absolute value of the Tensor?": {
        "answer": "Alias forabs",
        "question": "What is the name of the function that returns the absolute value of the Tensor?",
        "context": "IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor that returns a new tensor containing real values of theselftensor?": {
        "answer": "Alias",
        "question": "What is the name of the tensor that returns a new tensor containing real values of theselftensor?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Where is the Tensor located?": {
        "answer": "thetorch.device",
        "question": "Where is the Tensor located?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the function that returns the Tensor.absolute?": {
        "answer": "Alias forabs_()",
        "question": "What is the name of the function that returns the Tensor.absolute?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Who is forabs in Tensor.absolute?": {
        "answer": "Alias",
        "question": "Who is forabs in Tensor.absolute?",
        "context": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "When does this attribute become a Tensor?": {
        "answer": "tobackward()computes gradients forself",
        "question": "When does this attribute become a Tensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "Who forabs() Tensor.absolute Returns a new tensor containing real values of theselftensor?": {
        "answer": "Alias",
        "question": "Who forabs() Tensor.absolute Returns a new tensor containing real values of theselftensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the imag return a new tensor containing?": {
        "answer": "imaginary values of theselftensor",
        "question": "What does the imag return a new tensor containing?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What does the new tensor contain?": {
        "answer": "real values of theselftensor",
        "question": "What does the new tensor contain?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What type of tensor can be added to a selftensor?": {
        "answer": "scalar",
        "question": "What type of tensor can be added to a selftensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the name of the tensor that adds a scalar or tensor toselftensor?": {
        "answer": "Tensor.addbmm",
        "question": "What is the name of the tensor that adds a scalar or tensor toselftensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    "What is the order of a n-D tensor along a given axis in?": {
        "answer": "dims",
        "question": "What is the order of a n-D tensor along a given axis in?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What is torch.flip expected to be?": {
        "answer": "slower thannp.flip",
        "question": "What is torch.flip expected to be?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What does dims(a listortuple) flip on?": {
        "answer": "axis",
        "question": "What does dims(a listortuple) flip on?",
        "context": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    "What returns the reduced singular value decomposition?": {
        "answer": "IfsomeisTrue",
        "question": "What returns the reduced singular value decomposition?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "If the last two dimensions ofinputaremandn, the returnedUandVmatrices will contain what?": {
        "answer": "onlymin(n, m)orthonormal columns",
        "question": "If the last two dimensions ofinputaremandn, the returnedUandVmatrices will contain what?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What happens whencompute_uvisFalse?": {
        "answer": "argumentsomehas no effect",
        "question": "What happens whencompute_uvisFalse?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What types of data types does torch support?": {
        "answer": "float, double, cfloat and cdouble data types",
        "question": "What types of data types does torch support?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The dtypes ofUandVare the same asinput's.Swill always be what?": {
        "answer": "real-valued",
        "question": "The dtypes ofUandVare the same asinput's.Swill always be what?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is deprecated in favor oftorch.linalg.svd()?": {
        "answer": "Warning torch.svd()is",
        "question": "What is deprecated in favor oftorch.linalg.svd()?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What should be replaced with torch.linalg.svd()?": {
        "answer": "Note Differences",
        "question": "What should be replaced with torch.linalg.svd()?",
        "context": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd() return?": {
        "answer": "torch.svd()returnsV",
        "question": "What does torch.svd() return?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does torch.svd() return ifcompute_uvisFalse?": {
        "answer": "zero-filled tensors forUandVh",
        "question": "What does torch.svd() return ifcompute_uvisFalse?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "In what order are the singular values returned?": {
        "answer": "descending order",
        "question": "In what order are the singular values returned?",
        "context": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the batch of matrices that returns the singular values of each matrix in the batch in descending order?": {
        "answer": "Ifinputis",
        "question": "What is the name of the batch of matrices that returns the singular values of each matrix in the batch in descending order?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The Tensor can only be used to do what?": {
        "answer": "compute gradients ifcompute_uvisTrue",
        "question": "The Tensor can only be used to do what?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does theStensor can only be used to compute gradients ifcompute_uvisTrue?": {
        "answer": "Note",
        "question": "What does theStensor can only be used to compute gradients ifcompute_uvisTrue?",
        "context": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "Ifcompute_uvisFalse, what does torch.svd() return?": {
        "answer": "zero-filled tensors",
        "question": "Ifcompute_uvisFalse, what does torch.svd() return?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What can TheStensor be used to compute gradients ifcompute_uvisTrue?": {
        "answer": "Note",
        "question": "What can TheStensor be used to compute gradients ifcompute_uvisTrue?",
        "context": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When someisFalse, the gradients onU[..., :, min(m, n):]andV[...": {
        "answer": "the backward pass",
        "question": "When someisFalse, the gradients onU[..., :, min(m, n):]andV[...",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the algorithm used in the implementation oftorch.linalg.svd() on CPU?": {
        "answer": "LAPACK",
        "question": "What is the name of the algorithm used in the implementation oftorch.linalg.svd() on CPU?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What program uses routinegesddon earlier versions of CUDA?": {
        "answer": "MAGMA",
        "question": "What program uses routinegesddon earlier versions of CUDA?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What does the implementation oftorch.linalg.svd()on GPU use?": {
        "answer": "Note",
        "question": "What does the implementation oftorch.linalg.svd()on GPU use?",
        "context": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "The gradients with respect toUandVwill only be what when the input does not have zero or repeated singular values?": {
        "answer": "finite",
        "question": "The gradients with respect toUandVwill only be what when the input does not have zero or repeated singular values?",
        "context": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be what?": {
        "answer": "numerically unstable",
        "question": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be what?",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When the matrix has what, the gradients also depend onS1?": {
        "answer": "small singular values",
        "question": "When the matrix has what, the gradients also depend onS1?",
        "context": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values is close to": {
        "answer": "Warning",
        "question": "What is the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values is close to",
        "context": "Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the warning that the gradients with respect toUandVwill be numerically unstable when the distance between any two singular values is close to zero": {
        "answer": "Warning",
        "question": "What is the warning that the gradients with respect toUandVwill be numerically unstable when the distance between any two singular values is close to zero",
        "context": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What will be numerically unstable if the distance between any two singular values is close to zero?": {
        "answer": "the gradients with respect toUandV",
        "question": "What will be numerically unstable if the distance between any two singular values is close to zero?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is a warning when the distance between any two singular values is close to zero?": {
        "answer": "Warning",
        "question": "What is a warning when the distance between any two singular values is close to zero?",
        "context": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the name of the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values": {
        "answer": "Warning",
        "question": "What is the name of the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "For what type of input is the singular value decomposition not unique?": {
        "answer": "complex-valuedinput",
        "question": "For what type of input is the singular value decomposition not unique?",
        "context": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "When does the same happen for complex-valuedinput?": {
        "answer": "wheninputhas repeated singular values",
        "question": "When does the same happen for complex-valuedinput?",
        "context": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What may produce differentUandVtensors?": {
        "answer": "Different platforms",
        "question": "What may produce differentUandVtensors?",
        "context": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    "What is the function that converts a DLPack to a tensor?": {
        "answer": "Decodes a DLPack to a tensor",
        "question": "What is the function that converts a DLPack to a tensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is the dlpack?": {
        "answer": "PyCapsule object",
        "question": "What is the dlpack?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "How many times can a dlpack be consumed?": {
        "answer": "once",
        "question": "How many times can a dlpack be consumed?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What represents the tensor?": {
        "answer": "a DLPack",
        "question": "What represents the tensor?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is the name of the tensor to be exported?": {
        "answer": "tensor",
        "question": "What is the name of the tensor to be exported?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "How many times can each dlpack be consumed?": {
        "answer": "once",
        "question": "How many times can each dlpack be consumed?",
        "context": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once. ",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    "What is a float tensor converted to?": {
        "answer": "per-channel quantized tensor",
        "question": "What is a float tensor converted to?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What type of tensor is input(Tensor)?": {
        "answer": "float 1D tensor",
        "question": "What type of tensor is input(Tensor)?",
        "context": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What does input(Tensor) use to quantize scales?": {
        "answer": "float tensor",
        "question": "What does input(Tensor) use to quantize scales?",
        "context": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    "What is the function that computes the eigenvalues and eigenvectors of a real square matrix?": {
        "answer": "Computes the eigenvalues and eigenvectors of a real square matrix",
        "question": "What is the function that computes the eigenvalues and eigenvectors of a real square matrix?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is supported only if eigenvalues and eigenvectors are all real valued?": {
        "answer": "backward pass",
        "question": "What is supported only if eigenvalues and eigenvectors are all real valued?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Wheninputis on CUDA,torch.eig()causes what?": {
        "answer": "host-device synchronization",
        "question": "Wheninputis on CUDA,torch.eig()causes what?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is deprecated in favor oftorch.linalg.eig()?": {
        "answer": "torch.eig()",
        "question": "What is deprecated in favor oftorch.linalg.eig()?",
        "context": "torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Backward pass is supported only if eigenvalues and eigenvectors are all real valued?": {
        "answer": "eigenvalues and eigenvectors",
        "question": "Backward pass is supported only if eigenvalues and eigenvectors are all real valued?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What should be replaced with something else?": {
        "answer": "L,_=torch.eig(A)",
        "question": "What should be replaced with something else?",
        "context": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What does torch.eig() cause wheninputis on CUDA?": {
        "answer": "host-device synchronization",
        "question": "What does torch.eig() cause wheninputis on CUDA?",
        "context": "Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What should be replaced with L,_=torch.eig(A)?": {
        "answer": "L,V=torch.eig(A,eigenvectors=True)",
        "question": "What should be replaced with L,_=torch.eig(A)?",
        "context": "Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What should be replaced with L,V=torch.eig(A,eigenvectors=True)?": {
        "answer": "L,_=torch.eig(A)",
        "question": "What should be replaced with L,V=torch.eig(A,eigenvectors=True)?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the square matrix of shape?": {
        "answer": "input(Tensor)",
        "question": "What is the square matrix of shape?",
        "context": "input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is a namedtuple containing eigenvalues, eigenvectors?": {
        "answer": "output tensors",
        "question": "What is a namedtuple containing eigenvalues, eigenvectors?",
        "context": "eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the first element of an eigenvalue of input?": {
        "answer": "the first element is the real part and the second element is the imaginary part",
        "question": "What is the first element of an eigenvalue of input?",
        "context": "eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Are eigenvalues ordered or ordered?": {
        "answer": "not necessarily ordered",
        "question": "Are eigenvalues ordered or ordered?",
        "context": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Out(tuple,optional) - the output what?": {
        "answer": "tensors",
        "question": "Out(tuple,optional) - the output what?",
        "context": "out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the real part of an eigenvalue of input?": {
        "answer": "the first element is the real part and the second element is the imaginary part",
        "question": "What is the real part of an eigenvalue of input?",
        "context": "out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "Are eigenvalues ordered?": {
        "answer": "not necessarily",
        "question": "Are eigenvalues ordered?",
        "context": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is a namedtuple containing eigenvalues(Tensor)?": {
        "answer": "Shape",
        "question": "What is a namedtuple containing eigenvalues(Tensor)?",
        "context": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the second element of an eigenvalue of input?": {
        "answer": "imaginary part",
        "question": "What is the second element of an eigenvalue of input?",
        "context": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is the eigenvalue of input?": {
        "answer": "the first element is the real part and the second element is the imaginary part",
        "question": "What is the eigenvalue of input?",
        "context": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    "What is another name for Tensor?": {
        "answer": "Tensor",
        "question": "What is another name for Tensor?",
        "context": "(Tensor,Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    }
}