[
    {
        "X": "How many dimensions does each input tensor have?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "zero"
    },
    {
        "X": "What dimension is the view of each input tensor with zero dimensions?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "3-dimensional"
    },
    {
        "X": "Input tensors with how many dimensions are returned as-is?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "three or more"
    },
    {
        "X": "What is another name for a tuple of Tensors?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "output"
    },
    {
        "X": "What is returned of each input tensor with zero dimensions?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "a 3-dimensional view"
    },
    {
        "X": "How are input tensors with three or more dimensions returned?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "as-is"
    },
    {
        "X": "What is the output of a tuple of Tensors?",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "Y": "output"
    },
    {
        "X": "What does the input tensor do?",
        "Z": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "Y": "Computes the bitwise XOR"
    },
    {
        "X": "The input tensor must be of what types?",
        "Z": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "Y": "integral or Boolean types"
    },
    {
        "X": "For what type of tensor does it compute the logical XOR?",
        "Z": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "Y": "bool tensors"
    },
    {
        "X": "What is the second input tensor out?",
        "Z": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "Y": "output tensor"
    },
    {
        "X": "What is the output tensor?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. ",
        "Y": "Computes the base two exponential function of input"
    },
    {
        "X": "Returns a new tensor with what of the elements of input?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "logit"
    },
    {
        "X": "What is the input tensor?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "1-D"
    },
    {
        "X": "Out (Tensor, optional) - what is the output tensor?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "output tensor"
    },
    {
        "X": "What type of tensor is the output tensor?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "optional"
    },
    {
        "X": "What may use the Sleef library when input is on the CPU?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "torch.sinh"
    },
    {
        "X": "Where can you find details about the Sleef library?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "here"
    },
    {
        "X": "Returns what with the hyperbolic sine of the elements of input?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "a new tensor"
    },
    {
        "X": "When input is on the CPU, the implementation of torch.sinh may use what library?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "Sleef library"
    },
    {
        "X": "What does the Sleef library provide?",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "Y": "details"
    },
    {
        "X": "What does a tensor do when a dimension is not specified?",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "Roll the tensor along the given dimension(s)."
    },
    {
        "X": "What is re-introduced at the first position?",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "Elements that are shifted beyond the last position"
    },
    {
        "X": "If a dimension is not specified, the tensor will be what?",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "flattened before rolling"
    },
    {
        "X": "What does the tensor do along the given dimension(s)?",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "Roll the tensor"
    },
    {
        "X": "What is shifts a tuple of?",
        "Z": "input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "python:ints"
    },
    {
        "X": "If shifts is a tuple, dims must be what?",
        "Z": "input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "a tuple of the same size"
    },
    {
        "X": "What is the number of places by which the elements of the tensor are shifted?",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "shifts"
    },
    {
        "X": "If shifts is what, dims must be a tuple of the same size?",
        "Z": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "Y": "tuple"
    },
    {
        "X": "What is constructed by repeating the elements of input?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "Y": "tensor"
    },
    {
        "X": "What specifies the number of repetitions in each dimension?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "The reps argument"
    },
    {
        "X": "If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified?",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "Y": "fewer"
    },
    {
        "X": "If input has shape (8, 6, 4, 2) and reps is (2, 2), reps is treated as what?",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "Y": "(1, 1, 2, 2)."
    },
    {
        "X": "How is a tensor constructed?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "by repeating the elements of input"
    },
    {
        "X": "If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified.",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "fewer"
    },
    {
        "X": "What are reps treated as if input has shape and reps is 2?",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "Y": "(1, 1, 2, 2)"
    },
    {
        "X": "What specifies fewer dimensions than input has?",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "Y": "reps"
    },
    {
        "X": "What is the size of the input tensor along a given dimension?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "Y": "k largest elements"
    },
    {
        "X": "If what is not given, the last dimension of the input is chosen?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "dim"
    },
    {
        "X": "If largest is what, the smallest elements of the input tensor are returned?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "False"
    },
    {
        "X": "What is returned, where the indices are the indices of the elements in the original input tensor?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "A namedtuple of (values, indices)"
    },
    {
        "X": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted?",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "input (Tensor)"
    },
    {
        "X": "What is the k in \"top-k\"?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "Y": "k"
    },
    {
        "X": "Returns what of the given input tensor along a given dimension?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "k largest elements"
    },
    {
        "X": "If largest is what, the k smallest elements are returned?",
        "Z": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along ",
        "Y": "False"
    },
    {
        "X": "What returns the indices of the elements in the original input tensor?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "Y": "A namedtuple of (values, indices)"
    },
    {
        "X": "The boolean option sorted if True, will make sure that the returned k elements are themselves what?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "sorted input"
    },
    {
        "X": "What is the k in \u201ctop-k\u201d?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "Y": "k (int)"
    },
    {
        "X": "If largest is what, the smallest elements are returned?",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "Y": "False"
    },
    {
        "X": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted input (Tensor)",
        "Z": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along ",
        "Y": "input tensor"
    },
    {
        "X": "What is the k in \"top-k\" dim?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "k"
    },
    {
        "X": "What is chosen if dim is not given?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "the last dimension of the input"
    },
    {
        "X": "What is returned if largest is False?",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "Y": "A namedtuple of"
    },
    {
        "X": "What is the dimension to sort along with the input tensor?",
        "Z": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along ",
        "Y": "k"
    },
    {
        "X": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted what?",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "Y": "input (Tensor)"
    },
    {
        "X": "Which dimension controls whether to return largest or smallest elements?",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "Y": "k"
    },
    {
        "X": "What are returned if largest is False?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "k smallest elements"
    },
    {
        "X": "A namedtuple of (values, indices, etc.) is returned, where the indices are the indices of",
        "Z": "A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order ",
        "Y": "indices"
    },
    {
        "X": "What option makes sure that the returned k elements are themselves sorted input?",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "boolean"
    },
    {
        "X": "What is returned where the indices are the indices of the elements in the original input tensor?",
        "Z": "A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order ",
        "Y": "A namedtuple of (values, indices)"
    },
    {
        "X": "The dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or smallest elements sorted (bool, optional)",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "k"
    },
    {
        "X": "The boolean option sorted if True, will ma what?",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "Y": "k"
    },
    {
        "X": "What type of convolution is applied over an input signal composed of several input planes?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "1D convolution"
    },
    {
        "X": "What type of convolution is applied over an input image composed of several input planes?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "2D convolution"
    },
    {
        "X": "What is another name for a 1D transposed convolution operator over an input signal composed of several input planes?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "deconvolution"
    },
    {
        "X": "What type of convolution over an input image composed of several input planes?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "3D"
    },
    {
        "X": "What type of operator is applied over an input signal composed of several input planes?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   ",
        "Y": "1D transposed convolution operator"
    },
    {
        "X": "What type of operator is used over an input image composed of several input planes?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "2D transposed convolution operator"
    },
    {
        "X": "What is another name for a 2D transposed convolution operator?",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   ",
        "Y": "deconvolution"
    },
    {
        "X": "Applies a 3D convolution over an input image composed of what?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   ",
        "Y": "several input planes"
    },
    {
        "X": "What is a 3D transposed convolution operator over an input image composed of several input planes sometimes called?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   ",
        "Y": "deconvolution"
    },
    {
        "X": "What type of transposed convolution operator is used over an input image composed of several input planes?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "2D"
    },
    {
        "X": "What type of input tensor is used to extract sliding local blocks?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   ",
        "Y": "batched input tensor"
    },
    {
        "X": "What is another name for a 1D transposed convolution operator?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   ",
        "Y": "deconvolution"
    },
    {
        "X": "What is another name for a 3D transposed convolution operator?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "Y": "deconvolution"
    },
    {
        "X": "What type of operator is applied over an input image composed of several input planes?",
        "Z": "  Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "2D transposed convolution operator"
    },
    {
        "X": "What type of input tensor is the input tensor?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "batched"
    },
    {
        "X": "What does a 3D transposed convolution operator do?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "Y": "Combines an array of sliding local blocks into a large containing tensor"
    },
    {
        "X": "What is applied over an input signal composed of several input planes?",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   ",
        "Y": "1D average pooling"
    },
    {
        "X": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called what?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "deconvolution"
    },
    {
        "X": "What is combined an array of sliding local blocks into?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "large containing tensor"
    },
    {
        "X": "What type of pooling is applied over an input signal composed of several input planes?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "2D max pooling"
    },
    {
        "X": "What type of operation is applied in kHkWkH times kWkHkW regions?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   ",
        "Y": "2D average-pooling operation"
    },
    {
        "X": "What type of operation is applied in kHkWkH times kWkHkW regions by step size?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   ",
        "Y": "2D average-pooling operation"
    },
    {
        "X": "What does this do?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device"
    },
    {
        "X": "What type of average-pooling operation is applied in kHkWkH times kWkHkW regions?",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   ",
        "Y": "2D"
    },
    {
        "X": "What is applied in kTkHkWkT times kH times kWkTk",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "3D average-pooling operation"
    },
    {
        "X": "Applies what over an input signal composed of several input planes?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "1D average pooling"
    },
    {
        "X": "What type of operation does kHkWkH times kWkHkW regions by step size?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "Y": "2D average-pooling"
    },
    {
        "X": "What type of operation is applied in kTkHkWkT times kH times kWk",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   ",
        "Y": "3D average-pooling"
    },
    {
        "X": "What is a large containing?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "Y": "tensor"
    },
    {
        "X": "What type of average-pooling operation does kHkWkH times kWkHkW regions by step size?",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "Y": "2D"
    },
    {
        "X": "What does kTkHkWkT times kH times kWkTkH",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "3D average-pooling operation"
    },
    {
        "X": "What type of average-pooling operation does kTkHkWkT times kH times ",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   ",
        "Y": "3D"
    },
    {
        "X": "What type of pooling operation does kHkWkH times kWkHkW regions by step size?",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "Y": "2D average-pooling"
    },
    {
        "X": "What operation does kHkWkH times kWkHkW regions by step size sHsWs",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "2D average-pooling"
    },
    {
        "X": "Applies a 2D max pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "Applies a 3D max pooling over an input signal composed of what?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What is the max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "3D"
    },
    {
        "X": "Computes a partial inverse of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "MaxPool3d"
    },
    {
        "X": "What operation does kTkHkWkT times kH times kWkTkH",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "3D average-pooling"
    },
    {
        "X": "A 3D max pooling over an input signal composed of what?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 3D max pooling over an input signal composed of several input planes do?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What does the 3D pooling over an input signal consist of several input planes do?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "What does a 3D pooling over an input signal consist of several input planes do?",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What does a partial inverse of MaxPool1d do?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "What does a partial inverse of MaxPool2d do?",
        "Z": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What is the pooling over an input signal composed of several input planes?",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "1D power-average"
    },
    {
        "X": "What type of max pooling is applied over an input signal composed of several input planes?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "3D"
    },
    {
        "X": "What does the 3D max pooling over an input signal consist of?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What is the result of Computes a partial inverse of MaxPool3d?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What does a 3D max pooling over an input signal consist of several input planes do?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "A 2D power-average pooling over an input signal composed of what?",
        "Z": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "What is the adaptive max pooling over an input signal composed of several input planes?",
        "Z": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "1D"
    },
    {
        "X": "What does the 3D max pooling over an input signal consist of several input planes do?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What is the result of the 3D max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "What is the power-average pooling over an input signal composed of several input planes?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "1D"
    },
    {
        "X": "An array of sliding local blocks is combined into a large containing what?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "tensor"
    },
    {
        "X": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes called what?",
        "Z": "  Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "deconvolution"
    },
    {
        "X": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called what?",
        "Z": "  Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "deconvolution"
    },
    {
        "X": "A 2D adaptive max pooling over an input signal composed of what?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What does MaxPool1d do?",
        "Z": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse"
    },
    {
        "X": "What does Computes a partial inverse of MaxPool2d?",
        "Z": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "What does Computes a partial inverse of MaxPool3d?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "What type of power-average pooling over an input signal composed of several input planes?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "2D"
    },
    {
        "X": "Applies a 1D adaptive max pooling over an input signal composed of what?",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "Applies a 2D adaptive max pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "What does MaxPool2d do?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What does Compute a partial inverse of MaxPool3d do?",
        "Z": "Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What does MaxPool3d apply over an input signal composed of several input planes?",
        "Z": "Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "3D adaptive max pooling"
    },
    {
        "X": "What type of adaptive max pooling over an input signal composed of several input planes?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "1D"
    },
    {
        "X": "What does MaxPool3d do?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "A 1D adaptive max pooling over an input signal composed of what?",
        "Z": "Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What type of adaptive max pooling is applied over an input signal composed of several input planes?",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "3D"
    },
    {
        "X": "How large is the adaptive max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "1D"
    },
    {
        "X": "A 2D adaptive max pooling over an input signal is composed of what?",
        "Z": "Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What is the adaptive average pooling over an input signal composed of several input planes?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "1D"
    },
    {
        "X": "Applies a 1D adaptive average pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "What type of adaptive average pooling over an input signal composed of several input planes?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "3D"
    },
    {
        "X": "A 1D adaptive average pooling over an input signal composed of what?",
        "Z": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "A 2D adaptive average pooling over an input signal composed of what?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "Applies a 3D adaptive max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "2D"
    },
    {
        "X": "How large is the adaptive average pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "1D"
    },
    {
        "X": "What is the 3D adaptive average pooling over an input signal composed of?",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "Applies a 3D adaptive max pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "Applies a 2D adaptive average pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "A 3D adaptive average pooling over an input signal composed of what?",
        "Z": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 3D adaptive average pooling over an input signal composed of several input planes do?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "2D fractional max pooling over an input signal composed of several input planes"
    },
    {
        "X": "Applies a 3D adaptive average pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "What is the name of each element of the input Tensor?",
        "Z": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "Thresholds"
    },
    {
        "X": "What is the in-place version of?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "hardtanh"
    },
    {
        "X": "What does threshold apply element-wise?",
        "Z": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "rectified linear unit function"
    },
    {
        "X": "What is the in-place version of threshold()?",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "relu()"
    },
    {
        "X": "What is the name of the function that is element-wise?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "hardswish function"
    },
    {
        "X": "What version of threshold() is used?",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "In-place"
    },
    {
        "X": "What element-wise function does threshold() use?",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "rectified linear unit function"
    },
    {
        "X": "In-place version of what function?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.   ",
        "Y": "elu()"
    },
    {
        "X": "What is the element-wise function of the input Tensor?",
        "Z": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "hardswish function"
    },
    {
        "X": "What does threshold() apply element-wise?",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "rectified linear unit function"
    },
    {
        "X": "What element-wise function does relu() use?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "rectified linear unit function"
    },
    {
        "X": "What is the element-wise function of the HardTanh function?",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "hardswish function"
    },
    {
        "X": "What does relu do element-wise?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "rectified linear unit function"
    },
    {
        "X": "What is the in-place version of the rectified linear unit function?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "relu()"
    },
    {
        "X": "What is the in-place version of HardTanh?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "hardtanh()"
    },
    {
        "X": "What is the in-place version of hardtanh()?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "HardTanh"
    },
    {
        "X": "What is the in-place version of relu()?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "hardtanh()"
    },
    {
        "X": "What is the element-wise function that is described in the paper?",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "hardswish function"
    },
    {
        "X": "How does elu(x) apply?",
        "Z": "Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "element-wise"
    },
    {
        "X": "In-place version of elu(). Applies what?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   ",
        "Y": "element-wise"
    },
    {
        "X": "How does SELU(x)=scale(max(0,x)+min(0,(exp(x",
        "Z": "Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   ",
        "Y": "element-wise"
    },
    {
        "X": "What =max(0,x)+min(0,(exp(x/)1))?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "Y": "CELU(x)"
    },
    {
        "X": "What =max(0,x)+negative_slopemin(0,x)?",
        "Z": "Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "Y": "LeakyReLU(x)"
    },
    {
        "X": "What does x =max(0,x)+negative_slopemin(0,x)textLeaky",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "Y": "LeakyReLU"
    },
    {
        "X": "What is the value of the function PReLU(x)?",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "Y": "0,x"
    },
    {
        "X": "What is the in-place version of rrelu()?",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "Y": "gated linear unit"
    },
    {
        "X": "What is the in-place version of Randomized leaky ReLU?",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "Y": "rrelu()"
    },
    {
        "X": "What type of unit is the LeakyReLU?",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "Y": "gated linear unit"
    },
    {
        "X": "What does PReLU(x)=max(0,x)+?",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "Y": "weight"
    },
    {
        "X": "What is the in-place version of leaky_relu()?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "Y": "elu()"
    },
    {
        "X": "What type of transformation does y=xAT+by = xAT + by=xAT+b?",
        "Z": "  Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b ",
        "Y": "linear"
    },
    {
        "X": "What type of transformation does y=x1TAx2+by apply to the incoming data?",
        "Z": "  Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b ",
        "Y": "bilinear"
    },
    {
        "X": "What is the probability of zeroing some of the elements of the input tensor?",
        "Z": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "probability p"
    },
    {
        "X": "What is applied to the input?",
        "Z": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "alpha dropout"
    },
    {
        "X": "What is a channel?",
        "Z": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "feature map"
    },
    {
        "X": "What type of map is a channel?",
        "Z": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "3D feature map"
    },
    {
        "X": "Randomly zeroes some of the elements of the input tensor with probability p using samples from what?",
        "Z": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "a Bernoulli distribution"
    },
    {
        "X": "What type of dropout is applied to the input?",
        "Z": "Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   ",
        "Y": "alpha dropout"
    },
    {
        "X": "Applies what to the input?",
        "Z": "Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   ",
        "Y": "alpha dropout"
    },
    {
        "X": "What happens when a channel is a feature map?",
        "Z": "  Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "Randomly masks out entire channels"
    },
    {
        "X": "A channel is a what?",
        "Z": "  Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "2D feature map"
    },
    {
        "X": "What is a channel called?",
        "Z": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "Y": "3D feature map"
    },
    {
        "X": "A simple lookup table that looks up embeddings in what?",
        "Z": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "Y": "a fixed dictionary and size"
    },
    {
        "X": "Computes sums, means or maxes of bags of embeddings without what?",
        "Z": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "Y": "instantiating the intermediate embeddings"
    },
    {
        "X": "What does the tensor of shape have everywhere except where the index of last dimension matches the corresponding value of the input tensor?",
        "Z": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "Y": "zeros"
    },
    {
        "X": "What looks up embeddings in a fixed dictionary and size?",
        "Z": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "Y": "A simple lookup table"
    },
    {
        "X": "What does the lookup table do without instantiating the intermediate embeddings?",
        "Z": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "Y": "Computes sums, means or maxes"
    },
    {
        "X": "What is the name of the tensor that returns a tensor of shape?",
        "Z": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "Y": "num_classes"
    },
    {
        "X": "Cosine similarity between x1 and x2 is computed along what?",
        "Z": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "Y": "dim"
    },
    {
        "X": "What is the distance between every pair of row vectors in the input?",
        "Z": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "Y": "p-norm"
    },
    {
        "X": "What type of similarity between x1 and x2 is returned by torch.nn.PairwiseDistance?",
        "Z": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "Y": "cosine"
    },
    {
        "X": "Computes the distance between every pair of row vectors in the input?",
        "Z": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "Y": "p-norm"
    },
    {
        "X": "What is the Binary Cross Entropy between?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "Y": "the target and the output"
    },
    {
        "X": "What does the function measure between target and output logits?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "Y": "Binary Cross Entropy"
    },
    {
        "X": "What type of negative log likelihood loss does CosineEmbeddingLoss combine?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "Y": "Poisson"
    },
    {
        "X": "What criterion combines log_softmax and nll_loss in a single function?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "CosineEmbeddingLoss"
    },
    {
        "X": "CosineEmbeddingLoss combines what two criterions in a single function?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "log_softmax and nll_loss"
    },
    {
        "X": "What is the criterion that combines log_softmax and nll_loss in a single function?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "CosineEmbeddingLoss"
    },
    {
        "X": "What type of negative log likelihood loss is HingeEmbeddingLoss?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Gaussian"
    },
    {
        "X": "What is the Gaussian negative log likelihood loss?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Connectionist Temporal Classification loss"
    },
    {
        "X": "What is the name of the function that measures the Binary Cross Entropy between the target and the output?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "Y": "Kullback-Leibler divergence Loss"
    },
    {
        "X": "What does the function measure between the target and the output?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Binary Cross Entropy"
    },
    {
        "X": "What does the function measure between the target and output logits?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Binary Cross Entropy"
    },
    {
        "X": "What is a function that measures Binary Cross Entropy between target and output logits?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "Y": "Poisson negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that measures Poisson negative log likelihood loss?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "CosineEmbeddingLoss"
    },
    {
        "X": "What criterion combines in a single function?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "log_softmax and nll_loss"
    },
    {
        "X": "What is a Gaussian negative log likelihood loss?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Connectionist Temporal Classification loss"
    },
    {
        "X": "What type of loss is HingeEmbeddingLoss?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "What divergence Loss?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "Y": "Kullback-Leibler"
    },
    {
        "X": "What type of negative log likelihood loss does CosineEmbeddingLoss measure?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "Y": "Poisson"
    },
    {
        "X": "CosineEmbeddingLoss combines what two criterion in a single function?",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   ",
        "Y": "log_softmax and nll_loss"
    },
    {
        "X": "What is the criterion for Gaussian negative log likelihood loss?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "Y": "Connectionist Temporal Classification loss"
    },
    {
        "X": "What takes the mean element-wise absolute value difference?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "The Kullback-Leibler divergence Loss Function"
    },
    {
        "X": "What is the function that measures Binary Cross Entropy between target and output logits?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "Y": "Poisson negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that measures Binary Cross Entropy between target and output logits?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "Y": "CosineEmbeddingLoss"
    },
    {
        "X": "What is the term for the loss of log_softmax and nll_loss in a single function?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "Y": "Connectionist Temporal Classification loss"
    },
    {
        "X": "What type of loss does HingeEmbeddingLoss measure?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "What divergence Loss Function takes the mean element-wise absolute value difference?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Kullback-Leibler"
    },
    {
        "X": "CosineEmbeddingLoss combines log_softmax and nll_loss in a single function?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "Poisson"
    },
    {
        "X": "What is the name of the criterion that combines log_softmax and nll_loss in a single function?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "Y": "Connectionist Temporal Classification loss"
    },
    {
        "X": "What does the Kullback-Leibler divergence Loss Function measure?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Measures the element-wise mean squared error"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function that measures the element-wise mean squared error?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "Y": "MarginRankingLoss"
    },
    {
        "X": "What is the loss that combines log_softmax and nll_loss in a single function?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "Y": "Poisson negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that combines log_softmax and nll_loss in a single function?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "Y": "CosineEmbeddingLoss"
    },
    {
        "X": "What type of negative log likelihood loss?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "Gaussian"
    },
    {
        "X": "What function measures the element-wise mean squared error?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "Y": "MarginRankingLoss"
    },
    {
        "X": "What is the name of the Gaussian negative log likelihood loss function?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "HingeEmbeddingLoss"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function called?",
        "Z": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "MultiLabelMarginLoss"
    },
    {
        "X": "What does the Kullback-Leibler divergence Loss Function use for details?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "CosineEmbeddingLoss combines what in a single function?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "log_softmax and nll_loss"
    },
    {
        "X": "What is another term for the Connectionist Temporal Classification loss?",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that takes the mean element-wise absolute value difference?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "MultiLabelMarginLoss"
    },
    {
        "X": "What is the name of the function that measures the mean element-wise mean squared error?",
        "Z": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "MultiLabelMarginLoss"
    },
    {
        "X": "What does this criterion combine in a single function?",
        "Z": "This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   ",
        "Y": "log_softmax and nll_loss"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function that takes the mean element-wise absolute value difference?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "MultiLabelMarginLoss"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function?",
        "Z": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "What is the name of the MultiLabelSoftMarginLoss function?",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "This criterion combines log_softmax and what else in a single function?",
        "Z": "This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   ",
        "Y": "nll_loss"
    },
    {
        "X": "What is the Connectionist Temporal Classification loss?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that measures the element-wise mean squared error?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "What is HingeEmbeddingLoss for details?",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "What is the name of the Loss Function that takes the mean element-wise absolute value difference?",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "Y": "MultiLabelMarginLoss"
    },
    {
        "X": "What is another name for Gaussian negative log likelihood loss?",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "Y": "HingeEmbeddingLoss"
    },
    {
        "X": "What is another name for MultiLabelSoftMarginLoss?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "What is the term for input, target, p=1, margin=1, weight=None, size_average=None?",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "Y": "multi_margin_loss"
    },
    {
        "X": "What function measures the negative log likelihood loss?",
        "Z": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "multi_margin_loss"
    },
    {
        "X": "What is HingeEmbeddingLoss?",
        "Z": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that measures the mean squared error?",
        "Z": "See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "MultiLabelMarginLoss"
    },
    {
        "X": "What is multi_margin_loss?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "negative log likelihood loss"
    },
    {
        "X": "Multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=N",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "negative log likelihood loss"
    },
    {
        "X": "What is multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average",
        "Z": "See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "negative log likelihood loss"
    },
    {
        "X": "What is the result of multi_margin_loss?",
        "Z": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "Y": "negative log likelihood loss"
    },
    {
        "X": "What does the function take?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "the mean element-wise absolute value difference"
    },
    {
        "X": "What does MarginRankingLoss do?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "Measures the element-wise mean squared error"
    },
    {
        "X": "What function uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "When does a squared term use a function that uses a squared term?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise"
    },
    {
        "X": "What does the function measure?",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "the element-wise mean squared error"
    },
    {
        "X": "What does multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "negative log likelihood loss"
    },
    {
        "X": "What term is used if the absolute element-wise error falls below delta?",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "squared term"
    },
    {
        "X": "What does MarginRankingLoss measure?",
        "Z": "Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "the element-wise mean squared error"
    },
    {
        "X": "When does a function use a squared term?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "if the absolute element-wise error falls below beta and an L1 term otherwise"
    },
    {
        "X": "What does multi_margin_loss measure?",
        "Z": "Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "Y": "negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that uses a squared term if the absolute element-wise error falls below beta?",
        "Z": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "TripletMarginLoss"
    },
    {
        "X": "What is the name of the function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "SoftMarginLoss"
    },
    {
        "X": "What does a function use if the absolute element-wise error falls below delta?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "a squared term"
    },
    {
        "X": "What does a function use if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "a squared term"
    },
    {
        "X": "What is the name of the function that uses a squared term if the absolute element-wise error falls below delta?",
        "Z": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "What term is used if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "squared term"
    },
    {
        "X": "What is the absolute element-wise error that uses a squared term if the absolute element-wise error falls below?",
        "Z": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "beta"
    },
    {
        "X": "What does the absolute element-wise error fall below?",
        "Z": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "delta"
    },
    {
        "X": "What does a function use if the absolute element-wise error falls below beta?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "a squared term"
    },
    {
        "X": "What is the absolute element-wise error below?",
        "Z": "See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details ",
        "Y": "delta"
    },
    {
        "X": "What is the name of the function that uses a squared term if the absolute element-wise error falls below delta and a delta-scale",
        "Z": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "MultiLabelSoftMarginLoss"
    },
    {
        "X": "What is r?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "upscale_factor"
    },
    {
        "X": "What operation is reversed by rearranging elements in a tensor of shape?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "PixelShuffle"
    },
    {
        "X": "What is the downscale_factor of a tensor of shape?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "Pads tensor"
    },
    {
        "X": "What ranges elements in a tensor of shape?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "r"
    },
    {
        "X": "Reverses what operation by rearranging elements in a tensor of shape?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "PixelShuffle"
    },
    {
        "X": "What is the tensor of a tensor?",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   ",
        "Y": "Pads"
    },
    {
        "X": "What is used to upsample the input?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "bilinear upsampling"
    },
    {
        "X": "What is the downscale_factor of the PixelShuffle operation?",
        "Z": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   ",
        "Y": "r"
    },
    {
        "X": "What is the tensor of the PixelShuffle operation?",
        "Z": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   ",
        "Y": "Pads"
    },
    {
        "X": "What is the name of the tensor?",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "Tensor"
    },
    {
        "X": "What is the name of the step that samples the input to the given size or the given scale_factor?",
        "Z": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "Down/up"
    },
    {
        "X": "What is used to compute the output of a flow-field grid?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "input values and pixel locations"
    },
    {
        "X": "What is given a batch of to generate a 2D or 3D flow field?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "affine matrices theta"
    },
    {
        "X": "What is the name of the tensor that Down/up samples the input to either the given size or the given scale_factor?",
        "Z": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "Pads tensor"
    },
    {
        "X": "Upsamples the input using what?",
        "Z": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "bilinear upsampling"
    },
    {
        "X": "What is computed from a flow-field grid?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "input values and pixel locations"
    },
    {
        "X": "What type of theta is used to generate a flow field?",
        "Z": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "affine matrices"
    },
    {
        "X": "What are the GPUs given in?",
        "Z": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids. ",
        "Y": "device_ids"
    },
    {
        "X": "What does torch.nn.parallel.data_parallel evaluate in parallel across the GPUs given in device_ids?",
        "Z": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids. ",
        "Y": "module"
    },
    {
        "X": "What does torch.nn.parallel.data_parallel use to evaluate module(input) in parallel across GPUs?",
        "Z": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids. ",
        "Y": "device_ids"
    },
    {
        "X": "What does the behavior depend on the dimensionality of the tensors as follows?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "Matrix product of two tensors"
    },
    {
        "X": "What is returned if both tensors are 1-dimensional?",
        "Z": "If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "dot product"
    },
    {
        "X": "What is returned if both arguments are 2-dimensional?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "matrix-matrix product"
    },
    {
        "X": "What is the dimension of the first argument?",
        "Z": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "1"
    },
    {
        "X": "What happens after the matrix multiply?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. ",
        "Y": "the prepended dimension is removed"
    },
    {
        "X": "What product of two tensors is returned?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "Matrix product"
    },
    {
        "X": "What product is returned if both arguments are 2-dimensional?",
        "Z": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "matrix-matrix"
    },
    {
        "X": "What is the dimensionality of the first argument?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. ",
        "Y": "1"
    },
    {
        "X": "What is removed after the matrix multiply?",
        "Z": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "the prepended dimension"
    },
    {
        "X": "When is the prepended dimension removed?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "matrix multiply"
    },
    {
        "X": "If the first argument is 1-dimensional and the second argument is 2-dimensional, what is prepended to its dimension for the purpose of the matrix multiply?",
        "Z": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "a 1"
    },
    {
        "X": "What is returned if the first argument is 2-dimensional and the second argument is 1-dimensional?",
        "Z": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "matrix-vector product"
    },
    {
        "X": "What product is returned if the first argument is 2-dimensional and the second argument is 1-dimensional?",
        "Z": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "matrix-vector"
    },
    {
        "X": "If the first argument is 2-dimensional and the second is 2-dimensional, how many dimensions does the matrix-matrix product return?",
        "Z": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "1"
    },
    {
        "X": "What happens to the prepended dimension after the matrix multiply?",
        "Z": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "removed"
    },
    {
        "X": "The matrix-vector product is returned if the first argument is what?",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "2-dimensional"
    },
    {
        "X": "If the first argument is 2-dimensional and the second argument is what?",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "1-dimensional"
    },
    {
        "X": "What does the broadcasting logic only look at when determining if the inputs are broadcastable?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions"
    },
    {
        "X": "What are different in the matrix dimensions?",
        "Z": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. ",
        "Y": "the final two dimensions"
    },
    {
        "X": "What are the final two dimensions?",
        "Z": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. ",
        "Y": "matrix dimensions"
    },
    {
        "X": "What does this operator support?",
        "Z": "This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "TensorFloat32"
    },
    {
        "X": "What dimension does the dot product version of this function not support an out parameter?",
        "Z": "This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "1-dimensional"
    },
    {
        "X": "What is an alias for?",
        "Z": "Alias for torch.acos(). ",
        "Y": "torch.acos"
    },
    {
        "X": "What is an Alias for?",
        "Z": "Alias for torch.mul(). ",
        "Y": "torch.mul()"
    },
    {
        "X": "What does it do when a tensor input is not specified?",
        "Z": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "Y": "Counts the number of non-zero values in the tensor input along the given dim"
    },
    {
        "X": "What is counted if no dim is specified?",
        "Z": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "Y": "all non-zeros"
    },
    {
        "X": "What is a tuple of dims along which to count non-zeros?",
        "Z": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "Y": "python:ints"
    },
    {
        "X": "What is an example of a tuple of dims along which to count non-zeros?",
        "Z": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "Y": "Example:"
    },
    {
        "X": "What is the implementation based on?",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "Algorithm 5.1"
    },
    {
        "X": "To obtain repeatable results, reset the seed for what?",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "pseudorandom number generator"
    },
    {
        "X": "How much higher performance is torch.linalg.svd() compared to the full-rank SVD implementation?",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "10-fold"
    },
    {
        "X": "What type of matrices will low-rank SVD be useful for?",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "sparse matrices"
    },
    {
        "X": "When was the Algorithm 5.1 implemented?",
        "Z": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. ",
        "Y": "2009"
    },
    {
        "X": "What is the input assumed to be?",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "low-rank matrix"
    },
    {
        "X": "Why should you use the full-rank SVD implementation for dense matrices?",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "10-fold higher performance characteristics"
    },
    {
        "X": "What type of matrices is low-rank SVD useful for?",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "huge sparse matrices"
    },
    {
        "X": "How much higher performance is torch.linalg.svd() compared to full-rank SVD?",
        "Z": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "10-fold"
    },
    {
        "X": "What is the input tensor of size?",
        "Z": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "Y": "A"
    },
    {
        "X": "What is the low-rank SVD useful for?",
        "Z": "Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "Y": "huge sparse matrices"
    },
    {
        "X": "What is a slightly overestimated rank of A?",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "q"
    },
    {
        "X": "What must be a nonnegative integer?",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "niter"
    },
    {
        "X": "The low-rank SVD is useful for what type of matrices that torch.linalg.svd() cannot handle?",
        "Z": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "huge sparse matrices"
    },
    {
        "X": "How much higher performance characteristics do full-rank SVD implementations have?",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "10-fold"
    },
    {
        "X": "What will be useful for huge sparse matrices that torch.linalg.svd() cannot handle?",
        "Z": "Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "low-rank SVD"
    },
    {
        "X": "What is useful for huge sparse matrices that torch.linalg.svd() cannot handle?",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "low-rank SVD"
    },
    {
        "X": "What does q mean?",
        "Z": "A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "a slightly overestimated rank of A. conduct"
    },
    {
        "X": "What is the name of the book that Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp published in 2009?",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "Y": "arXiv:0909.4061"
    },
    {
        "X": "What grid is defined by expanding the iii th input over dimensions defined by other inputs?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "iii th"
    },
    {
        "X": "What can NNN tensors be?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically ",
        "Y": "scalar or 1-dimensional vector"
    },
    {
        "X": "What are tensors?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "list of scalars or 1 dimensional tensors"
    },
    {
        "X": "What will be treated as tensors of size automatically?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically ",
        "Y": "Scalars"
    },
    {
        "X": "What is tensors?",
        "Z": "tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "list of scalars or 1 dimensional tensors"
    },
    {
        "X": "What will be treated as tensors of size?",
        "Z": "tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "Scalars"
    },
    {
        "X": "What is an example of a sequence of Tensors?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "seq"
    },
    {
        "X": "If the input has what of size (N1,),(N2,),...,(Nk,)(N_1,), (N_2,",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "kkk tensors"
    },
    {
        "X": "What is the name for the sequence of Tensors?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "seq"
    },
    {
        "X": "Returns what number of threads used for inter-op parallelism on CPU?",
        "Z": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter) ",
        "Y": "the number of threads"
    },
    {
        "X": "In what is the number of threads used for inter-op parallelism on CPU used?",
        "Z": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter) ",
        "Y": "JIT interpreter"
    },
    {
        "X": "Returns True what if the input is a single element tensor?",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples: ",
        "Y": "if the input is a single element tensor"
    },
    {
        "X": "Returns True if the input is a single element tensor which is not equal to what?",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples: ",
        "Y": "torch.tensor([0]) or torch.tensor([False])"
    },
    {
        "X": "What throws if torch.numel()!= 1?",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples: ",
        "Y": "RuntimeError"
    },
    {
        "X": "What is a multi-dimensional matrix containing elements of a single data type?",
        "Z": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor ",
        "Y": "A torch.Tensor"
    },
    {
        "X": "How many tensor types does Torch define?",
        "Z": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "Y": "10 tensor types with CPU and GPU variants"
    },
    {
        "X": "What are the two variants of Torch's tensor types?",
        "Z": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 ",
        "Y": "CPU and GPU"
    },
    {
        "X": "What is the bit floating point torch?",
        "Z": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 ",
        "Y": "32"
    },
    {
        "X": "What is the complex of a BFloat16Tensor torch?",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex ",
        "Y": "128-bit"
    },
    {
        "X": "What is the data type dtype CPU tensor GPU tensor?",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "Y": "dtype"
    },
    {
        "X": "What type of tensor is a 32-bit floating point torch?",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "Y": "dtype CPU tensor GPU tensor"
    },
    {
        "X": "What type of torch is a GPU tensor?",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is the name of the GPU tensor?",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is the GPU tensor?",
        "Z": "GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What bit floating point torch is GPU tensor?",
        "Z": "GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "Y": "32"
    },
    {
        "X": "What type of torch is float32?",
        "Z": "torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "Y": "torch"
    },
    {
        "X": "What is a double 8-bit integer?",
        "Z": "32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) ",
        "Y": "unsigned"
    },
    {
        "X": "What is the name of the torch?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "bfloat16 torch"
    },
    {
        "X": "What is another name for a double-tensor torch?",
        "Z": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "Y": "double torch"
    },
    {
        "X": "What type of complex torch is included in the BFloat16Tensor 32-bit complex torch?",
        "Z": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) ",
        "Y": "8-bit"
    },
    {
        "X": "What is a double torch?",
        "Z": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "Y": "double torch"
    },
    {
        "X": "What is another name for a double torch?",
        "Z": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "Y": "double torch"
    },
    {
        "X": "What is the floating point of a DoubleTensor torch?",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "Y": "16-bit"
    },
    {
        "X": "What is a half torch?",
        "Z": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor ",
        "Y": "16-bit floating point 1 torch"
    },
    {
        "X": "What is 1 torch.float16 or torch.half torch?",
        "Z": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 ",
        "Y": "16-bit floating point"
    },
    {
        "X": "What type of torch is a HalfTensor torch?",
        "Z": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "half torch"
    },
    {
        "X": "What are the names of the two types of torch?",
        "Z": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "torch.float16 or torch.half torch"
    },
    {
        "X": "How many bits does a BFloat16Tensor have?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor ",
        "Y": "32"
    },
    {
        "X": "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or",
        "Z": "torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) ",
        "Y": "32"
    },
    {
        "X": "What type of torch is a HalfTensor?",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point 2 torch"
    },
    {
        "X": "What is the name of HalfTensor 16-bit floating point 2 torch?",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int ",
        "Y": "torch.cuda"
    },
    {
        "X": "What type of torch is a bfloat16 torch?",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point 2 torch"
    },
    {
        "X": "What type of torch is bfloat16?",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor ",
        "Y": "16-bit floating point 2"
    },
    {
        "X": "What is the 32-bit complex torch.complex32 64-bit complex torch.complex64?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor ",
        "Y": "32-bit complex torch.complex32 64-bit complex torch.complex64"
    },
    {
        "X": "What type of complex torch is a BFloat16Tensor 32-bit complex torch?",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "Y": "64-bit"
    },
    {
        "X": "What is a torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.byteTensor",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "Y": "32-bit complex torch.complex32 64-bit complex torch.complex64"
    },
    {
        "X": "What type of complex torch is a BFloat16Tensor?",
        "Z": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "Y": "32-bit"
    },
    {
        "X": "How many bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch",
        "Z": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "Y": "32"
    },
    {
        "X": "What is the most common type of complex torch?",
        "Z": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor ",
        "Y": "32-bit"
    },
    {
        "X": "What is the name of the torch that has a 64-bit integer?",
        "Z": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor ",
        "Y": "LongTensor"
    },
    {
        "X": "What type of torch is LongTensor?",
        "Z": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor ",
        "Y": "long torch"
    },
    {
        "X": "What type of complex torch is a torch?",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean ",
        "Y": "64-bit"
    },
    {
        "X": "What is the name of the Boolean?",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean ",
        "Y": "LongTensor Boolean"
    },
    {
        "X": "What type of torch is a LongTensor?",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "Y": "Boolean torch"
    },
    {
        "X": "What is a LongTensor Boolean torch.bool?",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "Y": "LongTensor torch.cuda"
    },
    {
        "X": "How many bits is a complex torch?",
        "Z": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "Y": "128"
    },
    {
        "X": "What type of torch is.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch",
        "Z": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "Y": "128-bit complex torch"
    },
    {
        "X": "What is another name for unsigned 8-bit integer torch.uint8 torch?",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor ",
        "Y": "torch.cdouble"
    },
    {
        "X": "What type of integer is a torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "Y": "8-bit"
    },
    {
        "X": "What is a torch.uint8 torch?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is a ByteTensor?",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the sign of a ByteTensor torch?",
        "Z": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What type of torch is a ByteTensor?",
        "Z": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What does ByteTensor stand for?",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is a torch?",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "32-bit integer"
    },
    {
        "X": "What is the sign of a torch?",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 ",
        "Y": "16-bit integer"
    },
    {
        "X": "How many bits is a CharTensor?",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "Y": "16"
    },
    {
        "X": "What is the name of the torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed)",
        "Z": "torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "Y": "torch.int8"
    },
    {
        "X": "What is a CharTensor 16-bit integer (signed)?",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "Y": "torch.int16 or torch.short torch"
    },
    {
        "X": "What type of integer is a short torch?",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "Y": "16-bit"
    },
    {
        "X": "Int16 or torch.short torch.ShortTensor torch.cuda.IntTensor 32-bit",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "Y": "16"
    },
    {
        "X": "What is a shorttensor?",
        "Z": "torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "Y": "32-bit integer"
    },
    {
        "X": "How many 2-bit integers are in a torch?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 ",
        "Y": "3"
    },
    {
        "X": "What is the sign of an IntTensor?",
        "Z": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 ",
        "Y": "4-bit integer"
    },
    {
        "X": "What is a torch.int32 or torch.int torch?",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "32-bit integer"
    },
    {
        "X": "What language can a tensor be constructed from?",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "Y": "Python"
    },
    {
        "X": "What are two ways to avoid a copy of a Tensor?",
        "Z": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "requires_grad_() or detach()"
    },
    {
        "X": "What is used if you have a numpy array and want to avoid a copy?",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "Y": "torch.as_tensor()"
    },
    {
        "X": "How can a tensor of specific data type be constructed?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "by passing a torch.dtype and/or a torch"
    },
    {
        "X": "A tensor can be constructed from a Python list or sequence using what constructor?",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "torch.tensor()"
    },
    {
        "X": "What are two ways to avoid a copy of a tensor?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "requires_grad_() or detach()"
    },
    {
        "X": "If you have a numpy array and want to avoid a copy, use what?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "torch.as_tensor()"
    },
    {
        "X": "A tensor of specific data type can be constructed by passing a what?",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "Y": "torch.dtype"
    },
    {
        "X": "What always copies data?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "Warning torch.tensor()"
    },
    {
        "X": "What type of data does torch.as_tensor() copy?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "numpy array"
    },
    {
        "X": "For more information about building Tensors, see what?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "Creation Ops"
    },
    {
        "X": "What does torch.tensor() do?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "always copies data"
    },
    {
        "X": "What language can be used to access and modify the contents of a tensor?",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "Y": "Python"
    },
    {
        "X": "The contents of a tensor can be accessed and modified using Python\u2019s what?",
        "Z": "A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "indexing"
    },
    {
        "X": "The contents of a tensor can be accessed and modified using Python's what?",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "indexing and slicing notation"
    },
    {
        "X": "What is required to create a tensor?",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "requires_grad=True"
    },
    {
        "X": "What holds data for each tensor?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "an associated torch.Storage"
    },
    {
        "X": "What does the tensor class provide?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "multi-dimensional, strided view of a storage"
    },
    {
        "X": "What is the name of the tensor class that defines numeric operations on it?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "Note"
    },
    {
        "X": "What does torch.Tensor.item() get a Python number from?",
        "Z": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "a tensor"
    },
    {
        "X": "What holds a tensor's data?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "torch.Storage"
    },
    {
        "X": "What does the tensor class do?",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. ",
        "Y": "defines numeric operations on it"
    },
    {
        "X": "What is another name for a tensor class?",
        "Z": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "Y": "Note"
    },
    {
        "X": "For more information on tensor views, see what?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "Tensor Views"
    },
    {
        "X": "For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor,",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "Tensor Attributes"
    },
    {
        "X": "Methods that mutate a tensor are marked with what?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "an underscore suffix"
    },
    {
        "X": "What does torch.FloatTensor.abs() compute the result in?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "a new tensor"
    },
    {
        "X": "What does torch.FloatTensor.abs() do?",
        "Z": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note ",
        "Y": "Note"
    },
    {
        "X": "Methods which mutate a tensor are marked with what?",
        "Z": "For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note ",
        "Y": "underscore suffix"
    },
    {
        "X": "What are methods that mutate a tensor marked with?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "underscore suffix"
    },
    {
        "X": "What method is used to change an existing tensor's torch.device and/or torch.dtype?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "to() method"
    },
    {
        "X": "What does the to() method do to an existing tensor?",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "For more information on the torch.Tensor, see what?",
        "Z": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Tensor Attributes"
    },
    {
        "X": "What suffix mark methods that mutate a tensor?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "underscore"
    },
    {
        "X": "What does torch.FloatTensor.abs_() do?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "computes the absolute value in-place and returns the modified tensor"
    },
    {
        "X": "What is the name of the warning that is issued when a tensor is changed?",
        "Z": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What method can be used to change an existing tensor's torch.device and/or torch.dtype?",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "to() method"
    },
    {
        "X": "Methods which mutate a tensor are marked with what suffix?",
        "Z": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "underscore"
    },
    {
        "X": "What suffix is used to mark methods that mutate a tensor?",
        "Z": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "underscore"
    },
    {
        "X": "What computes the absolute value in-place and returns the modified tensor?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "torch.FloatTensor.abs_()"
    },
    {
        "X": "What is a warning about a mutated tensor?",
        "Z": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What does the to() method do?",
        "Z": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What is the name of the warning that a method that mutates a tensor is marked with an underscore suffix?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What does the to() method do to change an existing tensor's torch.device and/or torch.dtype?",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What kind of memory usage might be caused by the current implementation of torch.Tensor?",
        "Z": "Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "unexpectedly high"
    },
    {
        "X": "What should you use if you have a lot of tiny tensors?",
        "Z": "Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "one large structure"
    },
    {
        "X": "What does the current implementation of torch.Tensor introduce?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "memory overhead"
    },
    {
        "X": "What should you use if you have many tiny tensors?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "one large structure"
    },
    {
        "X": "What is the main way to create a tensor?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. ",
        "Y": "tensor"
    },
    {
        "X": "What does torch.tensor() create a tensor with?",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "pre-existing data"
    },
    {
        "X": "What do you need to create a tensor with?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. ",
        "Y": "specific size"
    },
    {
        "X": "What is the name of the tensor creation ops?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "Y": "Creation Ops"
    },
    {
        "X": "What type of tensor does torch create?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. ",
        "Y": "tensor with the same size (and similar types) as another tensor"
    },
    {
        "X": "What type of tensor is similar to another tensor but different in size?",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. ",
        "Y": "tensor"
    },
    {
        "X": "What type of tensor is created with the same size as another tensor?",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "tensor"
    },
    {
        "X": "What type of tensor is created with the same type but different size as another tensor?",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "tensor"
    },
    {
        "X": "What is reversed in a tensor?",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "Y": "its dimensions"
    },
    {
        "X": "What does torch create a tensor with?",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. ",
        "Y": "specific size"
    },
    {
        "X": "What is another name for tensor creation ops?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "Creation Ops"
    },
    {
        "X": "What is used to create a tensor with the same size as another tensor?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "torch"
    },
    {
        "X": "To create a tensor with similar type but different size as another tensor, use what?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "Y": "tensor.new_* creation ops"
    },
    {
        "X": "Is this Tensor with its dimensions reversed or reversed?",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin ",
        "Y": "reversed"
    },
    {
        "X": "If n is the number of dimensions in x, x.T is equivalent to what?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "Y": "x.permute"
    },
    {
        "X": "What is used to create a tensor with similar type but different size?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor ",
        "Y": "Tensor.new_tensor"
    },
    {
        "X": "What is used to create a tensor with similar type but different size as another tensor?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "Y": "tensor.new_* creation ops"
    },
    {
        "X": "If n is the number of dimensions in what integer, x.T is equivalent to x.permute?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones ",
        "Y": "x"
    },
    {
        "X": "What returns a new Tensor with data as the tensor data?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Tensor"
    },
    {
        "X": "What does Tensor.new_tensor return as the tensor data?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones ",
        "Y": "data"
    },
    {
        "X": "What does tensor.new_full return a Tensor of size size filled with?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() ",
        "Y": "fill_value"
    },
    {
        "X": "What does tensor.new_empty return a Tensor of size size filled with?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() ",
        "Y": "uninitialized data"
    },
    {
        "X": "What returns a Tensor of size size filled with uninitialized data?",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device ",
        "Y": "Tensor.device"
    },
    {
        "X": "What does Tensor.new_full return a Tensor of size size filled with?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "Y": "fill_value"
    },
    {
        "X": "What returns a tensor of size size filled with uninitialized data?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones ",
        "Y": "Tensor.new_ones"
    },
    {
        "X": "If n is the number of dimensions in what, x.T is equivalent to x.permute(n-1, n-2",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "Y": "x"
    },
    {
        "X": "What is returned as the tensor data?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "Y": "data"
    },
    {
        "X": "What does the Tensor.new_empty return a Tensor of size size filled with?",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "Y": "uninitialized data"
    },
    {
        "X": "Tensor.new_ones Returns a Tensor of size size filled with what value?",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() ",
        "Y": "1"
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of size size filled with what value?",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() ",
        "Y": "0."
    },
    {
        "X": "What is the name of the Tensor with its dimensions reversed?",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "Y": "Tensor.is_cuda"
    },
    {
        "X": "What data does Tensor.new_empty return a Tensor of size size filled with?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "Y": "uninitialized data"
    },
    {
        "X": "Tensor.new_ones Returns a Tensor of size size filled with what?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "Y": "1"
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of size size filled with what?",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "Y": "0."
    },
    {
        "X": "What returns a Tensor of size size filled with 0.",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "Y": "Tensor.is_cuda"
    },
    {
        "X": "What is x.T equivalent to?",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "Y": "x.permute"
    },
    {
        "X": "What does Tensor.new_empty return a Tensor of size size filled with?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "Y": "uninitialized data"
    },
    {
        "X": "Is the Tensor.is_cuda True or False if the Tensor is stored on the GPU?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "Y": "True"
    },
    {
        "X": "What does a Tensor.new_empty return a Tensor of size size filled with?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "uninitialized data"
    },
    {
        "X": "What is the tensor of size size filled with?",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. ",
        "Y": "0."
    },
    {
        "X": "Is Tensor.is_cuda True or False if the Tensor is stored on the GPU?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "Y": "True"
    },
    {
        "X": "What does the new Tensor return as the tensor data?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "Y": "data"
    },
    {
        "X": "Tensor.is_cuda Is True if the Tensor is stored on what?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "Y": "GPU"
    },
    {
        "X": "Is the Tensor.is_quantized True or False if the Tensor is quantized?",
        "Z": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv ",
        "Y": "True"
    },
    {
        "X": "What is true if the Tensor is stored on the GPU?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Tensor.is_cuda"
    },
    {
        "X": "What is true if the Tensor is quantized?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "Y": "Tensor.is_quantized"
    },
    {
        "X": "Where is the Tensor stored?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "Y": "the GPU"
    },
    {
        "X": "Returns what with data as the tensor data?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "a new Tensor"
    },
    {
        "X": "What is the Tensor true if it is a meta tensor?",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ ",
        "Y": "meta tensor"
    },
    {
        "X": "What is True if the Tensor is a meta tensor?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "Y": "meta"
    },
    {
        "X": "What is true if the Tensor is a meta tensor?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() ",
        "Y": "meta"
    },
    {
        "X": "What is the Tensor of size size filled with?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "uninitialized data"
    },
    {
        "X": "Tensor.is_quantized Is what if the Tensor is quantized?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "True"
    },
    {
        "X": "What type of Tensor is True if the Tensor is a meta tensor?",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "Y": "meta tensor"
    },
    {
        "X": "Returns a Tensor of size size filled with what?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "Y": "1"
    },
    {
        "X": "What is the device where the Tensor is located?",
        "Z": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "Y": "torch"
    },
    {
        "X": "What is the torch.device where the Tensor is located?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "Y": "the torch.device where this Tensor is"
    },
    {
        "X": "What device is used to store the Tensor?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "Y": "torch"
    },
    {
        "X": "What is the torch.device where the Tensor is?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "Y": "Tensor.grad"
    },
    {
        "X": "What does Tensor.new_ones return?",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "Y": "Tensor of size size filled with 1."
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of size size filled with how many zeros?",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ ",
        "Y": "0."
    },
    {
        "X": "What device is used to store a Tensor?",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "Y": "torch"
    },
    {
        "X": "What is the device where the Tensor is stored?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "Y": "torch"
    },
    {
        "X": "What is the size size of a Tensor of size size filled with?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "Y": "1"
    },
    {
        "X": "What is the device where the Tensor is?",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "Y": "torch"
    },
    {
        "X": "What is the torch.device where this Tensor is?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "Y": "Tensor.grad"
    },
    {
        "X": "What does Tensor.new_zeros return?",
        "Z": "Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real ",
        "Y": "Tensor of size size filled with 0."
    },
    {
        "X": "What is the default value of Tensor.grad?",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "Y": "None"
    },
    {
        "X": "What Alias for dim() Tensor.real?",
        "Z": "Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real ",
        "Y": "dim"
    },
    {
        "X": "What is the size of the Tensor?",
        "Z": "Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real ",
        "Y": "0."
    },
    {
        "X": "Is True if the Tensor is a what?",
        "Z": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "Y": "meta tensor"
    },
    {
        "X": "What Alias for dim() Tensor.n Returns a new tensor containing real values of the self tens",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "Y": "dim"
    },
    {
        "X": "What returns a new tensor containing real values of the self tensor?",
        "Z": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr ",
        "Y": "real"
    },
    {
        "X": "Is Tensor.is_quantized True or False if the Tensor is quantized?",
        "Z": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv ",
        "Y": "True"
    },
    {
        "X": "What is the Tensor.device?",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "Y": "torch.device"
    },
    {
        "X": "What Alias for dim() Tensor.real Returns a new tensor containing real values of the self tens",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() ",
        "Y": "dim"
    },
    {
        "X": "Is the Tensor quantized?",
        "Z": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs ",
        "Y": "True"
    },
    {
        "X": "What is the default value of the attribute Tensor.grad?",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "Y": "None"
    },
    {
        "X": "What does the imag return a new tensor containing?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "Y": "imaginary values of the self tensor"
    },
    {
        "X": "What returns a new tensor containing imaginary values of the self tensor?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax ",
        "Y": "Tensor.imag"
    },
    {
        "X": "Is True if the Tensor is what?",
        "Z": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "Y": "quantized"
    },
    {
        "X": "What is the name of the device where the Tensor is?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "torch.device"
    },
    {
        "X": "What is the default attribute of Tensor.grad?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "Y": "None"
    },
    {
        "X": "What does Tensor.ndim return a new tensor containing?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "What does Tensor.imag return a new tensor containing?",
        "Z": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "imaginary values of the self tensor"
    },
    {
        "X": "Is the Tensor.is_meta True or False if the Tensor is a meta tensor?",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "Y": "True"
    },
    {
        "X": "Where is the Tensor?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "torch.device"
    },
    {
        "X": "What does imag return a new tensor containing?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "Y": "imaginary values of the self tensor"
    },
    {
        "X": "What is the name of the In-place version of abs()?",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() ",
        "Y": "abs()"
    },
    {
        "X": "Is Tensor.is_meta True or False if the Tensor is a meta tensor?",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "Y": "True"
    },
    {
        "X": "What is the In-place version of abs()?",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() ",
        "Y": "abs()"
    },
    {
        "X": "Is the Tensor a meta tensor?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "Y": "True"
    },
    {
        "X": "What is the name of the device where the Tensor is located?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "Y": "torch.device"
    },
    {
        "X": "What is the in-place version of abs() Tensor.absolute?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute ",
        "Y": "abs() Tensor.abs"
    },
    {
        "X": "Is True if the Tensor is a meta tensor?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "Y": "if the Tensor is a meta tensor"
    },
    {
        "X": "What is the torch.abs() Tensor.abs_ In-place version of?",
        "Z": "Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "Y": "abs() Tensor.absolute"
    },
    {
        "X": "What is used for abs() Tensor.absolute?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "Y": "Alias"
    },
    {
        "X": "What is the name of the device where this Tensor is?",
        "Z": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "Y": "torch.device"
    },
    {
        "X": "What does Tensor.ndim Alias for dim() Tensor.real Return a new tensor containing?",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "Where is the Tensor located?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "Y": "torch.device"
    },
    {
        "X": "Where is this Tensor located?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "Y": "torch.device"
    },
    {
        "X": "What Alias does torch.acos() provide for?",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() ",
        "Y": "abs_() Tensor.acos"
    },
    {
        "X": "What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for",
        "Z": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What is the default value of the attribute that becomes a Tensor the first time a call to backward() computes gradients for self",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() ",
        "Y": "None"
    },
    {
        "X": "What is the name of the Alias for abs?",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ ",
        "Y": "Tensor.acos"
    },
    {
        "X": "What is the default value of this attribute?",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "Y": "None by default"
    },
    {
        "X": "What Alias for abs() Tensor.absolute_ In-place version of absolute()?",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "Y": "abs() Tensor.absolute"
    },
    {
        "X": "What is another name for a tensor?",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "Y": "add"
    },
    {
        "X": "What does the new tensor contain?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "imaginary values of the self tensor"
    },
    {
        "X": "What Alias for abs() Tensor.absolute Returns a new tensor containing real values of the self",
        "Z": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "Y": "abs"
    },
    {
        "X": "Alias for dim() Tensor.real Returns a new tensor containing what?",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "What does a new tensor do?",
        "Z": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "Y": "add"
    },
    {
        "X": "What Returns a new tensor containing real values of the self tensor?",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() ",
        "Y": "real"
    },
    {
        "X": "What Returns a new tensor containing imaginary values of the self tensor?",
        "Z": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "Y": "Tensor.abs"
    },
    {
        "X": "Returns a new tensor containing what?",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "imaginary values of the self tensor"
    },
    {
        "X": "What does add to self tensor?",
        "Z": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr ",
        "Y": "a scalar or tensor"
    },
    {
        "X": "What type of tensor can be added to a self tensor?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax ",
        "Y": "scalar"
    },
    {
        "X": "What is the name of the In-place version of add() Tensor?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ ",
        "Y": "add"
    },
    {
        "X": "What is added to self tensor?",
        "Z": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv ",
        "Y": "scalar or tensor"
    },
    {
        "X": "What is the In-place version of Tensor.add?",
        "Z": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "Y": "add() Tensor"
    },
    {
        "X": "What does add add to self tensor?",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() ",
        "Y": "scalar or tensor"
    },
    {
        "X": "What is the In-place version of abs() Tensor?",
        "Z": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() ",
        "Y": "abs() Tensor"
    },
    {
        "X": "What is the name of the In-place version of addbmm() Tensor?",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "Y": "addbmm"
    },
    {
        "X": "What is the In-place version of addbmm() Tensor?",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "Y": "addbmm"
    },
    {
        "X": "What is the name of the in-place version of abs() Tensor?",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "abs() Tensor"
    },
    {
        "X": "Abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "abs() Tensor"
    },
    {
        "X": "What is the in-place version of Tensor?",
        "Z": "Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "abs() Tensor"
    },
    {
        "X": "What is the name of the in-place version of add() Tensor?",
        "Z": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "Y": "addbmm"
    },
    {
        "X": "What Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_()",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() ",
        "Y": "absolute"
    },
    {
        "X": "What is the In-place version of add() Tensor?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "add"
    },
    {
        "X": "What is the In-place version of Alias for abs() Tensor.absolute?",
        "Z": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "Y": "abs() Tensor.absolute"
    },
    {
        "X": "What does Tensor.absolute use for abs?",
        "Z": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "Y": "Alias"
    },
    {
        "X": "What is the name of the In-place version of addcdiv?",
        "Z": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "Y": "addcdiv"
    },
    {
        "X": "What do you add to a scalar or tensor to self tensor?",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). ",
        "Y": "a scalar or tensor to self tensor"
    },
    {
        "X": "What is the name of the in-place version of absolute() Alias for abs_() Tensor?",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Alias for abs() Tensor"
    },
    {
        "X": "What is the name of the function that adds a scalar or tensor to self tensor?",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "Y": "addcmul"
    },
    {
        "X": "What does add add add?",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "a scalar or tensor to self tensor"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs() Tensor?",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "absolute"
    },
    {
        "X": "What is used for abs_() Tensor?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ ",
        "Y": "Alias"
    },
    {
        "X": "What is the name of the in-place version of addcdiv?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ ",
        "Y": "addcmul"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs_() Tensor.acos?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ ",
        "Y": "acos() Tensor.arccos"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs_() Tensor?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "absolute"
    },
    {
        "X": "Alias for what?",
        "Z": "Alias for torch.mul(). ",
        "Y": "torch.mul()"
    },
    {
        "X": "When talking about storing only what type of elements of a sparse array, the usage of adjective \"non-zero\" is not",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "non-zero"
    },
    {
        "X": "What do we use \"specified elements\" for?",
        "Z": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "array elements that are actually stored"
    },
    {
        "X": "What are unspecified elements assumed to have?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "zero value"
    },
    {
        "X": "What does the term \"fill value\" mean?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note ",
        "Y": "Note"
    },
    {
        "X": "When talking about storing only what elements of a sparse array, the usage of adjective \u201cnon-zero\u201d is not strict:",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "non-zero"
    },
    {
        "X": "What do we use for those array elements that are actually stored?",
        "Z": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "specified elements"
    },
    {
        "X": "What term is used to denote unspecified elements?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "fill value"
    },
    {
        "X": "What can be advantageous only when the size and sparsity levels of arrays are high?",
        "Z": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "Y": "sparse storage format"
    },
    {
        "X": "What is the most efficient way to store low-sparsity arrays?",
        "Z": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "contiguous memory storage format"
    },
    {
        "X": "The PyTorch API of sparse tensors is in what state?",
        "Z": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "beta"
    },
    {
        "X": "What can a sparse storage format be advantageous for storing?",
        "Z": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "sparse arrays"
    },
    {
        "X": "What is the most efficient way to store sparse arrays?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "contiguous memory storage format"
    },
    {
        "X": "What API of sparse tensors is in beta?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "PyTorch API"
    },
    {
        "X": "When can using a sparse storage format be advantageous?",
        "Z": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "when the size and sparsity levels of arrays are high"
    },
    {
        "X": "What is the most efficient approach for small-sized or low-sparsity arrays?",
        "Z": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "Y": "contiguous memory storage format"
    },
    {
        "X": "In what state is the PyTorch API of sparse tensors?",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "beta"
    },
    {
        "X": "What does PyTorch implement as one of the storage formats for implementing sparse tensors?",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "Coordinate format"
    },
    {
        "X": "What is another name for Coordinate format?",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "Y": "COO format"
    },
    {
        "X": "What are the specified elements stored as in COO format?",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "tuples of element indices and the corresponding values"
    },
    {
        "X": "In what format are the indices of specified elements collected?",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "Y": "indices tensor of size"
    },
    {
        "X": "What is the storage format for sparse tensors?",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "Coordinate format"
    },
    {
        "X": "What are the indices of specified elements collected in?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "Y": "indices tensor of size"
    },
    {
        "X": "What is the name of the indices of specified elements in COO format?",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "Y": "torch.int64"
    },
    {
        "X": "Which API of sparse tensors is in beta?",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "PyTorch"
    },
    {
        "X": "What does PyTorch implement?",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "Y": "Coordinate format"
    },
    {
        "X": "PyTorch implements what format for sparse tensors?",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "Y": "Coordinate format"
    },
    {
        "X": "What is the name of the element type in the COO format?",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "Y": "torch"
    },
    {
        "X": "What is the dimensionality of the tensor and nse is the number of specified elements?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "Y": "ndim"
    },
    {
        "X": "The indices of specified elements are collected in indices tensor of size (ndim) and with element type torch.int",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "Y": "nse"
    },
    {
        "X": "What is another name for indices tensor of size?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "nse"
    },
    {
        "X": "In indices tensor of size, what is the dimensionality of the tensor?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "Y": "ndim"
    },
    {
        "X": "What is the tensor of size?",
        "Z": "the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). ",
        "Y": "nse"
    },
    {
        "X": "What is the dimensionality of the tensor?",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "Y": "ndim"
    },
    {
        "X": "What is the memory consumption of a sparse COO tensor?",
        "Z": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "Y": "nse bytes"
    },
    {
        "X": "The memory consumption of a sparse COO tensor is at least what?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "Y": "nse bytes"
    },
    {
        "X": "What is the memory consumption of a strided tensor?",
        "Z": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "Y": "product"
    },
    {
        "X": "What is the number of specified elements in a tensor?",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "nse"
    },
    {
        "X": "What tensor is at least product(tensor shape>) * size of element type in bytes>?",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "strided tensor"
    },
    {
        "X": "What type of tensor is at least product(tensor shape>) * size of element type in bytes>?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "strided tensor"
    },
    {
        "X": "What is the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "100 000 = 2 000 000 bytes"
    },
    {
        "X": "How much memory saving occurs when using the COO storage format?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "Y": "200 fold"
    },
    {
        "X": "What is the memory consumption of a 10 000 x 10 000 tensor with COO tensor layout?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "Y": "2 000 000 bytes"
    },
    {
        "X": "What is the memory saving from using the COO storage format?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "200 fold"
    },
    {
        "X": "How can a sparse COO tensor be constructed?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "by providing the two tensors of indices and values, as well as the size of the sparse tensor"
    },
    {
        "X": "What is the default value of the sparse tensor?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: ",
        "Y": "1, 0"
    },
    {
        "X": "What is the default value of a sparse COO tensor?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "zero"
    },
    {
        "X": "What would we do to define a sparse COO tensor?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: ",
        "Y": "write"
    },
    {
        "X": "What is the location of the sparse tensor?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "entry 3"
    },
    {
        "X": "What is the default value of the fill value?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "zero"
    },
    {
        "X": "What should we note about the input i?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "the input i is NOT a list of index tuples"
    },
    {
        "X": "How can an empty sparse COO tensor be constructed?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "specifying its size"
    },
    {
        "X": "What is the default value of a sparse tensor?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "zero"
    },
    {
        "X": "What would we write to define a sparse tensor?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "i"
    },
    {
        "X": "The input i is NOT a list of what?",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "index tuples"
    },
    {
        "X": "What implements an extension of sparse tensors with scalar values to sparse tensors with (",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "Pytorch"
    },
    {
        "X": "What are sparse tensors with contiguous tensor values called?",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "Y": "hybrid tensors"
    },
    {
        "X": "Pytorch implements an extension of sparse tensors with what values?",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "Y": "scalar"
    },
    {
        "X": "What are sparse tensors with (contiguous) tensor values called?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "hybrid tensors"
    },
    {
        "X": "The indices of specified elements are collected in what?",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "Y": "indices tensor of size"
    },
    {
        "X": "What can be constructed by specifying its size only?",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "Y": "empty sparse COO tensor"
    },
    {
        "X": "What do Pytorch extend sparse tensors with?",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "Y": "scalar values"
    },
    {
        "X": "Which hybrid COO tensor extends the sparse COO tensor?",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "PyTorch"
    },
    {
        "X": "What does PyTorch hybrid COO tensor do?",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note ",
        "Y": "Note"
    },
    {
        "X": "What extends the sparse COO tensor?",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "PyTorch hybrid COO tensor"
    },
    {
        "X": "The indices of specified elements are collected in what indices of size?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "Y": "tensor"
    },
    {
        "X": "The indices of specified elements are collected in indices tensor of size (sparse_dims, dense_",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "Y": "nse"
    },
    {
        "X": "What is the name of the dimensional tensor to denote a N-dimensional hybrid sparse tensor?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "M + K"
    },
    {
        "X": "What are the numbers of sparse and dense dimensions?",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "M + K"
    },
    {
        "X": "What are the corresponding values of nse, dense_dims, and with an arbitrary integer or floating point number element type?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "Y": "tensor"
    },
    {
        "X": "What are the values tensor of size?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "Y": "nse, dense_dims"
    },
    {
        "X": "What is the number of sparse and dense dimensions?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "M + K"
    },
    {
        "X": "What is the name of the entry in a 2 + 1-dimensional tensor?",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "entry [3, 4] at location (0, 2)"
    },
    {
        "X": "What does M =?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "Y": "s.sparse_dim()"
    },
    {
        "X": "Where is the entry [5, 6] in a sparse tensor?",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "(1, 0"
    },
    {
        "X": "What type of tensor is s?",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "sparse COO tensor"
    },
    {
        "X": "What is the name of the sparse COO tensor?",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "s.sparse_dim()"
    },
    {
        "X": "What is the location of a sparse tensor?",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "(1, 0"
    },
    {
        "X": "What is the sum of the number of sparse and dense dimensions?",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "dimensionality"
    },
    {
        "X": "Where is the entry [5, 6] in a 2 + 1-dimensional tensor?",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "(1, 0"
    },
    {
        "X": "If s is a COO tensor and M = s.sparse_dim(), K = s",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "sparse"
    },
    {
        "X": "s.values().layout == torch.strided - values are stored as what?",
        "Z": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "strided tensors"
    },
    {
        "X": "What is the meaning of strided tensors?",
        "Z": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the dimensionality of a tensor the sum of the number of sparse and dense dimensions?",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "Y": "s.ndim"
    },
    {
        "X": "Dense dimensions always follow what?",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "Y": "sparse dimensions"
    },
    {
        "X": "What are values stored as strided tensors?",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "strided"
    },
    {
        "X": "What are values stored as?",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "Y": "strided tensors"
    },
    {
        "X": "What do dense dimensions always follow?",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "sparse dimensions"
    },
    {
        "X": "What is not supported when mixing sparse and dense dimensions?",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "dense"
    },
    {
        "X": "What permits uncoalesced sparse tensors?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "Y": "PyTorch sparse COO tensor format"
    },
    {
        "X": "What is an uncoalesced tensor?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "1-D"
    },
    {
        "X": "What is the uncoalesced tensor?",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "1-D"
    },
    {
        "X": "What uncoalesced tensor can be created when multiple values are specified for the same index?",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: ",
        "Y": "1-D"
    },
    {
        "X": "What is the value at an uncoalesced sparse COO tensor?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "Y": "the sum of all duplicate value entries"
    },
    {
        "X": "What does the coalescing process use to accumulate multi-valued elements into a single value?",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. ",
        "Y": "summation"
    },
    {
        "X": "What process will accumulate the multi-valued elements into a single value using summation?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "Y": "the coalescing process"
    },
    {
        "X": "What does the torch.Tensor.is_coalesced() return True?",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "Note"
    },
    {
        "X": "What are the properties of the output of torch.Tensor.coalesce() method?",
        "Z": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, ",
        "Y": "the indices of specified tensor elements are unique"
    },
    {
        "X": "What is the result of the torch.Tensor.is_coalesced() method?",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "Note"
    },
    {
        "X": "What does torch.Tensor.is_coalesced() return?",
        "Z": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "True"
    },
    {
        "X": "What type of sparse tensor will most operations work identically given?",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "Y": "coalesced or uncoalesced sparse tensor"
    },
    {
        "X": "In what order are the indices of specified tensor elements sorted?",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "lexicographical order"
    },
    {
        "X": "What happens when a sparse tensor is coalesced or uncoalesced?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "most operations will work identically"
    },
    {
        "X": "What is a sparse tensor?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "coalesced or not"
    },
    {
        "X": "Some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "Y": "coalesced tensors"
    },
    {
        "X": "What type of tensor is coalesced?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "Y": "sparse tensor"
    },
    {
        "X": "Some operations can be implemented more efficiently on what?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "Y": "coalesced tensors"
    },
    {
        "X": "In what order are the indices sorted?",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "lexicographical order"
    },
    {
        "X": "Some operations can be implemented more efficiently on uncoalesced what?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "coalesced tensors"
    },
    {
        "X": "How is addition of sparse COO tensors implemented?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "concatenating the indices and values tensors"
    },
    {
        "X": "What should you not care about a sparse tensor being?",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "Y": "coalesced or not"
    },
    {
        "X": "What is implemented by simply concatenating the indices and values tensors?",
        "Z": "s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "addition of sparse COO tensors"
    },
    {
        "X": "What can be advantageous for implementing algorithms that involve many element selection operations?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "lexicographical ordering"
    },
    {
        "X": "What should you do to your sparse tensors to prevent them from growing too large?",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "coalesce"
    },
    {
        "X": "What is an example of a lexicographical ordering of indices?",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "Let\u2019s consider the following example"
    },
    {
        "X": "What should you do with sparse tensors to prevent them from growing too large?",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "coalesce"
    },
    {
        "X": "What is an example of lexicographical ordering of indices?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "example"
    },
    {
        "X": "What is a torch.Tensor instance?",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "a sparse COO tensor"
    },
    {
        "X": "What is the name of a sparse COO tensor?",
        "Z": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "torch"
    },
    {
        "X": "What methods can be used to acquire the COO format data of a sparse COO tensor?",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "methods torch.Tensor.indices() and torch.Tensor.values()"
    },
    {
        "X": "What does a sparse COO tensor do?",
        "Z": "As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note ",
        "Y": "Note"
    },
    {
        "X": "How can the number of sparse and dense dimensions be acquired?",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "Y": "methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim()"
    },
    {
        "X": "Currently, one can acquire the COO format data only when the tensor instance is what?",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "coalesced"
    },
    {
        "X": "What dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "Y": "sparse and dense"
    },
    {
        "X": "If s is a what, then its COO format data can be acquired using methods torch.Tensor.indices() and torch",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "Y": "sparse COO tensor"
    },
    {
        "X": "For acquiring the COO format data of what tensor, use torch.Tensor._values() and torch.T",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "Y": "uncoalesced"
    },
    {
        "X": "When can one acquire the COO format data?",
        "Z": "Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. ",
        "Y": "coalesced"
    },
    {
        "X": "What type of tensor is uncoalesced?",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "coalesced"
    },
    {
        "X": "What can one construct of a sparse COO tensor using the torch.Tensor.coalesce() method?",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "coalesced copy"
    },
    {
        "X": "What method can be used to construct a coalesced copy of a sparse COO tensor?",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "torch.Tensor.coalesce()"
    },
    {
        "X": "What can be used to construct a coalesced copy of a sparse COO tensor?",
        "Z": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "the torch"
    },
    {
        "X": "What method can be used to create a coalesced copy of a sparse COO tensor?",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "torch"
    },
    {
        "X": "What can one construct of a sparse COO tensor using the torch?",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "coalesced copy"
    },
    {
        "X": "What can be used to build a coalesced copy of a sparse COO tensor?",
        "Z": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "torch"
    },
    {
        "X": "What is supported only for dense dimensions?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "Slicing"
    },
    {
        "X": "In PyTorch, the fill value of a sparse tensor is assumed to be what?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "zero"
    },
    {
        "X": "Is the fill value of a sparse tensor assumed to be zero in general?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "there exists operations that may interpret the fill value differently"
    },
    {
        "X": "What computes the softmax with the assumption that the fill value is negative infinity?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "torch.sparse.softmax"
    },
    {
        "X": "Slicing of a sparse COO tensor is supported only for what dimensions?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "dense dimensions"
    },
    {
        "X": "What is the fill value of a sparse tensor assumed to be?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "zero"
    },
    {
        "X": "What can be done with the fill value of a sparse tensor?",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "Y": "operations that may interpret the fill value differently"
    },
    {
        "X": "What implements the CSR format for storage of 2 dimensional tensors?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "The CSR (Compressed Sparse Row) sparse tensor format"
    },
    {
        "X": "What two backends are used in sparse matrix-vector multiplication?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. ",
        "Y": "MKL and MAGMA backends"
    },
    {
        "X": "What does not exist as of now?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. ",
        "Y": "CUDA"
    },
    {
        "X": "What are the three 1-D tensors in a CSR sparse tensor?",
        "Z": "A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "crow_indices, col_indices and values"
    },
    {
        "X": "What does CSR stand for?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: ",
        "Y": "Compressed Sparse Row"
    },
    {
        "X": "What is the primary advantage over the COO format?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "better use of storage and much faster computation operations"
    },
    {
        "X": "What does the crow_indices tensor consist of?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "compressed row indices"
    },
    {
        "X": "What is the crow_indices tensor?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "1-D tensor of size size[0] + 1"
    },
    {
        "X": "What is the last element of a CSR sparse tensor?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "number of non-zeros"
    },
    {
        "X": "What encodes the index in values and col_indices depending on where the given row starts?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "tensor"
    },
    {
        "X": "What does each successive number in the tensor subtracted by the number before it denote?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "the number of elements in a given row"
    },
    {
        "X": "What is the size of the crow_indices tensor?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "1-D tensor"
    },
    {
        "X": "The crow_indices tensor encodes the index in values and col_indices depending on what?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "where the given row starts"
    },
    {
        "X": "What is the last element of the crow_indices tensor?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "number of non-zeros"
    },
    {
        "X": "The crow_indices tensor encodes the index in values and what else depending on where the given row starts?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. ",
        "Y": "col_indices"
    },
    {
        "X": "What contains the column indices of each value?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "col_indices tensor"
    },
    {
        "X": "What is the col_indices tensor?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "1-D tensor of size nnz"
    },
    {
        "X": "Each successive number in the tensor subtracted by the number before it denotes what?",
        "Z": "A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "number of elements in a given row"
    },
    {
        "X": "What does the col_indices tensor contain?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "column indices of each value"
    },
    {
        "X": "What tensor contains the column indices of each value?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "col_indices"
    },
    {
        "X": "What contains the values of the CSR tensor?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "values tensor"
    },
    {
        "X": "What is the CSR tensor?",
        "Z": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "1-D tensor"
    },
    {
        "X": "The index tensors crow_indices and col_indices should have element type either what?",
        "Z": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "torch.int64"
    },
    {
        "X": "If you want to use MKL-enabled matrix operations, use what?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "torch.int32"
    },
    {
        "X": "What does MKL LP64 use?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "32 bit integer indexing"
    },
    {
        "X": "What is the col_indices tensor of size nnz?",
        "Z": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "1-D tensor"
    },
    {
        "X": "What tensor contains the values of the CSR tensor?",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "values"
    },
    {
        "X": "What is the size of the CSR tensor?",
        "Z": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "1-D"
    },
    {
        "X": "What is the default linking of pytorch?",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "MKL LP64"
    },
    {
        "X": "If you want to use MKL-enabled matrix operations, what should you use?",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "torch.int32"
    },
    {
        "X": "What is the default element type for crow_indices and col_indices?",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "Y": "torch.int64"
    },
    {
        "X": "What is the default element type for index tensors?",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "torch.int64"
    },
    {
        "X": "The index tensors crow_indices and col_indices should have element type what?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "torch.int64"
    },
    {
        "X": "What is the name of the method used to construct sparse CSR matrices?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "torch"
    },
    {
        "X": "What must the user supply separately?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. ",
        "Y": "row and column indices and values tensors"
    },
    {
        "X": "The size argument is optional and will be deduced from what if it is not present?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. ",
        "Y": "crow_indices and col_indices"
    },
    {
        "X": "What is the simplest way of constructing a sparse CSR from a strided or sparse COO tens",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "tensor"
    },
    {
        "X": "Any zeros in the (strided) tensor will be interpreted as what in the sparse tensor?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "missing values"
    },
    {
        "X": "What method can be used to construct sparse CSR matrices?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "torch"
    },
    {
        "X": "Is the size argument optional or optional?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "optional"
    },
    {
        "X": "Any zeros in the strided tensor will be interpreted as what in the sparse tensor?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "missing values"
    },
    {
        "X": "What is the simplest way of constructing a sparse CSR tensor from a strided or sparse CO",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "tensor._to_sparse_csr()"
    },
    {
        "X": "The sparse matrix-vector multiplication can be performed with what method?",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "tensor.matmul()"
    },
    {
        "X": "The sparse matrix-vector multiplication is the only math operation supported on what?",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "CSR tensors"
    },
    {
        "X": "Any zeros in the strided tensor will be interpreted as what?",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "missing values"
    },
    {
        "X": "The sparse matrix-vector multiplication is currently the only what operation supported on CSR tensors?",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "math operation"
    },
    {
        "X": "What method can be used to construct Sparse CSR matrices?",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. ",
        "Y": "torch._sparse_csr_tensor() method"
    },
    {
        "X": "The following table summarizes supported what on sparse matrices?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "Y": "Linear Algebra operations"
    },
    {
        "X": "What denotes a tensor with a given layout?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "Y": "T"
    },
    {
        "X": "What does M[layout] denote?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "Y": "matrix"
    },
    {
        "X": "What denotes a scalar?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "Y": "f"
    },
    {
        "X": "What is a PyTorch operation?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "Y": "Sparse grad?"
    },
    {
        "X": "What is Layout signature torch.mv()?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "Y": "no"
    },
    {
        "X": "The following table summarizes what on sparse matrices where the operands layouts may vary?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "supported Linear Algebra operations"
    },
    {
        "X": "What does T[layout] denote with a given layout?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "a tensor"
    },
    {
        "X": "What is the name of the PyTorch operation?",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "Y": "Sparse grad?"
    },
    {
        "X": "What is the name of the Layout signature?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "Y": "torch.mv()"
    },
    {
        "X": "What is the default value for M[sparse_coo] at V[strided] -> V[stride",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "Y": "no"
    },
    {
        "X": "What is the name of the PyTorch operation Sparse grad?",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "Y": "PyTorch operation Sparse grad"
    },
    {
        "X": "What does torch.mv() no M[sparse_coo] @ V[strided] -> V[s",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "Y": "Layout signature"
    },
    {
        "X": "What is the term for a hybrid sparse?",
        "Z": "Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] ",
        "Y": "Sparse grad"
    },
    {
        "X": "What is the name of the Sparse grad at V[strided] -> V[strided] torch?",
        "Z": "Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What is the value of M[sparse_coo] at V[strided] -> V[strided",
        "Z": "Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "no"
    },
    {
        "X": "What is not present at V[strided] -> V[strided] torch?",
        "Z": "Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does torch.mv() say about M[sparse_coo] at V[strided] -> V[",
        "Z": "torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "no"
    },
    {
        "X": "What does torch.mv() no?",
        "Z": "torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What is the number of M[sparse_coo] at V[strided] -> V[strided",
        "Z": "no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "no"
    },
    {
        "X": "What does no M[sparse_coo] @ V[strided] -> V[strided] torch",
        "Z": "no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does.mv() no M[sparse_csr] @ V[strided] -> V[",
        "Z": "M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "M[sparse_coo] @ V[strided] -> V[strided] torch"
    },
    {
        "X": "What does M[sparse_coo] do at V[strided] -> V[strided] torch",
        "Z": "M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What is the value of the M[sparse_csr] at V[strided] -> V[stri",
        "Z": "torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "no"
    },
    {
        "X": "What is the name of the node that does not have a T[sparse_coo] at T[strided",
        "Z": "torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "addmm"
    },
    {
        "X": "What is the number of M[sparse_csr] at V[strided] -> V[stride",
        "Z": "no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "no"
    },
    {
        "X": "What does no M[sparse_coo] @ M[strided] -> M[strided] torch",
        "Z": "no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "addmm"
    },
    {
        "X": "What does matmul() no M[sparse_coo] @ M[strided] -> M[stride",
        "Z": "M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "M[sparse_csr]"
    },
    {
        "X": "Matmul() no what @ M[strided] -> M[strided] torch?",
        "Z": "M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "Where does M[sparse_coo] come from?",
        "Z": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[strided]"
    },
    {
        "X": "What does torch.matmul() no?",
        "Z": "torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes ",
        "Y": "M[sparse_csr]"
    },
    {
        "X": "When does no M[sparse_coo] occur?",
        "Z": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[strided]"
    },
    {
        "X": "What does M[sparse_coo] do at M[strided] -> M[strided] torch",
        "Z": "M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "Where does M[sparse_csr] come from?",
        "Z": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[strided]"
    },
    {
        "X": "When does no M[sparse_csr] occur?",
        "Z": "no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes ",
        "Y": "M[strided]"
    },
    {
        "X": "What does M[sparse_csr] do at M[strided] -> M[strided]",
        "Z": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_csr]"
    },
    {
        "X": "What @ M[strided] -> M[strided] torch?",
        "Z": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes ",
        "Y": "M[sparse_csr]"
    },
    {
        "X": "What does torch.mm() no?",
        "Z": "torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does M[sparse_coo] @ M[strided] -> M[strided] torch?",
        "Z": "no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does torch.sparse.mm() say at M[strided] -> M[strided] torch?",
        "Z": "torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does torch.sparse.mm() yes?",
        "Z": "torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does M[strided] -> M[strided] torch have?",
        "Z": "M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does lobpcg stand for?",
        "Z": "yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no ",
        "Y": "lobpcg"
    },
    {
        "X": "What does torch.smm() no at M[strided] -> M[sparse_coo] torch?",
        "Z": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does torch.smm() no?",
        "Z": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does not exist at M[strided] -> M[sparse_coo] torch?",
        "Z": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does M[sparse_coo] do at M[strided] -> M[sparse_co",
        "Z": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does M[sparse_coo] mean?",
        "Z": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "Where is M[sparse_coo] at?",
        "Z": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[strided]"
    },
    {
        "X": "What @ M[strided] -> M[sparse_coo] torch?",
        "Z": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does torch.hspmm() no?",
        "Z": "torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does not exist at M[strided] -> M[hybrid sparse_coo] torch?",
        "Z": "no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does M[sparse_coo] @ M[strided] -> M[hybrid sparse",
        "Z": "no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes ",
        "Y": "no"
    },
    {
        "X": "What does M[hybrid sparse_coo] mean?",
        "Z": "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What @ M[strided] -> M[hybrid sparse_coo] torch?",
        "Z": "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does f * mean?",
        "Z": "f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does sspaddmm() no?",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "Y": "f * M[sparse_coo]"
    },
    {
        "X": "What is the answer to T[sparse_coo] at T[strided]?",
        "Z": "no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "Y": "no"
    },
    {
        "X": "What is T[sparse_coo] @ T[strided] -> T[strided] torch?",
        "Z": "no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "Y": "no"
    },
    {
        "X": "What is the acronym for \"M[sparse_coo]\"?",
        "Z": "T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "Y": "PCA"
    },
    {
        "X": "What does M[strided] + f * mean at M[strided]?",
        "Z": "no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does f * M[strided] + f * (M[sparse_coo] @ M[s",
        "Z": "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does f * M[strided] + f * mean at M[strided]?",
        "Z": "torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What is M[sparse_coo]?",
        "Z": "yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "SVD"
    },
    {
        "X": "What does M[sparse_coo] stand for?",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "Y": "SVD"
    },
    {
        "X": "What does torch.sspaddmm() no f *?",
        "Z": "torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What does torch.sspaddmm() no f * (M[sparse_coo] @ M[stride",
        "Z": "torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "Y": "M[sparse_coo]"
    },
    {
        "X": "What is true if the Tensor uses sparse storage layout?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Tensor.is_sparse Is True"
    },
    {
        "X": "What returns the number of dense dimensions in a sparse tensor self?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.dense_dim"
    },
    {
        "X": "What returns the number of sparse dimensions in a sparse tensor self?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.sparse_dim"
    },
    {
        "X": "What are the values of the strided tensor self filtered by?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. ",
        "Y": "indices of the sparse tensor mask"
    },
    {
        "X": "What does Tensor.to_sparse return?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "a sparse copy of the tensor"
    },
    {
        "X": "What does Tensor.is_sparse use if the Tensor uses?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. ",
        "Y": "sparse storage layout"
    },
    {
        "X": "What is the number of dense dimensions in a sparse tensor self?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "dense"
    },
    {
        "X": "What is returned in a sparse tensor self?",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "number of dense dimensions"
    },
    {
        "X": "What are the values from a strided tensor self filtered by?",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. ",
        "Y": "indices of the sparse tensor mask"
    },
    {
        "X": "What are the values of the sparse tensor mask filtered by?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. ",
        "Y": "indices"
    },
    {
        "X": "Tensor._to_sparse_csr Convert a tensor to what format?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "compressed row storage format"
    },
    {
        "X": "Is True if the Tensor uses sparse storage layout?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "False"
    },
    {
        "X": "What _dim Return the number of sparse dimensions in a sparse tensor self?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "sparse"
    },
    {
        "X": "What format does Tensor._to_sparse_csr convert a tensor to?",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ ",
        "Y": "compressed row storage format"
    },
    {
        "X": "What returns a new sparse tensor with values from a strided tensor self filtered by the indice",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.sparse_mask"
    },
    {
        "X": "What is used to convert a tensor to compressed row storage format?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "Y": "Tensor.indices"
    },
    {
        "X": "What does the sparse tensor mask filter?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "Y": "indices"
    },
    {
        "X": "What Converts a tensor to compressed row storage format?",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "Y": "Tensor._to_sparse_csr"
    },
    {
        "X": "What does the new sparse tensor return values from?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "a strided tensor self filtered by the indices of the sparse tensor mask"
    },
    {
        "X": "What returns the indices tensor of a sparse COO tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.indices"
    },
    {
        "X": "What Returns the number of dense dimensions in a sparse tensor self?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.dense_dim"
    },
    {
        "X": "What return the number of sparse dimensions in a sparse tensor self?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "sparse"
    },
    {
        "X": "What Convert a tensor to compressed row storage format?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor._to_sparse_csr"
    },
    {
        "X": "What does a sparse tensor self return?",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. ",
        "Y": "the number of sparse dimensions"
    },
    {
        "X": "What returns a sparse copy of the tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.to_sparse"
    },
    {
        "X": "Return the number of dense dimensions in a what tensor self?",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values ",
        "Y": "sparse"
    },
    {
        "X": "What returns the values tensor of a sparse COO tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.values"
    },
    {
        "X": "What does Tensor.sparse_dim return?",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. ",
        "Y": "the number of sparse dimensions in a sparse tensor self"
    },
    {
        "X": "What Returns the indices tensor of a sparse COO tensor?",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "Y": "Tensor.indices"
    },
    {
        "X": "What Returns the values tensor of a sparse COO tensor?",
        "Z": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "Tensor.values"
    },
    {
        "X": "What is the return value of a sparse tensor?",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense ",
        "Y": "number of sparse dimensions in a sparse tensor self"
    },
    {
        "X": "What Tensor method is specific to sparse COO tensors?",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "Y": "Tensor.coalesce"
    },
    {
        "X": "Where are the values of a new sparse tensor retrieved from?",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "Y": "a strided tensor self filtered by the indices of the sparse tensor mask"
    },
    {
        "X": "What is the sparse tensor mask filtered by?",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "Y": "indices"
    },
    {
        "X": "What are the following Tensor methods specific to sparse COO tensors?",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "Y": "Tensor.coalesce"
    },
    {
        "X": "What type of copy of self does Tensor.coalesce return if self is an uncoalesced tensor?",
        "Z": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "coalesced"
    },
    {
        "X": "What Tensor method returns a coalesced copy of self if self is an uncoalesced tensor?",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ ",
        "Y": "Tensor.sparse_resize_"
    },
    {
        "X": "What is returned if self is an uncoalesced tensor?",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ ",
        "Y": "self"
    },
    {
        "X": "What returns a coalesced copy of self if self is an uncoalesced tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.coalesce"
    },
    {
        "X": "What type of copy of the tensor does Tensor._to_sparse_csr return?",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. ",
        "Y": "sparse"
    },
    {
        "X": "What resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.sparse_resize"
    },
    {
        "X": "What type of tensor does Tensor._to_sparse_csr return?",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "Y": "a sparse copy"
    },
    {
        "X": "Returns a coalesced copy of self if self is an what type of tensor?",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "uncoalesced"
    },
    {
        "X": "What type of tensor is self?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "uncoalesced"
    },
    {
        "X": "What does Tensor.sparse_resize_ Resize self sparse tensor to?",
        "Z": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "the desired size and the number of sparse and dense dimensions"
    },
    {
        "X": "What is the name of the Tensor method that resizes a sparse tensor?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "If self is an uncoalesced tensor, what is returned?",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "if self is an uncoalesced tensor"
    },
    {
        "X": "What is the name of the tensor method that resizes a sparse tensor?",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What format can a tensor be converted to?",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "Y": "compressed row storage format"
    },
    {
        "X": "What is one way to convert a tensor to a compressed row storage format?",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Convert a tensor to compressed row storage format"
    },
    {
        "X": "What does Tensor.sparse_resize_ Resize?",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "self sparse tensor"
    },
    {
        "X": "What is the name of the Tensor method that resizes a sparse COO tensor?",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What is another name for sparse tensors?",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What is the tensor of a sparse COO tensor?",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "Y": "indices"
    },
    {
        "X": "Return the tensor of a sparse COO tensor?",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "indices"
    },
    {
        "X": "What does Tensor.sparse_resize resize?",
        "Z": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "self sparse tensor"
    },
    {
        "X": "What happens when a sparse tensor is removed?",
        "Z": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "resizes self to the desired size and the number of sparse and dense dimensions"
    },
    {
        "X": "What does Tensor.sparse_resize_and_clear_ do?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Removes all specified elements"
    },
    {
        "X": "What does return the value of a sparse COO tensor?",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "values tensor of a sparse COO tensor"
    },
    {
        "X": "What removes all specified elements from a sparse tensor?",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What is returned by the value tensor of a sparse COO tensor?",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "Return the values tensor of a sparse COO tensor"
    },
    {
        "X": "Returns a coalesced copy of self if self is an what tensor?",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "uncoalesced"
    },
    {
        "X": "If self is a sparse COO tensor, what type of tensor is it?",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "uncoalesced"
    },
    {
        "X": "What is _coalesced?",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "Tensor.is"
    },
    {
        "X": "When does Tensor.coalesce return a coalesced copy of self?",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "if self is an uncoalesced tensor"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what returns true?",
        "Z": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: ",
        "Y": "False"
    },
    {
        "X": "What returns true if self is a sparse COO tensor that is coalesced?",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense ",
        "Y": "Tensor.to_dense"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what type of tensor is returned?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "uncoalesced"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what return does Tensor.is_co",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "True"
    },
    {
        "X": "What type of copy of self is returned if self is an uncoalesced tensor?",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "coalesced"
    },
    {
        "X": "What type of copy of self does Tensor.to_dense create?",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "strided"
    },
    {
        "X": "Returns what type of self if self is an uncoalesced tensor?",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "coalesced copy"
    },
    {
        "X": "What does Tensor.sparse_resize_and_clear_ do to a sparse tensor?",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: ",
        "Y": "Removes all specified elements"
    },
    {
        "X": "What does Tensor.to_dense create?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "strided copy of self"
    },
    {
        "X": "The following methods are specific to what?",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "sparse CSR tensors"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what value does Tensor.is_co",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "True"
    },
    {
        "X": "What resizes to the desired size and the number of sparse and dense dimensions?",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "self sparse tensor"
    },
    {
        "X": "Is self a sparse COO tensor that is coalesced?",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "False"
    },
    {
        "X": "What methods are specific to sparse CSR tensors?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "Y": "Tensor.crow_indices"
    },
    {
        "X": "Resizes self sparse tensor to what size?",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "the desired size"
    },
    {
        "X": "What method is specific to sparse CSR tensors?",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "Y": "Tensor.crow_indices"
    },
    {
        "X": "What happens to a sparse tensor?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "resizes self to the desired size and the number of sparse and dense dimensions"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what return value is returned?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "False"
    },
    {
        "X": "The following methods are specific to sparse what?",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "Y": "CSR tensors"
    },
    {
        "X": "What removes all specified elements from a sparse tensor self?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What does resize self to the desired size and the number of sparse and dense dimensions?",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "Removes all specified elements"
    },
    {
        "X": "What does tensor.col_indices return when self is a sparse CSR tensor of layout sparse",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "column indices of the self tensor"
    },
    {
        "X": "What does the tensor return when self is a sparse CSR tensor of layout sparse_csr",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "column indices"
    },
    {
        "X": "Returns True if self is a sparse COO tensor that is coalesced, what else?",
        "Z": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "False"
    },
    {
        "X": "What creates a strided copy of self?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.to_dense"
    },
    {
        "X": "What returns the tensor containing the compressed row indices of the self tensor when self is a sparse C",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.col_indices"
    },
    {
        "X": "Returns what if self is a sparse COO tensor that is coalesced?",
        "Z": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "True"
    },
    {
        "X": "What returns the tensor containing the column indices of the self tensor when self is a sparse CSR",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.col_indices"
    },
    {
        "X": "What does Tensor.crow_indices return when self is a sparse CSR tensor of layout sparse",
        "Z": "Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "tensor containing the compressed row indices of the self tensor"
    },
    {
        "X": "What does Tensor.col_indices return when self is a sparse CSR tensor of layout sparse_",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "column indices of the self tensor"
    },
    {
        "X": "The following Tensor methods support sparse what?",
        "Z": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "COO tensors"
    },
    {
        "X": "The following methods are specific to what type of CSR tensors?",
        "Z": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "sparse"
    },
    {
        "X": "What indices of the self tensor is returned when self is a sparse CSR tensor of layout spars",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "column"
    },
    {
        "X": "What do the following Tensor methods support?",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "The following Tensor methods support sparse COO tensors"
    },
    {
        "X": "What type of copy of self is created?",
        "Z": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "strided"
    },
    {
        "X": "Creates what copy of self?",
        "Z": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "strided copy"
    },
    {
        "X": "What indices does the tensor return when self is a sparse CSR tensor of layout sparse_",
        "Z": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "column"
    },
    {
        "X": "When self is a sparse CSR tensor of layout what?",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "sparse_csr"
    },
    {
        "X": "What does return when self is a sparse CSR tensor of layout sparse_csr?",
        "Z": "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "the tensor containing the compressed row indices"
    },
    {
        "X": "Returns the tensor containing what indices of the self tensor when self is a sparse CSR ",
        "Z": "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "column"
    },
    {
        "X": "Self is a sparse CSR tensor of what?",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "layout sparse_csr"
    },
    {
        "X": "When self is a sparse CSR tensor of layout, what does Tensor.col_indices return?",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "sparse_csr"
    },
    {
        "X": "What is a sparse CSR tensor of layout sparse_csr?",
        "Z": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "self"
    },
    {
        "X": "Returns what when self is a sparse CSR tensor of layout sparse_csr?",
        "Z": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "the tensor containing the column indices of the self tensor"
    },
    {
        "X": "What is the name of the method that supports sparse COO tensors?",
        "Z": "The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "t_()"
    },
    {
        "X": "What methods support sparse COO tensors?",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor methods"
    },
    {
        "X": "In what format is a sparse tensor constructed?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "COO(rdinate) format"
    },
    {
        "X": "What are the specified values for a sparse tensor in CSR?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "crow_indices and col_indices"
    },
    {
        "X": "What ensions dim does sparse.sum return the sum of each row of the sparse tensor input in?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "dim"
    },
    {
        "X": "What does sparse.addmm do exactly the same thing as?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "torch.addmm()"
    },
    {
        "X": "What is the name of the function that supports backward for sparse matrix mat1?",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm ",
        "Y": "sparse.mm"
    },
    {
        "X": "What are the specified values for the sparse tensor?",
        "Z": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   ",
        "Y": "crow_indices and col_indices"
    },
    {
        "X": "What type of ensions dim does sparse.sum return?",
        "Z": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   ",
        "Y": "dim"
    },
    {
        "X": "What does sparse.mm perform?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "matrix multiplication"
    },
    {
        "X": "What returns the sum of each row of the sparse tensor input in the given dimensions dim?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse.sum"
    },
    {
        "X": "What does sparse.addmm support backward for?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse matrix mat1"
    },
    {
        "X": "What does sparse.mm perform a matrix multiplication of?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse matrix mat1 and the (sparse or strided) matrix mat2"
    },
    {
        "X": "What multiplies a sparse tensor mat1 with a dense tensor mat2?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "Matrix"
    },
    {
        "X": "Returns the sum of each row of the sparse tensor input in what ensions dim?",
        "Z": "Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   ",
        "Y": "dim"
    },
    {
        "X": "What does sparse.addmm return?",
        "Z": "Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   ",
        "Y": "the sum of each row of the sparse tensor input"
    },
    {
        "X": "What does sparse.mm perform of the sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "Z": "sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "Y": "matrix multiplication"
    },
    {
        "X": "What performs a sparse COO matrix mat1 and a strided matrix mat2?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "matrix multiplication"
    },
    {
        "X": "Matrix performs a matrix multiplication of a sparse COO matrix mat1 and what else?",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "Y": "a strided matrix mat2"
    },
    {
        "X": "What does this function do exactly the same thing as in the forward?",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "Y": "torch.addmm()"
    },
    {
        "X": "What type of matrix does matrix multiplication perform?",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "Y": "sparse"
    },
    {
        "X": "What does this function support backward for?",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "Y": "sparse matrix mat1"
    },
    {
        "X": "What performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "sparse.mm"
    },
    {
        "X": "What does sparse.mm perform of a sparse COO matrix mat1 and a strided matrix mat2?",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "matrix multiplication"
    },
    {
        "X": "What is used to perform a matrix multiplication of the sparse matrix input?",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "dense matrix mat"
    },
    {
        "X": "What does sparse.softmax apply?",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "softmax function"
    },
    {
        "X": "What does sparse.log_apply?",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "softmax"
    },
    {
        "X": "What do sparse.mm perform a matrix multiplication of?",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "sparse COO matrix mat1 and a strided matrix mat2"
    },
    {
        "X": "What does sparse.mm perform a matrix multiplication of with the dense matrix mat?",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "Y": "sparse matrix input"
    },
    {
        "X": "What Applies a softmax function?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse.softmax"
    },
    {
        "X": "What applies a softmax function?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "softmax"
    },
    {
        "X": "What is performed of the sparse matrix mat1 and the (sparse or strided) matrix mat2?",
        "Z": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "matrix multiplication"
    },
    {
        "X": "Matrix performs a matrix multiplication of what?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse COO matrix mat1 and a strided matrix mat2"
    },
    {
        "X": "Matrix multiplies sparse matrix input with what?",
        "Z": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "dense matrix mat"
    },
    {
        "X": "What Applies a softmax function followed by logarithm?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse.log_softmax"
    },
    {
        "X": "Which torch function supports sparse tensors?",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "Y": "hstack()"
    },
    {
        "X": "What is_signed() is_tensor()?",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "Y": "same_size()"
    },
    {
        "X": "What is the name of the torch function that supports sparse tensors?",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "Y": "svd_lowrank()"
    },
    {
        "X": "What do the following torch functions support?",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "Y": "sparse tensors"
    },
    {
        "X": "Returns what with the sine of the elements of input?",
        "Z": "Returns a new tensor with the sine of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "a new tensor"
    },
    {
        "X": "What does torch.full_like(input, fill_value, layout=input.layout, device=input.device",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. ",
        "Y": "fill_value"
    },
    {
        "X": "What is equivalent to torch.full?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "torch.full_like"
    },
    {
        "X": "What is the size of input that determines the size of the output tensor?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "input"
    },
    {
        "X": "What is the number to fill the output tensor with?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "fill_value"
    },
    {
        "X": "What is the desired data type of returned Tensor?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "dtype"
    },
    {
        "X": "What defaults to the dtype of input?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "if None"
    },
    {
        "X": "What is returned with the same size as input filled with fill_value?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "tensor"
    },
    {
        "X": "What is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "torch.full"
    },
    {
        "X": "What is fill_value?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. ",
        "Y": "the number to fill the output tensor with"
    },
    {
        "X": "dtype (torch.dtype, optional) \u2013 what?",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "desired data type of returned tensor"
    },
    {
        "X": "What happens if None?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. ",
        "Y": "defaults to the dtype of input"
    },
    {
        "X": "What determines size of the output tensor?",
        "Z": "input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "Y": "input"
    },
    {
        "X": "What is the desired layout of returned tensor?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "layout"
    },
    {
        "X": "What defaults to the layout of input?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "if None"
    },
    {
        "X": "What is the desired device of returned tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "device"
    },
    {
        "X": "What defaults to the device of input?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "if None"
    },
    {
        "X": "Default: if what, defaults to the dtype of input?",
        "Z": "fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "Y": "None"
    },
    {
        "X": "If none, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned ten",
        "Z": "fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "Y": "None"
    },
    {
        "X": "Default: if what, defaults to the device of input?",
        "Z": "fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "Y": "None"
    },
    {
        "X": "What should record operations on the returned tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "autograd"
    },
    {
        "X": "What is the default value for autograd to record operations on the returned tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "False"
    },
    {
        "X": "If None, defaults to the dtype of input?",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "Default"
    },
    {
        "X": "Default: if what, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "None"
    },
    {
        "X": "What does None do to the device of input?",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "defaults"
    },
    {
        "X": "What is the default setting for autograd to record operations on the returned tensor?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "False"
    },
    {
        "X": "What is the desired memory format of returned Tensor?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "memory_format"
    },
    {
        "X": "What is the default memory format of returned Tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "torch.preserve_format"
    },
    {
        "X": "Default: if None, what is the default to the device of input?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "defaults"
    },
    {
        "X": "If autograd should record operations on the returned tensor, what is optional?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "requires_grad"
    },
    {
        "X": "What is the default memory format of returned tensor?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "torch.preserve_format"
    },
    {
        "X": "What is STFT?",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "Short-time Fourier transform"
    },
    {
        "X": "What must always be given explicitly for real inputs?",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "return_complex"
    },
    {
        "X": "What function will only return complex tensors?",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. ",
        "Y": "return_complex=True"
    },
    {
        "X": "What can be used to recover a real tensor with an extra last dimension for real and imaginary components?",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "torch.view_as_real()"
    },
    {
        "X": "What does the STFT compute of short overlapping windows of the input?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "Y": "Fourier transform"
    },
    {
        "X": "What does the Fourier transform of short overlapping windows of the input give?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "Y": "frequency components of the signal"
    },
    {
        "X": "The interface of the STFT is modeled after what?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "Y": "librosa stft function"
    },
    {
        "X": "Ignoring what, the STFT computes the following expression?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "Y": "optional batch dimension"
    },
    {
        "X": "What computes the Fourier transform of short overlapping windows of the input?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "Y": "The STFT"
    },
    {
        "X": "Ignoring the optional batch dimension, this method computes what?",
        "Z": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "Y": "the following expression"
    },
    {
        "X": "The STFT computes the Fourier transform of short overlapping windows of the input to give what of the signal as they change over time?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "Y": "frequency components"
    },
    {
        "X": "What does the STFT compute without the optional batch dimension?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "Y": "the following expression"
    },
    {
        "X": "What is the index of the sliding window?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "mmm"
    },
    {
        "X": "Input must be either a time sequence or a 2-D batch of time sequences?",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). ",
        "Y": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "X": "If hop_length is None, it is treated as equal to what?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "floor(n_fft / 4)"
    },
    {
        "X": "Input must be either a what?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "X": "If hop_length is None (default), it is treated as equal to what?",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "floor(n_fft / 4)"
    },
    {
        "X": "Input must be a what?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "Y": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "X": "If win_length is what, it is treated as equal to n_fft?",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "None"
    },
    {
        "X": "If win_length is None (default), it is treated as equal to what?",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "n_fft"
    },
    {
        "X": "What must input be?",
        "Z": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "X": "If win_length is None, it is treated as equal to what?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "n_fft"
    },
    {
        "X": "What is the win_length of a window?",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "1-D tensor of size"
    },
    {
        "X": "If window is None (default), it is treated as if having what number everywhere in the window?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "111"
    },
    {
        "X": "What happens to window if win_lengthn_ffttextwin_lengthn_fft?",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "padded on both sides to length n_fft"
    },
    {
        "X": "What can a window be a size of win_length?",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "1-D tensor"
    },
    {
        "X": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "padded"
    },
    {
        "X": "What happens to window if win_lengthn_ffttextwin_length  textn",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "padded on both sides to length n_fft"
    },
    {
        "X": "What can a window be of size win_length?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "1-D tensor"
    },
    {
        "X": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "padded"
    },
    {
        "X": "If window is None (default), it is treated as if it has what number everywhere in the window?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "111"
    },
    {
        "X": "If window is what (default) value, it is treated as if having 111 everywhere in the window?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "None"
    },
    {
        "X": "What is centered at time thop_lengtht times texthop_lengththop_length",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "ttt-th frame"
    },
    {
        "X": "What begins at time thop_lengtht times texthop_lengththop_length?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "ttt-th frame"
    },
    {
        "X": "What determines the padding method used on input when center is True?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "pad_mode"
    },
    {
        "X": "What is the name of the function that determines the padding method used on input when center is True?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "torch.nn.functional.pad()"
    },
    {
        "X": "What is the default padding method used when center is True?",
        "Z": "pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "reflect"
    },
    {
        "X": "If center is True, input will be padded on both sides so that which frame is centered at time thop_lengtht ",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "ttt-th frame"
    },
    {
        "X": "What is the name of the padding method used on input when center is True?",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "torch.nn.functional.pad()"
    },
    {
        "X": "What is the default setting for padding when center is True?",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "\"reflect\""
    },
    {
        "X": "What is the default value of pad_mode?",
        "Z": "pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "reflect"
    },
    {
        "X": "What is the default for real input?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "True"
    },
    {
        "X": "What is not possible if the input or window tensors are complex?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "Y": "onesided output"
    },
    {
        "X": "If onesided is what (default for real input)?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "Y": "True"
    },
    {
        "X": "If what is true, the function returns the normalized STFT results?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "Y": "normalized is True"
    },
    {
        "X": "If return_complex is True, the return is what?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "Y": "input.dim()"
    },
    {
        "X": "If return_complex is False, the output is what?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "input.dim()"
    },
    {
        "X": "What is the default value of normalized?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "False"
    },
    {
        "X": "What represents the real and imaginary components?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "the last dimension"
    },
    {
        "X": "What is the return if return_complex is True?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "input.dim()"
    },
    {
        "X": "What is the last dimension of the input.dim() + 2 dimensional real tensor?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "the last dimension represents the real and imaginary components"
    },
    {
        "X": "If what is true, returns a complex tensor of size?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor ",
        "Y": "return_complex"
    },
    {
        "X": "What is the total number of frames used?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "TTT"
    },
    {
        "X": "When did this function change signature?",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "version 0.4.1"
    },
    {
        "X": "What may cause error or return incorrect result?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "Calling with the previous signature"
    },
    {
        "X": "Returns either a complex tensor of size (NT)(* times N times T)",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "if return_complex is true"
    },
    {
        "X": "What is TTT?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor ",
        "Y": "the total number of frames"
    },
    {
        "X": "Calling with the previous signature may cause what?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "Y": "error or return incorrect"
    },
    {
        "X": "What version of the function changed signature?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "0.4.1"
    },
    {
        "X": "What is a warning when calling with the previous signature?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "Calling with the previous signature may cause error or return incorrect result"
    },
    {
        "X": "What is the size of the distance between neighboring sliding window frames?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "Fourier transform hop_length"
    },
    {
        "X": "What is the default value of the input tensor?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "None"
    },
    {
        "X": "What is the default value for Win_length?",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "floor(n_fft / 4)"
    },
    {
        "X": "What is the default value for the size of window frame and STFT filter?",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) ",
        "Y": "None"
    },
    {
        "X": "What can happen if a function is called with a previous signature?",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) ",
        "Y": "Calling with the previous signature may cause error or return incorrect result"
    },
    {
        "X": "What is window (Tensor, optional)?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "optional window function"
    },
    {
        "X": "Default: None (treated as window of all how many s)",
        "Z": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "111"
    },
    {
        "X": "What is the size of window frame and STFT filter?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "win_length"
    },
    {
        "X": "What is the default window of all 111 s?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" ",
        "Y": "None"
    },
    {
        "X": "n_fft (int) \u2013 size of what?",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "Fourier transform hop_length"
    },
    {
        "X": "What is n_fft equal to?",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "floor"
    },
    {
        "X": "What is the size of Fourier transform hop_length?",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "n_fft"
    },
    {
        "X": "Default: None (treated as equal to n_fft) window (Tensor, what) \u2013 the optional window function",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "optional"
    },
    {
        "X": "What is the distance between neighboring sliding window frames?",
        "Z": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "hop_length"
    },
    {
        "X": "What is hop_length equal to?",
        "Z": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "floor(n_fft / 4)"
    },
    {
        "X": "Default: None (treated as window of all?",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "111 s"
    },
    {
        "X": "What controls the padding method used when center is True?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "True pad_mode"
    },
    {
        "X": "What is the default name of the padding method used when center is True?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" ",
        "Y": "\"reflect\""
    },
    {
        "X": "What is the default window function?",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "None"
    },
    {
        "X": "What is the default value for the padding method used when center is True?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" ",
        "Y": "reflect"
    },
    {
        "X": "What is the window function?",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "optional"
    },
    {
        "X": "What controls whether to return the normalized STFT results?",
        "Z": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "\"reflect\" normalized"
    },
    {
        "X": "Window (Tensor) \u2013 the optional window function. Default: None (treated as window of all 111 s)",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "optional"
    },
    {
        "X": "What is the default setting for return normalized STFT results?",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "False"
    },
    {
        "X": "What is center?",
        "Z": "center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "whether to pad input on both sides"
    },
    {
        "X": "What type of STFT results does \"reflect\" return?",
        "Z": "center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "normalized"
    },
    {
        "X": "What do you want to avoid with real inputs?",
        "Z": "pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "redundancy"
    },
    {
        "X": "What is the default for real input and window?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "True"
    },
    {
        "X": "What is the default value for normalized STFT results?",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "False"
    },
    {
        "X": "Default: False onesided (bool, optional) \u2013 controls whether to return what percentage of results?",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "half"
    },
    {
        "X": "What is the default value for real input and window?",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "True"
    },
    {
        "X": "What returns a complex tensor or a real tensor with an extra last dimension for the real and imaginary components?",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "return_complex"
    },
    {
        "X": "A tensor containing the STFT result with what described above Tensor containing the STFT result with?",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "shape"
    },
    {
        "X": "Default: False onesided (bool, optional) \u2013 controls whether to return half of results to avoid what for real inputs?",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "redundancy"
    },
    {
        "X": "What bool controls whether to return a complex tensor, or a real tensor with an extra last dimension for the",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "return_complex"
    },
    {
        "X": "What containing the STFT result with shape described above?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "tensor"
    },
    {
        "X": "What is this function analogous to?",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "Y": "NumPy\u2019s gradient function"
    },
    {
        "X": "What does dim represent?",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "Y": "the dimension or dimensions to approximate the gradient over"
    },
    {
        "X": "What is edge_order?",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "Y": "unsupported"
    },
    {
        "X": "What is the default value of edge_order?",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "Y": "Example"
    },
    {
        "X": "What does the number of bins in an array of non-negative ints do?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "Y": "Count the frequency of each value in an array of non-negative ints"
    },
    {
        "X": "What is the size of the number of bins in an array of non-negative ints?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "Y": "size 1"
    },
    {
        "X": "If what is specified, the number of bins is at least minlength and if input is empty, the result is tensor of size",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "Y": "minlength"
    },
    {
        "X": "What is the value at position i?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "n"
    },
    {
        "X": "What does the number of bins in an array of non-negative ints need to be one larger than the largest value in input?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the result of the number of bins (size 1) being one larger than the largest value in input unless input is empty?",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "Y": "tensor of size 0."
    },
    {
        "X": "What does the number of bins (size 1) have to be larger than the largest value in input?",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "Y": "Note"
    },
    {
        "X": "When given tensors on what device may produce nondeterministic gradients?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "CUDA"
    },
    {
        "X": "What is the name of the operation that may produce nondeterministic gradients when given tensors on a CUDA device?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "Reproducibility"
    },
    {
        "X": "What is input (Tensor)?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. ",
        "Y": "input tensor"
    },
    {
        "X": "What should the input tensor be of?",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "same size"
    },
    {
        "X": "What is optional, minimum number of bins?",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "minlength"
    },
    {
        "X": "What should the minimum number of bins be?",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "non-negative"
    },
    {
        "X": "What happens to a tensor of shape Size([max(input) + 1])?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "if input is non-empty"
    },
    {
        "X": "Sets what state?",
        "Z": "Sets the random number generator state. new_state (torch.ByteTensor) \u2013 The desired state ",
        "Y": "random number generator"
    },
    {
        "X": "What is the desired state of the random number generator?",
        "Z": "Sets the random number generator state. new_state (torch.ByteTensor) \u2013 The desired state ",
        "Y": "new_state"
    },
    {
        "X": "What does torch.conj() do?",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Computes the element-wise conjugate"
    },
    {
        "X": "What has a non-complex dtype?",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "input"
    },
    {
        "X": "What may return a non-writeable view for an input of non-complex dtype?",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "torch.conj()"
    },
    {
        "X": "What function returns a non-writeable view for an input of non-complex dtype?",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "torch.conj()"
    },
    {
        "X": "What is the name of the alias?",
        "Z": "Alias of torch.vstack(). ",
        "Y": "torch.vstack()"
    },
    {
        "X": "Fills self tensor with elements samples from the normal distribution parameterized by what?",
        "Z": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std. ",
        "Y": "mean and std"
    },
    {
        "X": "What fills with elements from the normal distribution?",
        "Z": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std. ",
        "Y": "self tensor"
    },
    {
        "X": "Fills what with elements samples from the normal distribution parameterized by mean and std?",
        "Z": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std. ",
        "Y": "self tensor"
    },
    {
        "X": "What does torch.as_tensor() copy data from?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "NumPy ndarray"
    },
    {
        "X": "What does torch.tensor() always copy data?",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "a Tensor data"
    },
    {
        "X": "What type of data does torch.Tensor.requires_grad_() or torch.Tensor.detach",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "Tensor"
    },
    {
        "X": "What does torch.as_tensor() use if you want to avoid a copy of data?",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "NumPy ndarray"
    },
    {
        "X": "What does torch.tensor always copy data?",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "Warning"
    },
    {
        "X": "What does torch.tensor do?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "always copies data"
    },
    {
        "X": "What is a warning about a copy of data by a Tensor?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "Warning"
    },
    {
        "X": "What do you use if you want to avoid a copy of a Tensor data?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "torch.Tensor.requires_grad_() or torch.Tensor.detach()"
    },
    {
        "X": "What ndarray does torch.as_tensor() avoid a copy of?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "NumPy"
    },
    {
        "X": "What does torch.as_tensor() do?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "Y": "Warning"
    },
    {
        "X": "What variable does torch.tensor() construct when data is a tensor x?",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "leaf"
    },
    {
        "X": "What is equivalent to x.clone().detach()?",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "torch.tensor(x)"
    },
    {
        "X": "What are the equivalents of torch.tensor?",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "clone() and detach()"
    },
    {
        "X": "What is the initial data for the tensor?",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "array_like"
    },
    {
        "X": "What can data (array_like) \u2013 Initial data for the tensor be?",
        "Z": "Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types"
    },
    {
        "X": "What can initial data for the tensor be?",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types"
    },
    {
        "X": "What is the desired data type of returned tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "dtype"
    },
    {
        "X": "What infers data type from data?",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "if None"
    },
    {
        "X": "What default uses the current device for the default tensor type?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "if None"
    },
    {
        "X": "What will device be for CPU tensor types?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "CPU"
    },
    {
        "X": "Default: if what, infers data type from data. device (torch.device, optional) \u2013 the desired device",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "None"
    },
    {
        "X": "Default: if None, uses what for the default tensor type?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "current device"
    },
    {
        "X": "What is the device for CPU tensor types?",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "CPU"
    },
    {
        "X": "What is the default tensor type?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "current device"
    },
    {
        "X": "If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors.",
        "Z": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example: ",
        "Y": "pin_memory"
    },
    {
        "X": "What does pin_memory only work for?",
        "Z": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example: ",
        "Y": "CPU tensors"
    },
    {
        "X": "What is the default setting for pin_memory?",
        "Z": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example: ",
        "Y": "False"
    },
    {
        "X": "What is an example of a return tensor?",
        "Z": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example: ",
        "Y": "Example:"
    },
    {
        "X": "What does the second row of a 2-by-N Tensor contain?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "Y": "column coordinates"
    },
    {
        "X": "Indices are ordered based on what?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "rows and then columns"
    },
    {
        "X": "What is the upper triangular part of a matrix defined as?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "Y": "the elements on and above the diagonal"
    },
    {
        "X": "Returns what of the upper triangular part of a row by col matrix in a 2-by-N Tensor?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "the indices"
    },
    {
        "X": "What is the upper triangular part of the matrix defined as?",
        "Z": "The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "Y": "the elements on and above the diagonal"
    },
    {
        "X": "When running on what platform, row * col must be less than 259259259 to prevent overflow during calculation?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "CUDA"
    },
    {
        "X": "What is the number of rows in the 2-D matrix?",
        "Z": "row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "row"
    },
    {
        "X": "What is the name of the number of columns in the 2-D matrix?",
        "Z": "row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "col"
    },
    {
        "X": "What is the diagonal offset from the main diagonal?",
        "Z": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "offset"
    },
    {
        "X": "If not provided, what is the default?",
        "Z": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "Default"
    },
    {
        "X": "What is the default data type of returned tensor?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "Y": "Default"
    },
    {
        "X": "When running on what platform must row * col be less than 259259259 to prevent overflow during calculation?",
        "Z": "When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "CUDA"
    },
    {
        "X": "What is the default value for the offset from the main diagonal?",
        "Z": "row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "Default"
    },
    {
        "X": "What is the name of the diagonal offset from the main diagonal?",
        "Z": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "offset"
    },
    {
        "X": "What is the default value of torch.long?",
        "Z": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "if None"
    },
    {
        "X": "What is the number of columns in the 2-D matrix?",
        "Z": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "col"
    },
    {
        "X": "What is offset from the main diagonal?",
        "Z": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "diagonal offset"
    },
    {
        "X": "What is the default value for the offset?",
        "Z": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "Y": "if not provided"
    },
    {
        "X": "What is the default value for the data type of returned tensor?",
        "Z": "Constructs a tensor with data. Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "Y": "Default"
    },
    {
        "X": "What is the device used for CPU tensor types?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "CPU"
    },
    {
        "X": "What is the default value of offset?",
        "Z": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "Default"
    },
    {
        "X": "What is the current CUDA device for CUDA tensor types?",
        "Z": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "CPU"
    },
    {
        "X": "Computes what?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "complementary error function of input"
    },
    {
        "X": "The dividend and divisor may contain both for what?",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. ",
        "Y": "integer and floating point numbers"
    },
    {
        "X": "The remainder of division has the same what as the divisor other?",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. ",
        "Y": "sign"
    },
    {
        "X": "What is supported to a common shape, type promotion, and integer and float inputs?",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. ",
        "Y": "broadcasting"
    },
    {
        "X": "What inputs are not supported?",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also ",
        "Y": "Complex inputs"
    },
    {
        "X": "In some cases, it is what to satisfy the definition of a modulo operation with complex numbers?",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "Y": "not mathematically possible"
    },
    {
        "X": "What is the name of the function that handles division by zero?",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "Y": "torch.fmod()"
    },
    {
        "X": "What may contain both integer and floating point numbers?",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "Y": "dividend and divisor"
    },
    {
        "X": "What has the same sign as the divisor other?",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "Y": "The remainder"
    },
    {
        "X": "Supports what type of programming?",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "Y": "broadcasting"
    },
    {
        "X": "What is the input for the dividend?",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "Y": "Tensor"
    },
    {
        "X": "Supports what to a common shape, type promotion, and integer and float inputs?",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "broadcasting"
    },
    {
        "X": "In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with what?",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also ",
        "Y": "complex numbers"
    },
    {
        "X": "What is the input (Tensor) \u2013 the divisor out (Tensor, optional) \u2013 the output tens",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also ",
        "Y": "the dividend"
    },
    {
        "X": "What is not supported?",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "Y": "Complex inputs"
    },
    {
        "X": "Why are complex inputs not supported?",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "Y": "it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers"
    },
    {
        "X": "What is the input (Tensor)?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "input tensor"
    },
    {
        "X": "What computes the element-wise remainder of division equivalently to the C library function fmod()?",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "Y": "torch.fmod()"
    },
    {
        "X": "What is the input (Tensor) referred to as?",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "Y": "the dividend"
    },
    {
        "X": "Sets the seed for generating random numbers to what?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "a random number on all GPUs"
    },
    {
        "X": "What is the bit number used to seed the RNG?",
        "Z": "Sets the seed for generating random numbers to a non-deterministic\nrandom number. Returns a 64 bit number used to seed the RNG. ",
        "Y": "64"
    },
    {
        "X": "What is the number of threads used for on a CPU?",
        "Z": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "Y": "intraop parallelism"
    },
    {
        "X": "What must be called before running eager, JIT or autograd code?",
        "Z": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "Y": "set_num_threads"
    },
    {
        "X": "What does set set for intraop parallelism on CPU?",
        "Z": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "Y": "the number of threads"
    },
    {
        "X": "What type of tensor does torch return?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "2-D"
    },
    {
        "X": "What is the number of rows in the output tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "n"
    },
    {
        "X": "What is the default value of torch.set_default_tensor_type()?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "if None"
    },
    {
        "X": "What is the desired layout of returned Tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "layout"
    },
    {
        "X": "What is the default setting of the returned Tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "torch.strided"
    },
    {
        "X": "What is returned with ones on the diagonal and zeros elsewhere?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "a 2-D tensor"
    },
    {
        "X": "What is the default number of columns in the output tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "n out"
    },
    {
        "X": "Default: if None, uses a what default?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "global"
    },
    {
        "X": "What is the default setting for a 2-D tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "torch.strided"
    },
    {
        "X": "What does n out represent?",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "the output tensor"
    },
    {
        "X": "What uses a global default?",
        "Z": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "if None"
    },
    {
        "X": "What is the default setting for the returned Tensor?",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "torch.strided"
    },
    {
        "X": "n (int) \u2013 the nu of rows m (int, optional) \u2013 the number of columns with default being n",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "m"
    },
    {
        "X": "If None, uses a global default (see torch.set_default_tensor_type()).",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "Default"
    },
    {
        "X": "What is the default setting of returned Tensor?",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "torch.strided"
    },
    {
        "X": "What is the name of the output tensor?",
        "Z": "m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "output tensor"
    },
    {
        "X": "What is the default layout of the returned Tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "torch.strided"
    },
    {
        "X": "What is the default layout of returned Tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "torch.strided"
    },
    {
        "X": "Out (Tensor, optional) - what is the name of the output tensor?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "output tensor"
    },
    {
        "X": "What is the default setting for the output tensor?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "Y": "torch.strided"
    },
    {
        "X": "What is the default setting for the current device for the default tensor type?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "if None"
    },
    {
        "X": "What type of tensor has ones on the diagonal and zeros elsewhere?",
        "Z": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "2-D"
    },
    {
        "X": "What is the default setting for the default tensor type?",
        "Z": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example: ",
        "Y": "if None"
    },
    {
        "X": "What is a tensor with ones on the diagonal and zeros elsewhere?",
        "Z": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "2-D tensor"
    },
    {
        "X": "Returns what with the truncated integer values of the elements of input?",
        "Z": "Returns a new tensor with the truncated integer values of\nthe elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "a new tensor"
    },
    {
        "X": "What is an example of a new tensor?",
        "Z": "Returns a new tensor with the truncated integer values of\nthe elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Example"
    },
    {
        "X": "Returns what with the logarithm to the base 2 of the elements of input?",
        "Z": "Returns a new tensor with the logarithm to the base 2 of the elements\nof input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "a new tensor"
    },
    {
        "X": "What is an example of an output tensor?",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Example:"
    },
    {
        "X": "What is the name of the Alias for?",
        "Z": "Alias for torch.special.expm1(). ",
        "Y": "torch.special.expm1"
    },
    {
        "X": "What is the window_length?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "Y": "window length"
    },
    {
        "X": "Computes the Kaiser window with window length window_length and what other parameter?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: ",
        "Y": "shape parameter beta"
    },
    {
        "X": "What does torch.i0() do?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "Y": "Let I_0 be the zeroth order modified Bessel function of the first kind"
    },
    {
        "X": "What does this function do?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device"
    },
    {
        "X": "Computes the Kaiser window with what parameter?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: ",
        "Y": "window length"
    },
    {
        "X": "What is I_0?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: ",
        "Y": "the zeroth order modified Bessel function of the first kind"
    },
    {
        "X": "What is the zeroth order modified Bessel function of the first kind?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "L"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "Calling torch.kaiser_window"
    },
    {
        "X": "What is the periodic argument intended for?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "a helpful shorthand"
    },
    {
        "X": "What is the periodic argument intended to produce a periodic window as input to functions like torch.stft()?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note ",
        "Y": "Note"
    },
    {
        "X": "What is the name of the zeroth order modified Bessel function of the first kind?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note ",
        "Y": "L"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]?",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. ",
        "Y": "torch.kaiser_window(L, B, periodic=True)"
    },
    {
        "X": "What is the periodic argument intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note ",
        "Y": "Note"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)?",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. ",
        "Y": "Calling torch.kaiser_window"
    },
    {
        "X": "What is the returned window if window_length is one?",
        "Z": "Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "a single element tensor containing a one"
    },
    {
        "X": "What is the length of the window?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "window_length (int)"
    },
    {
        "X": "The periodic argument is intended as a useful shorthand to produce a periodic window as input to functions like what?",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "torch.stft()"
    },
    {
        "X": "If window_length is what, then the returned window is a single element tensor containing a one?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "one"
    },
    {
        "X": "What is window_length?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "length of the window"
    },
    {
        "X": "What is the window returned if window_length is one?",
        "Z": "Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "Y": "a single element tensor"
    },
    {
        "X": "What does window_length (int) mean?",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "length of the window"
    },
    {
        "X": "What is the window suitable for use in spectral analysis?",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "Y": "periodic"
    },
    {
        "X": "If False, returns a symmetric window suitable for use in what?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "filter design"
    },
    {
        "X": "What does beta (float, optional) represent for the window?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "shape parameter"
    },
    {
        "X": "If False, returns a window suitable for use in filter design.",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "symmetric"
    },
    {
        "X": "What is the shape parameter for the window?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "beta"
    },
    {
        "X": "If window_length is one, what is the returned window?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "a single element tensor containing a one"
    },
    {
        "X": "If True, returns a periodic window suitable for use in spectral analysis. If False, returns a symmetric window suitable for use",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "periodic"
    },
    {
        "X": "What is the name of the window that is suitable for use in spectral analysis?",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "periodic"
    },
    {
        "X": "What returns a symmetric window suitable for use in filter design?",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "If False"
    },
    {
        "X": "What does torch.set_default_tensor_type() use?",
        "Z": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "global default"
    },
    {
        "X": "What is the periodic window suitable for?",
        "Z": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "spectral analysis"
    },
    {
        "X": "What does beta (float, optional) provide for the window?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "shape parameter"
    },
    {
        "X": "What is dtype?",
        "Z": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "the desired data type of returned tensor"
    },
    {
        "X": "What is the desired layout of returned window tensor?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "layout"
    },
    {
        "X": "What is the only option that supports dense layout?",
        "Z": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "torch.strided"
    },
    {
        "X": "What parameter does beta (float, optional) provide for the window?",
        "Z": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "shape parameter"
    },
    {
        "X": "Default: if None, uses a what?",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "global default"
    },
    {
        "X": "What is the term for dense layout?",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "torch.strided"
    },
    {
        "X": "What is the default type of returned tensor?",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "Default"
    },
    {
        "X": "What is the name for a dense layout?",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "Y": "torch.strided"
    },
    {
        "X": "What is the name of the desired layout of returned window tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "torch.layout"
    },
    {
        "X": "What is the default layout of a window tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "torch.strided"
    },
    {
        "X": "What uses the current device for the default tensor type?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "if None"
    },
    {
        "X": "What is the layout of returned window tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "desired layout"
    },
    {
        "X": "What is the default layout of returned window tensor?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "torch.strided"
    },
    {
        "X": "Device will be what for CPU tensor types and the current CUDA device for CUDA tensor types?",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "CPU"
    },
    {
        "X": "What is a vector or matrix norm?",
        "Z": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "Common linear algebra operations"
    },
    {
        "X": "What is the determinant of a square matrix?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "matrix norm"
    },
    {
        "X": "What is a common linear algebra operation?",
        "Z": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "vector norm"
    },
    {
        "X": "Computes the sign and natural logarithm of the absolute value of what?",
        "Z": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   ",
        "Y": "determinant of a square matrix"
    },
    {
        "X": "What is the absolute value of the determinant of a square matrix?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "the sign and natural logarithm"
    },
    {
        "X": "What does it compute?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "the condition number of a matrix with respect to a matrix norm"
    },
    {
        "X": "What does the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix compute?",
        "Z": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "numerical rank"
    },
    {
        "X": "Who decomposes a complex Hermitian or real symmetric positive-definite matrix?",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "Cholesky"
    },
    {
        "X": "What operations are used to compute a vector or matrix norm?",
        "Z": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "Common linear algebra operations"
    },
    {
        "X": "Computes what of a matrix with respect to a matrix norm?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "condition number"
    },
    {
        "X": "Computes what of a matrix?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "singular values"
    },
    {
        "X": "What decomposition of a complex Hermitian or real symmetric positive-definite matrix is computed?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "Cholesky"
    },
    {
        "X": "What type of norm is computed?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "a vector or matrix norm"
    },
    {
        "X": "What is a matrix norm?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "vector norm"
    },
    {
        "X": "What does Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "determinant of a square matrix"
    },
    {
        "X": "What does the computation of the numerical rank of a matrix consist of?",
        "Z": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "Y": "the condition number of a matrix with respect to a matrix norm"
    },
    {
        "X": "What decomposition of a matrix is computed?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "QR"
    },
    {
        "X": "Computes the absolute value of what of a square matrix?",
        "Z": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "Y": "determinant"
    },
    {
        "X": "What does Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "Z": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "Y": "the QR decomposition of a matrix"
    },
    {
        "X": "Computes a vector norm.",
        "Z": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "Y": "matrix norm"
    },
    {
        "X": "Computes the absolute value of what?",
        "Z": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "Y": "determinant of a square matrix"
    },
    {
        "X": "Computes the condition number of a matrix with respect to what?",
        "Z": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "Y": "matrix norm"
    },
    {
        "X": "What is the decomposition of a square matrix if it exists?",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   ",
        "Y": "eigenvalue"
    },
    {
        "X": "Computes the sign and natural logarithm of the absolute value of what of a square matrix?",
        "Z": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "Y": "determinant"
    },
    {
        "X": "What does the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix if it exists?",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   ",
        "Y": "eigenvalues"
    },
    {
        "X": "Computes what of the absolute value of the determinant of a square matrix?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "the sign and natural logarithm"
    },
    {
        "X": "What decomposition of a square matrix if it exists?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "eigenvalue"
    },
    {
        "X": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes what of a square matrix?",
        "Z": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "Y": "eigenvalues"
    },
    {
        "X": "What does the Cholesky decomposition of a square matrix if it exists?",
        "Z": "Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "Y": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "What does the Cholesky decomposition of a complex Hermitian or real symmetric matrix compute?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "What is the eigenvalue decomposition of a square matrix?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "if it exists"
    },
    {
        "X": "Computes the eigenvalue decomposition of a square matrix if it exists.",
        "Z": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "Y": "Computes the eigenvalues of a square matrix"
    },
    {
        "X": "Computes the decomposition of a complex Hermitian or real symmetric matrix?",
        "Z": "Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   ",
        "Y": "eigenvalue"
    },
    {
        "X": "What does the Cholesky decomposition of a complex Hermitian or real symmetric matrix compute if it exists?",
        "Z": "Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   ",
        "Y": "eigenvalue decomposition of a square matrix"
    },
    {
        "X": "Computes what of a square matrix?",
        "Z": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "Y": "eigenvalues"
    },
    {
        "X": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.",
        "Z": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "Y": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "Computes what of a complex Hermitian or real symmetric matrix?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "eigenvalues"
    },
    {
        "X": "What does SVD stand for?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "singular value decomposition"
    },
    {
        "X": "Computes the decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "Cholesky"
    },
    {
        "X": "What does it compute if it exists?",
        "Z": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "Y": "eigenvalue decomposition of a square matrix"
    },
    {
        "X": "What does the eigenvalue decomposition of a complex Hermitian or real symmetric matrix compute?",
        "Z": "  Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "eigenvalues of a square matrix"
    },
    {
        "X": "What is the eigenvalue decomposition of a complex Hermitian or real symmetric matrix?",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "Y": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "What does the SVD of a matrix compute?",
        "Z": "Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   ",
        "Y": "eigenvalues of a square matrix"
    },
    {
        "X": "Computes the eigenvalue decomposition of a square matrix?",
        "Z": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "Y": "if it exists"
    },
    {
        "X": "What is the solution of?",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "a square system of linear equations with a unique solution"
    },
    {
        "X": "What does Computes the eigenvalue decomposition of a square matrix if it exists?",
        "Z": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "Y": "Computes the eigenvalues of a square matrix"
    },
    {
        "X": "Computes the singular value decomposition (SVD) of a matrix.",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "Y": "singular values of a matrix"
    },
    {
        "X": "Computes the solution of a square system of linear equations with what?",
        "Z": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "Y": "unique solution"
    },
    {
        "X": "What does the SVD of a matrix do?",
        "Z": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "Y": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "What solves the least squares problem of a system of linear equations?",
        "Z": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "a square system of linear equations with a unique solution"
    },
    {
        "X": "Computes a solution to the least squares problem of what?",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "a system of linear equations"
    },
    {
        "X": "Computes a solution to what problem of a system of linear equations?",
        "Z": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "Y": "the least squares problem"
    },
    {
        "X": "What is the name of a square matrix if it exists?",
        "Z": "Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   ",
        "Y": "inverse"
    },
    {
        "X": "Computes what of a square matrix if it exists?",
        "Z": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "Y": "inverse"
    },
    {
        "X": "What does Moore-Penrose inverse stand for?",
        "Z": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "Y": "pseudoinverse"
    },
    {
        "X": "What is the pseudoinverse of a matrix?",
        "Z": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "Y": "Moore-Penrose"
    },
    {
        "X": "Computes a what?",
        "Z": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "Y": "matrix norm"
    },
    {
        "X": "Computes the solution of what?",
        "Z": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "Y": "a square system of linear equations with a unique solution"
    },
    {
        "X": "Computes the what of a square matrix if it exists?",
        "Z": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "Y": "inverse"
    },
    {
        "X": "Computes what of a square matrix for an integer n?",
        "Z": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "Y": "n-th power"
    },
    {
        "X": "What is the first column of a product of Householder matrices?",
        "Z": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "Y": "first n columns"
    },
    {
        "X": "What is the power of a square matrix for an integer n?",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "Y": "n-th power"
    },
    {
        "X": "Efficiently multiplies two or more matrices by what?",
        "Z": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "Y": "reordering"
    },
    {
        "X": "Computes what of a product of Householder matrices?",
        "Z": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "Y": "first n columns"
    },
    {
        "X": "What does torch.tensordot() compute?",
        "Z": "  Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B. ",
        "Y": "multiplicative inverse"
    },
    {
        "X": "Computes the solution X to the system torch.tensordot(A, X) = what?",
        "Z": "  Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B. ",
        "Y": "B"
    },
    {
        "X": "What is the name of the decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "Cholesky"
    },
    {
        "X": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix if it is in",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "inverse"
    },
    {
        "X": "Computes what decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "Cholesky"
    },
    {
        "X": "Computes what of a square matrix if it is invertible?",
        "Z": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "the inverse"
    },
    {
        "X": "What format is used to construct a sparse tensor?",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given\nindices. Note This function returns an uncoalesced tensor. ",
        "Y": "COO(rdinate)"
    },
    {
        "X": "What type of tensor does this function return?",
        "Z": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "uncoalesced tensor"
    },
    {
        "X": "Constructs a sparse tensor in what format?",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given\nindices. Note This function returns an uncoalesced tensor. ",
        "Y": "COO(rdinate)"
    },
    {
        "X": "What does this function return?",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "returns an uncoalesced tensor"
    },
    {
        "X": "What types of indices can be used for the tensor?",
        "Z": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types"
    },
    {
        "X": "What is cast to the LongTensor internally?",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "torch"
    },
    {
        "X": "What is the first dimension of the indices?",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "Y": "the number of tensor dimensions"
    },
    {
        "X": "What can indices be?",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types"
    },
    {
        "X": "What is the name of the internal tensor?",
        "Z": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "LongTensor"
    },
    {
        "X": "What are the indices?",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "the coordinates of the non-zero values in the matrix"
    },
    {
        "X": "Can be a list, NumPy ndarray, scalar, and other types?",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "tuple"
    },
    {
        "X": "What is the name of the tensor internally?",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "Y": "LongTensor"
    },
    {
        "X": "What are some other types of indices?",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "Y": "tuple, NumPy ndarray, scalar"
    },
    {
        "X": "What will be cast to the LongTensor internally?",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "Y": "torch"
    },
    {
        "X": "What is the initial value for?",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "Y": "tensor"
    },
    {
        "X": "What are some other types of initial values for the tensor?",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "Y": "tuple, NumPy ndarray, scalar"
    },
    {
        "X": "What is optional for a sparse tensor?",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "Y": "size"
    },
    {
        "X": "What is the size of the sparse tensor inferred as if not provided?",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "Y": "the minimum size big enough to hold all non-zero elements"
    },
    {
        "X": "What default infers data type from values?",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "Y": "if None"
    },
    {
        "X": "What is the size of the sparse tensor?",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "Y": "size"
    },
    {
        "X": "What is the size of the sparse tensor if not provided?",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "Y": "minimum size big enough to hold all non-zero elements"
    },
    {
        "X": "Default: if what, infers data type from values?",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "Y": "None"
    },
    {
        "X": "What infers data type from values?",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "if None"
    },
    {
        "X": "Default: if what, infers data type from values. device (torch.device, optional) \u2013 the desired device",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "None"
    },
    {
        "X": "What does compute the element-wise logical OR of the given input tensors do?",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Computes the element-wise logical OR"
    },
    {
        "X": "What is treated as False?",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Zeros"
    },
    {
        "X": "What is the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor?",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "other"
    },
    {
        "X": "What is an example of a tensor to compute OR with out?",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Example"
    },
    {
        "X": "What contains L and U factors for LU factorization of A?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "LU"
    },
    {
        "X": "What type of inputs can torch.solve(B, A) take?",
        "Z": "LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "2D"
    },
    {
        "X": "What does torch.solve(B, A) return if the inputs are batches?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "batched outputs solution"
    },
    {
        "X": "What types of inputs does torch.solve(B, A) support?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "real-valued and complex-valued inputs"
    },
    {
        "X": "What does torch.solve(B, A) do?",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What is the LU factorization of A in order?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "namedtuple solution"
    },
    {
        "X": "What does LU contain for LU factorization of A?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "L and U factors"
    },
    {
        "X": "What does torch.solve(B, A) do when it takes inputs that are batches of 2D matrices?",
        "Z": "LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What type of inputs can torch.solve(B, A) take in?",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "2D"
    },
    {
        "X": "What does torch.solve return if the inputs are batches?",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "batched outputs solution"
    },
    {
        "X": "What types of inputs does torch.solve support?",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "real-valued and complex-valued"
    },
    {
        "X": "What can torch.solve(B, A) take in?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "2D inputs B, A or inputs that are batches of 2D matrices"
    },
    {
        "X": "What does torch.solve(B, A) provide?",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What types of inputs does torch.solve() support?",
        "Z": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "real-valued and complex-valued inputs"
    },
    {
        "X": "What is deprecated in favor of torch.linalg.solve()?",
        "Z": "Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "Warning torch.solve()"
    },
    {
        "X": "What does torch.linalg.solve() not return?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "LU factorization"
    },
    {
        "X": "What can be used with torch.lu_solve() and torch.lu_unpack() to get the LU factorization?",
        "Z": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.lu()"
    },
    {
        "X": "What should torch.solve(B, A) be replaced with?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "Note"
    },
    {
        "X": "What types of inputs does PyTorch support?",
        "Z": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "real-valued and complex-valued inputs"
    },
    {
        "X": "What is the replacement for torch.solve()?",
        "Z": "Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.linalg.solve()"
    },
    {
        "X": "What does torch.linalg.solve() do?",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "does not return the LU factorization of the input"
    },
    {
        "X": "What is used to get the LU factorization of the input?",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.lu()"
    },
    {
        "X": "What should X = torch.solve(B, A)solution be replaced with?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "Note"
    },
    {
        "X": "What is used to get the LU factorization?",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.lu()"
    },
    {
        "X": "What is torch.solve() replaced with?",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.linalg.solve()"
    },
    {
        "X": "What is torch.solve() replaced by?",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.linalg.solve()"
    },
    {
        "X": "What is the name of the strides used to transpose the returned matrices?",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "Y": "B.contiguous().transpose(-1, -2)"
    },
    {
        "X": "What is *?",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "Y": "zero or more batch dimensions"
    },
    {
        "X": "What is the input square matrix of size (,m,m)(*, m, m)(,m,m)",
        "Z": "X = torch.solve(B, A).solution should be replaced with Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. ",
        "Y": "A"
    },
    {
        "X": "What will be transposed regardless of the original strides?",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "Y": "the returned matrices solution and LU"
    },
    {
        "X": "What is the name of the input square matrix of size (,m,m)(*, m, m)(,m",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "Y": "A"
    },
    {
        "X": "Out ((Tensor, Tensor)) \u2013 optional output tuple.",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "Y": "optional"
    },
    {
        "X": "What do CUDA tensor types use for computation?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "Y": "GPUs"
    },
    {
        "X": "What is used to determine if your system supports CUDA?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "is_available()"
    },
    {
        "X": "What has more details about working with CUDA?",
        "Z": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "Y": "CUDA semantics"
    },
    {
        "X": "What selects a given stream?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "Context-manager StreamContext"
    },
    {
        "X": "What does the Context-manager do?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Checks if peer access between two devices is possible"
    },
    {
        "X": "What is the pointer to current cuBLAS handle?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   ",
        "Y": "cublasHandle_t"
    },
    {
        "X": "What is checked if the Context-manager selects a given stream?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   ",
        "Y": "peer access between two devices"
    },
    {
        "X": "What does cublasHandle_t pointer to?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "current cuBLAS handle"
    },
    {
        "X": "What does cublasHandle_t pointer return to current cuBLAS handle?",
        "Z": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   ",
        "Y": "cublasHandle_t pointer"
    },
    {
        "X": "What does cublasHandle_t return for a given device?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "currently selected Stream"
    },
    {
        "X": "What does this return for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "a dictionary of CUDA memory allocator statistics"
    },
    {
        "X": "Checks if what is possible?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "Y": "peer access between two devices is possible"
    },
    {
        "X": "What is returned for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "maximum GPU memory occupied by tensors in bytes"
    },
    {
        "X": "What changes the selected device?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "Context-manager"
    },
    {
        "X": "Returns the number of what?",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "Y": "GPUs"
    },
    {
        "X": "Returns cublasHandle_t pointer to what?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "current cuBLAS handle"
    },
    {
        "X": "What does the Context-manager return?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "number of GPUs available"
    },
    {
        "X": "What is returned to current cuBLAS handle?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "cublasHandle_t pointer"
    },
    {
        "X": "What changes the current device to that of given object?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Context-manager"
    },
    {
        "X": "Who changes the selected device?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   ",
        "Y": "Context-manager"
    },
    {
        "X": "What does the Context-manager that changes the selected device return?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "Y": "number of GPUs available"
    },
    {
        "X": "What does the cublasHandle_t pointer return for a given device?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "Y": "the currently selected Stream"
    },
    {
        "X": "What does this library return a list of?",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "Y": "CUDA architectures"
    },
    {
        "X": "Returns list of what library was compiled for?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "CUDA architectures"
    },
    {
        "X": "What does this library return?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "What is returned when a library is compiled?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "Y": "Gets the cuda capability of a device"
    },
    {
        "X": "What is the name of a currently selected device?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete. ",
        "Y": "index"
    },
    {
        "X": "What does the Context-manager return for a given device?",
        "Z": "Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   ",
        "Y": "default Stream"
    },
    {
        "X": "What is the name of a device?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Gets the name of a device"
    },
    {
        "X": "What does the cuda capability of a device return?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "Y": "the name of a device"
    },
    {
        "X": "What do you get from a device?",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "Y": "properties"
    },
    {
        "X": "Returns the index of what?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete. ",
        "Y": "currently selected device"
    },
    {
        "X": "What does Context-manager return?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "number of GPUs available"
    },
    {
        "X": "Gets what of a device?",
        "Z": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "properties"
    },
    {
        "X": "What does this library get?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "Y": "the cuda capability of a device"
    },
    {
        "X": "What does the library get from a device?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "Y": "properties"
    },
    {
        "X": "What was this library compiled with?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "What does PyTorch return for a given device?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   ",
        "Y": "default Stream"
    },
    {
        "X": "What does PyTorch get from a device?",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "Y": "properties"
    },
    {
        "X": "What does PyTorch get?",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "Y": "the name of a device"
    },
    {
        "X": "What does PyTorch's CUDA state return?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "What library initializes its CUDA state?",
        "Z": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "Y": "PyTorch"
    },
    {
        "X": "What is the list this library was compiled for?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "Y": "CUDA architectures"
    },
    {
        "X": "What is the name of a device. Gets the name of a device. Gets the properties of a device.",
        "Z": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "Y": "cuda capability"
    },
    {
        "X": "What does PyTorch do?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "Y": "Gets the properties of a device"
    },
    {
        "X": "What does PyTorch return?",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "What collects GPU memory after it has been released by CUDA IPC?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Force"
    },
    {
        "X": "Returns what flags this library was compiled with?",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "What does Force collect GPU memory after it has been released by CUDA IPC?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "Initialize PyTorch\u2019s CUDA state"
    },
    {
        "X": "What library initializes CUDA state?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "Y": "PyTorch"
    },
    {
        "X": "What indicates if CUDA is currently available?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "bool"
    },
    {
        "X": "Returns what?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "current random seed of the current GPU"
    },
    {
        "X": "Returns what indicating if CUDA is currently available?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "bool"
    },
    {
        "X": "What did PyTorch's CUDA state return?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "What does Gets the cuda capability of a device?",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather ",
        "Y": "Gets the name of a device"
    },
    {
        "X": "What does this library list?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "Y": "CUDA architectures"
    },
    {
        "X": "What does the Force collect GPU memory after it has been released by CUDA IPC?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "Y": "Initialize PyTorch\u2019s CUDA state"
    },
    {
        "X": "What does this return?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "maximum GPU memory managed by the caching allocator in bytes for a given device"
    },
    {
        "X": "Returns list of what architectures this library was compiled for?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "Y": "CUDA architectures"
    },
    {
        "X": "Sets what if PyTorch\u2019s CUDA state has been initialized?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "Y": "current device"
    },
    {
        "X": "What does PyTorch do with its CUDA state?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "Y": "Initialize"
    },
    {
        "X": "What is a wrapper API to set the stream?",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "Y": "Sets the current device"
    },
    {
        "X": "What type of API is used to set the stream?",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "Y": "wrapper API"
    },
    {
        "X": "What does a device get?",
        "Z": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   ",
        "Y": "cuda capability"
    },
    {
        "X": "What does the bool indicating if PyTorch\u2019s CUDA state has been initialized return?",
        "Z": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "Sets what?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "random number generator state of all devices"
    },
    {
        "X": "What is a wrapper API to set the current stream?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Sets the current device"
    },
    {
        "X": "What is this to set the stream?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "wrapper API"
    },
    {
        "X": "What is the name of the API that selects a given stream?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "Y": "Wrapper"
    },
    {
        "X": "This is a wrapper API to what?",
        "Z": "Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "Y": "set the stream"
    },
    {
        "X": "Wrapper around what that selects a given stream?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "Context-manager StreamContext"
    },
    {
        "X": "Returns the random number generator state of the specified GPU as a what?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "ByteTensor"
    },
    {
        "X": "Returns what representation of the random number states of all devices?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "a list of ByteTensor"
    },
    {
        "X": "What does the ByteTensor do?",
        "Z": "Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "Sets the random number generator state of what?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "all devices"
    },
    {
        "X": "Sets the seed for generating random numbers for what?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "current GPU"
    },
    {
        "X": "Sets the seed for generating random numbers on what?",
        "Z": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "all GPUs"
    },
    {
        "X": "Sets the seed for generating random numbers to a random number for what?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "current GPU"
    },
    {
        "X": "What represents the random number states of all devices?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "ByteTensor"
    },
    {
        "X": "What does the ByteTensor set?",
        "Z": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "Y": "the random number generator state of all devices"
    },
    {
        "X": "What is the seed for generating random numbers for?",
        "Z": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "Y": "current GPU"
    },
    {
        "X": "The seed for generating random numbers is set on what?",
        "Z": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "Y": "all GPUs"
    },
    {
        "X": "What is the seed for generating random numbers set to?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "a random number on all GPUs"
    },
    {
        "X": "The seed for generating random numbers is set to a random number on what?",
        "Z": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "Y": "all GPUs"
    },
    {
        "X": "Returns a list of what representing the random number states of all devices?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "ByteTensor"
    },
    {
        "X": "Sets the seed for what on all GPUs?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "generating random numbers"
    },
    {
        "X": "Sets the seed for generating random numbers to what on all GPUs?",
        "Z": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "Y": "a random number"
    },
    {
        "X": "Sets what state of the specified GPU?",
        "Z": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "random number generator state"
    },
    {
        "X": "Sets the seed for generating random numbers to a random number on what?",
        "Z": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "all GPUs"
    },
    {
        "X": "What does Sets the seed for generating random numbers return?",
        "Z": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "current random seed of the current GPU"
    },
    {
        "X": "What sends a tensor to specified GPU devices?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.broadcast Broadcasts"
    },
    {
        "X": "What is a sequence of tensors to the specified GPUs?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.broadcast_coalesced Broadcasts"
    },
    {
        "X": "What does comm.reduce_add tensors from multiple GPUs do?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "Sums"
    },
    {
        "X": "What Scatters tensor across multiple GPUs?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.scatter"
    },
    {
        "X": "What gathers tensors from multiple GPU devices?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.gather"
    },
    {
        "X": "What Broadcasts a tensor to specified GPU devices?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.broadcast"
    },
    {
        "X": "What Broadcasts a sequence tensors to the specified GPUs?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.broadcast_coalesced"
    },
    {
        "X": "What Sums tensors from multiple GPUs?",
        "Z": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "Y": "comm.reduce_add"
    },
    {
        "X": "Wrapper around what?",
        "Z": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "Y": "a CUDA event"
    },
    {
        "X": "What is Wrapper around a CUDA event?",
        "Z": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "Y": "Wrapper around a CUDA stream"
    },
    {
        "X": "What is Wrapper around a CUDA stream?",
        "Z": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "Y": "Wrapper around a CUDA event"
    },
    {
        "X": "Releases all what currently held by the caching allocator?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "unoccupied cached memory"
    },
    {
        "X": "What type of memory allocator statistics does nvidia-smi return?",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "CUDA"
    },
    {
        "X": "What does it do to the unoccupied cached memory currently held by the caching allocator?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Releases all unoccupied cached memory currently held by the caching allocator"
    },
    {
        "X": "What is returned of the running processes and their GPU memory use for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "a human-readable printout"
    },
    {
        "X": "Returns a dictionary of what for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "CUDA memory allocator statistics"
    },
    {
        "X": "Returns a human-readable printout of what for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "current memory allocator statistics"
    },
    {
        "X": "Returns a what of the CUDA memory allocator state across all devices?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "snapshot"
    },
    {
        "X": "What is the current GPU memory occupied by for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "tensors in bytes"
    },
    {
        "X": "What does Returns a snapshot of the CUDA memory allocator state across all devices?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "current GPU memory occupied by tensors in bytes"
    },
    {
        "X": "What does return for a given device?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "a dictionary of CUDA memory allocator statistics"
    },
    {
        "X": "What is the current memory allocator statistics for a given device?",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   ",
        "Y": "a human-readable printout"
    },
    {
        "X": "What is the CUDA memory allocator state across all devices?",
        "Z": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "snapshot"
    },
    {
        "X": "What does Returns the current GPU memory occupied by tensors in bytes for a given device?",
        "Z": "Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   ",
        "Y": "maximum GPU memory occupied by tensors in bytes"
    },
    {
        "X": "Returns a snapshot of what across all devices?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "CUDA memory allocator state"
    },
    {
        "X": "What does it do in tracking maximum GPU memory occupied by tensors for a given device?",
        "Z": "Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   ",
        "Y": "Resets the starting point"
    },
    {
        "X": "What is the name of the memory allocator state across all devices?",
        "Z": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "Y": "CUDA"
    },
    {
        "X": "Returns a snapshot of what state across all devices?",
        "Z": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "Y": "CUDA memory allocator state"
    },
    {
        "X": "Returns the current GPU memory occupied by tensors in bytes for a given device.",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "maximum GPU memory occupied by tensors in bytes"
    },
    {
        "X": "What does this function do for a given device?",
        "Z": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors"
    },
    {
        "X": "Returns the current GPU memory managed by the caching allocator in bytes for a given device.",
        "Z": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "Y": "current GPU memory managed by the caching allocator in bytes"
    },
    {
        "X": "What does it do to the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device"
    },
    {
        "X": "Returns the current GPU memory managed by what for a given device?",
        "Z": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "caching allocator"
    },
    {
        "X": "What is occupied by tensors in bytes for a given device?",
        "Z": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "maximum GPU memory"
    },
    {
        "X": "What does Returns the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "current GPU memory managed by the caching allocator in bytes"
    },
    {
        "X": "Returns the maximum GPU memory managed by what for a given device?",
        "Z": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "the caching allocator in bytes"
    },
    {
        "X": "What does a process do for a process?",
        "Z": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "Set memory fraction"
    },
    {
        "X": "What is the maximum GPU memory occupied by?",
        "Z": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "tensors"
    },
    {
        "X": "What does memory_reserved do?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Set memory fraction for a process"
    },
    {
        "X": "What is memory_reserved()?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Deprecated"
    },
    {
        "X": "What is a deprecated function that returns the maximum GPU memory occupied by tensors in bytes for a given device",
        "Z": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "max_memory_reserved"
    },
    {
        "X": "Returns the maximum GPU memory occupied by what in bytes for a given device?",
        "Z": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "tensors"
    },
    {
        "X": "What does it do for a given device?",
        "Z": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors"
    },
    {
        "X": "What does memory_reserved() do for a process?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Set memory fraction"
    },
    {
        "X": "Set memory fraction for a process. Deprecated; see what?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "memory_reserved()"
    },
    {
        "X": "Deprecated; see what?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "max_memory_reserved()"
    },
    {
        "X": "What is a deprecated function that returns the maximum GPU memory occupied by tensors for a given device?",
        "Z": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "max_memory_reserved()"
    },
    {
        "X": "Resets the starting point in tracking maximum GPU memory occupied by what for a given device?",
        "Z": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "tensors"
    },
    {
        "X": "Returns what for a given device?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "default Stream"
    },
    {
        "X": "Set what for a process?",
        "Z": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "memory fraction"
    },
    {
        "X": "What does memory_reserved do for a process?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Set memory fraction"
    },
    {
        "X": "What is the name of the deprecated function that returns the maximum GPU memory managed by the caching allocator for a given device",
        "Z": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "max_memory_reserved()"
    },
    {
        "X": "Which memory allocator resets the \"peak\" stats?",
        "Z": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "CUDA"
    },
    {
        "X": "What is returned in bytes for a given device?",
        "Z": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "maximum GPU memory managed by the caching allocator"
    },
    {
        "X": "What is reset in tracking maximum GPU memory managed by the caching allocator for a given device?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "starting point"
    },
    {
        "X": "What does the \u201cpeak\u201d stats track?",
        "Z": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "CUDA memory allocator"
    },
    {
        "X": "nvtx.mark Describe an event that occurred at some point.",
        "Z": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "Y": "instantaneous"
    },
    {
        "X": "What pushes a range onto a stack of nested range spans?",
        "Z": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "Y": "nvtx.range_push"
    },
    {
        "X": "What pops a range off of a stack of nested range spans?",
        "Z": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "Y": "nvtx.range_pop"
    },
    {
        "X": "What does nvtx.mark describe?",
        "Z": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "Y": "an instantaneous event"
    },
    {
        "X": "What is modeled after SciPy's special module?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "The torch.special module"
    },
    {
        "X": "What version of PyTorch is the torch.special module in?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "BETA"
    },
    {
        "X": "What is the status of the torch.special module?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "New functions are still being added"
    },
    {
        "X": "Where can you find details of the torch.special module?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "the documentation of each function"
    },
    {
        "X": "Computes the entropy on input in what way?",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "elementwise"
    },
    {
        "X": "What does the torch.special module do?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the error function of input"
    },
    {
        "X": "What is the error function of input defined as?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is the torch.special module modeled after?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "SciPy\u2019s special module"
    },
    {
        "X": "What is still being added to the torch.special module?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "New functions"
    },
    {
        "X": "What is the name of each function in the torch.special module?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "documentation"
    },
    {
        "X": "What does the torch.special module compute on input?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "entropy"
    },
    {
        "X": "Out (Tensor, optional) \u2013 what?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "output tensor"
    },
    {
        "X": "What does the torch.special module compute?",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "Y": "the error function of input"
    },
    {
        "X": "What version of PyTorch is this module in?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "BETA"
    },
    {
        "X": "When may some functions change?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "PyTorch releases"
    },
    {
        "X": "What is the name of each function in PyTorch?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "documentation"
    },
    {
        "X": "Computes what on input?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "entropy"
    },
    {
        "X": "Out (Tensor, optional) \u2013 the output tensor.",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "optional"
    },
    {
        "X": "What is the function that computes the error function of input?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "Computes the error function of input"
    },
    {
        "X": "What is the error function of input?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "What does Compute the error function of input do?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "What is the complementary error function of input defined as?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is an example of a function that computes the error function of input?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "What error function is defined in the range (1,1)(-1, 1)(1,1)?",
        "Z": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: ",
        "Y": "inverse error function"
    },
    {
        "X": "What is defined in the range (1,1)(-1, 1)(1,1)?",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "inverse error function"
    },
    {
        "X": "What is the complementary error function of input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the inverse error function of input"
    },
    {
        "X": "What is out (Tensor, optional) defined as?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "output tensor"
    },
    {
        "X": "What is the inverse error function of input defined as?",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "input (Tensor)"
    },
    {
        "X": "What is another name for the logistic sigmoid function?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "expit"
    },
    {
        "X": "Computes what error function of input?",
        "Z": "Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. ",
        "Y": "inverse error function"
    },
    {
        "X": "What is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) \u2013 the input",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "inverse error function"
    },
    {
        "X": "What is the expit also known as?",
        "Z": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "the logistic sigmoid function"
    },
    {
        "X": "What is the name of the input tensor?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "input (Tensor)"
    },
    {
        "X": "What is the inverse error function of input?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the inverse error function of input"
    },
    {
        "X": "What is the inverse error function defined as?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "input (Tensor)"
    },
    {
        "X": "Computes the exponential of the elements minus what number of input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "1"
    },
    {
        "X": "What is the name of the function that computes the exponential of the elements minus 1 of input?",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "Note"
    },
    {
        "X": "What function is defined in the range (1,1)(-1, 1)(1,1)?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "inverse error function"
    },
    {
        "X": "Computes what of the elements minus 1 of input?",
        "Z": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note ",
        "Y": "exponential"
    },
    {
        "X": "What does the exponential of the elements minus 1 of input do?",
        "Z": "Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the inverse error function of input called?",
        "Z": "Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "input (Tensor)"
    },
    {
        "X": "Out (Tensor, optional) is what?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "output tensor"
    },
    {
        "X": "What function is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) \u2013 the",
        "Z": "Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "inverse error function"
    },
    {
        "X": "Computes what function of input?",
        "Z": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "base two exponential function"
    },
    {
        "X": "What is the logistic sigmoid function?",
        "Z": "Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "Y": "expit"
    },
    {
        "X": "Computes the exponential of the elements minus what amount of input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "1"
    },
    {
        "X": "The exponential of the elements minus 1 of input provides greater precision than what for small values of x?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "exp(x) - 1"
    },
    {
        "X": "What is the base of the exponential function of input?",
        "Z": "Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. ",
        "Y": "base two"
    },
    {
        "X": "Out (Tensor) - the output tensor.",
        "Z": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "optional"
    },
    {
        "X": "What does the exponential of the elements minus 1 of input provide?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "greater precision"
    },
    {
        "X": "Computes the expit also known as what?",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "the logistic sigmoid function"
    },
    {
        "X": "Computes the exponential of the elements minus what of input?",
        "Z": "Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. ",
        "Y": "1"
    },
    {
        "X": "This function provides greater precision than what for small values of x?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "exp(x) - 1"
    },
    {
        "X": "Computes the natural what of the absolute value of the gamma function on input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "logarithm"
    },
    {
        "X": "What function provides greater precision than exp(x) - 1 for small values of x?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "Computes the exponential of the elements minus 1 of input"
    },
    {
        "X": "What is the natural value of the absolute value of the gamma function on input?",
        "Z": "Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. ",
        "Y": "logarithm"
    },
    {
        "X": "What does this function provide than exp(x) - 1 for small values of x?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "greater precision"
    },
    {
        "X": "What does Compute?",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "the natural logarithm of the absolute value of the gamma function on input"
    },
    {
        "X": "What type of function is computed for each element of input?",
        "Z": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note ",
        "Y": "exponentially scaled zeroth order modified Bessel function of the first kind (as defined below) for each element of input"
    },
    {
        "X": "Computes the exponentially scaled zeroth order modified Bessel function of what kind?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "first kind"
    },
    {
        "X": "What is an example of a function that computes the base two exponential function of input?",
        "Z": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "Y": "Computes the base two exponential function of input"
    },
    {
        "X": "What natural function is computed of the absolute value of the gamma function on input?",
        "Z": "This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None ",
        "Y": "logarithm"
    },
    {
        "X": "What order modified Bessel function of the first kind is computed for each element of input?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "zeroth"
    },
    {
        "X": "What is the exponentially scaled zeroth order modified Bessel function of?",
        "Z": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "the first kind"
    },
    {
        "X": "What is the base two exponential function of input?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "* log1p(other)"
    },
    {
        "X": "What type of Tensor is the output tensor?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "optional"
    },
    {
        "X": "Computes what of input?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "base two exponential function"
    },
    {
        "X": "Computes what of the absolute value of the gamma function on input?",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "natural logarithm"
    },
    {
        "X": "What is computed for each element of input?",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "X": "What is the absolute value of the gamma function on input?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "natural logarithm"
    },
    {
        "X": "Out (Tensor, optional) - the output tensor.",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "optional"
    },
    {
        "X": "What order modified Bessel function is computed for each element of input?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "zeroth"
    },
    {
        "X": "What is input clamped to when eps is not None?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "eps"
    },
    {
        "X": "When eps is None and input  0 or input > 1, the function will yield what?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "NaN"
    },
    {
        "X": "What does the function do for each element of input?",
        "Z": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind"
    },
    {
        "X": "What is the epsilon for input clamp bound?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "float"
    },
    {
        "X": "What is the default value of the output tensor?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "None"
    },
    {
        "X": "What is the input with the following cases?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "Y": "* log1p(other)"
    },
    {
        "X": "What is scipy.special.xlog1py similar to?",
        "Z": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "Y": "SciPy"
    },
    {
        "X": "What is the default output tensor?",
        "Z": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "None out"
    },
    {
        "X": "What does the function compute with the following cases?",
        "Z": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "* log1p(other)"
    },
    {
        "X": "What does input (Number or Tensor) represent?",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "Y": "Multiplier"
    },
    {
        "X": "What is the name of the matrix that generates?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "Vandermonde"
    },
    {
        "X": "What are the columns of the output matrix?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "elementwise powers"
    },
    {
        "X": "If increasing is true, the order of the columns is reversed x0,x1,...,x(N1),x(N",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "Y": "True"
    },
    {
        "X": "Who is a Vandermonde matrix named for?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "Alexandre-Theophile Vandermonde"
    },
    {
        "X": "What type of input tensor is x (Tensor)?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "Y": "1-D"
    },
    {
        "X": "If increasing is what, the order of the columns is reversed?",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "Y": "True"
    },
    {
        "X": "Who is a matrix with a geometric progression in each row named for?",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "Y": "Alexandre-Theophile Vandermonde"
    },
    {
        "X": "What are the powers of the input vector?",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "Y": "elementwise"
    },
    {
        "X": "If increasing is what, the order of the columns is reversed x0,x1,...,x(N1)x0, ",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "Y": "True"
    },
    {
        "X": "What is x (Tensor)?",
        "Z": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "1-D input tensor"
    },
    {
        "X": "What is the number of columns in the output?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "N"
    },
    {
        "X": "If what is not specified, a square array is returned?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "N"
    },
    {
        "X": "What is increasing (bool, optional)?",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "Y": "Order of the powers of the columns"
    },
    {
        "X": "If what, the powers increase from left to right?",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "True"
    },
    {
        "X": "What does N stand for?",
        "Z": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "Number of columns in the output"
    },
    {
        "X": "What is returned if N is not specified?",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "a square array"
    },
    {
        "X": "What is the order of the powers of columns?",
        "Z": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "increasing"
    },
    {
        "X": "What happens if True?",
        "Z": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "the powers increase from left to right"
    },
    {
        "X": "What is N (int, optional)?",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "Number of columns in the output"
    },
    {
        "X": "What is N?",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "Number of columns in the output"
    },
    {
        "X": "What does increasing (bool, optional) mean?",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "Order of the powers of the columns"
    },
    {
        "X": "If True, what happens to the powers of columns?",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "Y": "the powers increase from left to right"
    },
    {
        "X": "What does PyTorch make?",
        "Z": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "Y": "fault-tolerant and elastic"
    },
    {
        "X": "What makes distributed PyTorch fault-tolerant?",
        "Z": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "Y": "Usage API Advanced Plugins"
    },
    {
        "X": "Is distributed PyTorch fault-tolerant or elastic?",
        "Z": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "Y": "fault-tolerant"
    },
    {
        "X": "Makes distributed PyTorch fault-tolerant and what else?",
        "Z": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "Y": "elastic"
    },
    {
        "X": "What makes distributed PyTorch fault-tolerant and elastic?",
        "Z": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "Y": "Usage API Advanced Plugins"
    },
    {
        "X": "What is the most important aspect of a computer?",
        "Z": "Usage ",
        "Y": "Usage"
    },
    {
        "X": "What is the term for what?",
        "Z": "Usage ",
        "Y": "Usage"
    },
    {
        "X": "What kind of plugins are available?",
        "Z": "API Advanced Plugins ",
        "Y": "API Advanced Plugins"
    },
    {
        "X": "What is the name of what?",
        "Z": "API Advanced Plugins ",
        "Y": "API Advanced Plugins"
    },
    {
        "X": "What does mantissa and exponent tensors do?",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "Y": "Decomposes input"
    },
    {
        "X": "What is the range of mantissa?",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "Y": "open interval"
    },
    {
        "X": "What type of input does mantissa support?",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "Y": "float inputs"
    },
    {
        "X": "What is the input tensor out?",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "Y": "input (Tensor)"
    },
    {
        "X": "Performs the element-wise division of tensor1 by what?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "Y": "tensor2"
    },
    {
        "X": "What is the result of the element-wise division of tensor1 by tensor2 multiplied by?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "Y": "scalar value"
    },
    {
        "X": "What is the name of the function that performs the element-wise division of tensor1 by tensor2?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "Y": "Warning"
    },
    {
        "X": "How does the element-wise division of tensor1 by tensor2 work?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "Y": "multiply the result by the scalar value"
    },
    {
        "X": "What is the result of the element-wise division of tensor1 by tensor2?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What division with addcdiv is no longer supported?",
        "Z": "Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "Warning Integer"
    },
    {
        "X": "What is the historic addcdiv behavior implemented as?",
        "Z": "Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "torch.trunc"
    },
    {
        "X": "The future addcdiv behavior is the same for all what?",
        "Z": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "dtypes"
    },
    {
        "X": "What is no longer supported for integer division?",
        "Z": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "addcdiv"
    },
    {
        "X": "What inputs can be implemented as (input + value * tensor1 / tensor2)?",
        "Z": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "float inputs"
    },
    {
        "X": "What does the future addcdiv behavior apply to?",
        "Z": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "all dtypes"
    },
    {
        "X": "The historic addcdiv behavior can be implemented as (input + value * what?",
        "Z": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "Y": "torch.trunc"
    },
    {
        "X": "The shapes of input, tensor1, and tensor2 must be what?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "broadcastable"
    },
    {
        "X": "For inputs of type FloatTensor or DoubleTensor, value must be a what?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "real number"
    },
    {
        "X": "Input (Tensor) \u2013 what is to be added tensor1 (Tensor)?",
        "Z": "The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "tensor"
    },
    {
        "X": "What is an example of a tensor that must be broadcastable?",
        "Z": "The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Example"
    },
    {
        "X": "What does the Future type encapsulate?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "asynchronous execution"
    },
    {
        "X": "What is the Future type primarily used by?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Distributed RPC Framework"
    },
    {
        "X": "What encapsulates an asynchronous execution of a callable?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "rpc_async()"
    },
    {
        "X": "What is an example of an asynchronous execution of a callable?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "rpc_async()"
    },
    {
        "X": "What does the Future type expose to add callback functions and set results?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "Y": "APIs"
    },
    {
        "X": "What is a beta feature?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "Warning GPU support"
    },
    {
        "X": "What does torch._C.Future encapsulate?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "asynchronous execution of a callable"
    },
    {
        "X": "What does torch._C.Future expose to add callback functions and set results?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "Y": "APIs"
    },
    {
        "X": "What version of Warning GPU support is subject to changes?",
        "Z": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. ",
        "Y": "beta"
    },
    {
        "X": "What is a beta feature, subject to changes?",
        "Z": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "Warning GPU support"
    },
    {
        "X": "When will the given callback function run?",
        "Z": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. ",
        "Y": "when the Future is completed"
    },
    {
        "X": "What can be added to the same Future, but the order in which they will be executed cannot be guaranteed?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "Multiple callbacks"
    },
    {
        "X": "How many arguments must a callback take?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "one argument"
    },
    {
        "X": "What can the callback function use to get the value?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "value() method"
    },
    {
        "X": "If the Future is already completed, the given callback will be run what?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "inline"
    },
    {
        "X": "What is GPU support a beta feature?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Warning"
    },
    {
        "X": "When will the given callback function be run?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "when the Future is completed"
    },
    {
        "X": "What cannot be guaranteed when multiple callbacks are added to the same Future?",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "the order in which they will be executed"
    },
    {
        "X": "What method can the callback function use to get the value?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "value()"
    },
    {
        "X": "What GPU support is a beta feature?",
        "Z": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. ",
        "Y": "GPU"
    },
    {
        "X": "What can a callback function use to get the value of a Future?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "value() method"
    },
    {
        "X": "Which method provides a way to synchronize after your callback has completed?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "then()"
    },
    {
        "X": "What can be cheaper if your callback does not return anything?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "add_done_callback"
    },
    {
        "X": "What kind of callback registration API do both then() and add_done_callback use?",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "same"
    },
    {
        "X": "What does add_done_callback behave in the same way as then()?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "GPU tensors"
    },
    {
        "X": "What is a Callable that takes in one argument, is the reference to this Future?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "callback"
    },
    {
        "X": "What is the name of the callable that takes in one argument?",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "Note"
    },
    {
        "X": "What method provides a way to synchronize after your callback has completed?",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "then()"
    },
    {
        "X": "What do both then() and add_done_callback use under the hood?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "same"
    },
    {
        "X": "What is the name of the callback that takes in one argument?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Future"
    },
    {
        "X": "What is the name of the callback that takes in one argument, is the reference to this Future?",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "which"
    },
    {
        "X": "What method behaves in the same way as GPU tensors?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "then()"
    },
    {
        "X": "What does this method behave in the same way as then()?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "GPU tensors"
    },
    {
        "X": "What is the name of the Callable that references the Future?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "Note"
    },
    {
        "X": "What is a callback for a Callable that takes in one argument, is the reference to this Future?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "Future"
    },
    {
        "X": "What is a callback that takes in one argument, is the reference to this Future?",
        "Z": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. ",
        "Y": "Future"
    },
    {
        "X": "What is a callback that takes in one argument?",
        "Z": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "Note"
    },
    {
        "X": "What is the reference to?",
        "Z": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "this Future"
    },
    {
        "X": "What must be carefully taken care of if the callback function throws?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "error handling"
    },
    {
        "X": "What is the user responsible for if the callback later completes additional futures?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "Y": "the user is responsible for handling completion/waiting on those futures independently"
    },
    {
        "X": "What is the user responsible for handling if the callback later completes additional futures?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "completion/waiting"
    },
    {
        "X": "What is the user responsible for handling on futures that are not marked as completed with an error?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "completion/waiting"
    },
    {
        "X": "What does Future.done() return if the Future is done?",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "True"
    },
    {
        "X": "A Future is done if it has what?",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "a result or an exception"
    },
    {
        "X": "What is populating the tensors that reside on GPUs?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "asynchronous kernels"
    },
    {
        "X": "What does Future.done() return if this Future is done?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "True"
    },
    {
        "X": "When is a Future done?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "if it has a result or an exception"
    },
    {
        "X": "If the result is already usable, what does Future.done() do?",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "synchronizations"
    },
    {
        "X": "If the value contains what that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are popul",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "tensors"
    },
    {
        "X": "What will return True if the value contains tensors that reside on GPUs?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "Future.done()"
    },
    {
        "X": "What does Future.done() do to ensure that the result is already usable?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "synchronizations"
    },
    {
        "X": "What function returns True if the value contains tensors that reside on GPUs?",
        "Z": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. ",
        "Y": "Future.done()"
    },
    {
        "X": "What will mark this Future as completed with an error and trigger all attached callbacks?",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "this Future"
    },
    {
        "X": "When calling wait()/value() on this Future, the exception set here will be raised what?",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "inline"
    },
    {
        "X": "What is the exception for this Future?",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "result"
    },
    {
        "X": "Set the result for this Future, which will mark this Future as what?",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "completed"
    },
    {
        "X": "What does a Future cannot be marked completed twice?",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "a Future cannot be marked completed twice"
    },
    {
        "X": "What does the result for this Future do?",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "mark this Future as completed"
    },
    {
        "X": "What will mark this Future as completed and trigger all attached callbacks?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "Set the result for this Future"
    },
    {
        "X": "How many times can a Future be marked completed?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "twice"
    },
    {
        "X": "What happens when a Future is marked as completed?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "trigger all attached callbacks"
    },
    {
        "X": "What does it mean that a Future cannot be marked completed twice?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "a Future cannot be marked completed twice"
    },
    {
        "X": "What is the result object of the Future?",
        "Z": "result (object) \u2013 the result object of this Future. ",
        "Y": "object"
    },
    {
        "X": "What is the result object of this Future?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "object"
    },
    {
        "X": "What does a callback return that reside on a GPU?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. ",
        "Y": "tensors"
    },
    {
        "X": "What must one do if one wants to change streams?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "re-synchronize them with the original streams"
    },
    {
        "X": "What is a Callable that takes this Future as the only argument?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "Callable"
    },
    {
        "X": "A new Future object that holds what of the callback will be marked as completed when the given callback finishes?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "return value"
    },
    {
        "X": "What does a Callable that takes this Future as the only argument do?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "Note"
    },
    {
        "X": "What is the only argument for a callback?",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "Future"
    },
    {
        "X": "When a new Future object holds the return value of the callback is marked as what?",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "completed"
    },
    {
        "X": "What does a Callable that takes this Future object hold the return value of the callback and will be marked as completed when the given callback finishes",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "Note"
    },
    {
        "X": "What is a new Future object that holds the return value of a callback that will be marked as completed when the given callback finishes?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "Note"
    },
    {
        "X": "When will a new Future object be marked as completed?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "completed when the given callback finishes"
    },
    {
        "X": "What holds the return value of the callback and will be marked as completed when the given callback finishes?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "A new Future object"
    },
    {
        "X": "What is a new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "Y": "Note"
    },
    {
        "X": "The future returned by fut.wait() will be marked appropriately with what?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "Y": "the encountered error"
    },
    {
        "X": "What happens if the callback function throws an error?",
        "Z": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "Y": "the future returned by then will be marked appropriately"
    },
    {
        "X": "What will be marked appropriately with the encountered error if the callback function throws?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "the future returned by then"
    },
    {
        "X": "What is the value of?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "an already-completed future"
    },
    {
        "X": "Obtain the value of an already-completed future. This method should only be called after a call to what?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "wait() has completed"
    },
    {
        "X": "What could happen if the method is called after a call to wait() has completed?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "Future may not yet hold a value"
    },
    {
        "X": "What is the purpose of this method?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "Obtain the value of an already-completed future"
    },
    {
        "X": "Obtain the value of an already-completed future. This method should only be called after a call to what function has completed?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "wait()"
    },
    {
        "X": "Where should a method be called to obtain the value of an already-completed future?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "inside a callback function"
    },
    {
        "X": "When should this method be called?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "after a call to wait() has completed"
    },
    {
        "X": "What may not yet hold a value?",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "Future"
    },
    {
        "X": "Where do tensors reside?",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "GPUs"
    },
    {
        "X": "What method should be used to perform additional synchronization if the value contains tensors that reside on GPUs?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "wait()"
    },
    {
        "X": "Where should this method be called after a call to wait() has completed?",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "inside a callback function"
    },
    {
        "X": "What could happen if a call to value() is called after a call to wait() has completed?",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "Future may not yet hold a value"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, this method will not perform any additional what?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "synchronization"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "wait()"
    },
    {
        "X": "The synchronization of tensors that reside on GPUs should be done separately through a call to what?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "wait()"
    },
    {
        "X": "What is the value held by?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "Future"
    },
    {
        "X": "What happens to the value held by this Future?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "the function (callback or RPC) creating the value has thrown an error"
    },
    {
        "X": "What does the value() method do until the value of this Future is ready?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "Block"
    },
    {
        "X": "What is the name of the method that performs synchronization of tensors that reside on GPUs?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "wait()"
    },
    {
        "X": "What is the value held by this Future?",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "The value held by this Future"
    },
    {
        "X": "What method will throw an error if the function creating the value has thrown an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "wait method"
    },
    {
        "X": "How long should the value() method block?",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "until the value of this Future is ready"
    },
    {
        "X": "What method throws an error if the function creating the value throws an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "value() method"
    },
    {
        "X": "How long is the value held by this Future blocked?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "until the value of this Future is ready"
    },
    {
        "X": "What is the name of the block until the value of the Future is ready?",
        "Z": "Block until the value of this Future is ready. ",
        "Y": "Block"
    },
    {
        "X": "How long until the value of this Future is ready?",
        "Z": "Block until the value of this Future is ready. ",
        "Y": "Block"
    },
    {
        "X": "What does the wait method return if the function creating the value has thrown an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "The value held by this Future"
    },
    {
        "X": "What functions create the value of a Future?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "callback or RPC"
    },
    {
        "X": "What is the function that collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed"
    },
    {
        "X": "What is a list of Future objects?",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "list"
    },
    {
        "X": "What does this return a Future object to?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "Returns a Future object to a list of the passed in Futures"
    },
    {
        "X": "What is the value held by a Future?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "value held by this Future"
    },
    {
        "X": "What function created the value has thrown an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "callback or RPC"
    },
    {
        "X": "What is done when all of the sub-futures are completed?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "Collects the provided Future objects into a single combined Future"
    },
    {
        "X": "Returns a Future object to what?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "a list of the passed in Futures"
    },
    {
        "X": "When is a single combined Future completed?",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "when all of the sub-futures are completed"
    },
    {
        "X": "What is a futures (list)?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "list of Future object"
    },
    {
        "X": "What does futures return?",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "Returns a Future object to a list of the passed in Futures"
    },
    {
        "X": "What does the method do that returns the list of completed values?",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "Waits for all provided futures to be complete"
    },
    {
        "X": "What happens if any of the futures encounter an error?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "exit early"
    },
    {
        "X": "What does the method do?",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "Waits for all provided futures to be complete"
    },
    {
        "X": "If any of the futures encounter an error, the method will do what?",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "exit early"
    },
    {
        "X": "What is futures (list)?",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "a list of Future object"
    },
    {
        "X": "What does this method do?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "Returns a Future object to a list of the passed in Futures"
    },
    {
        "X": "What happens if a future encounters an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "exit early"
    },
    {
        "X": "What is a list of in futures?",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "completed Future results"
    },
    {
        "X": "When will this method throw an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "if wait on any Future throws"
    },
    {
        "X": "Returns what to a list of the passed in Futures?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "a Future object"
    },
    {
        "X": "Futures (list) \u2013 a list of what?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "completed Future results"
    },
    {
        "X": "What does this method do if wait on any Future throws?",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "throw an error"
    },
    {
        "X": "What does the dividend and divisor compute?",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend ",
        "Y": "the element-wise remainder of division"
    },
    {
        "X": "The remainder has the same sign as what?",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend ",
        "Y": "dividend input"
    },
    {
        "X": "What does the dividend support?",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "Y": "broadcasting"
    },
    {
        "X": "When the divisor is zero, returns what for floating point dtypes on both CPU and GPU?",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "NaN"
    },
    {
        "X": "What is the dividend input?",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend ",
        "Y": "input (Tensor)"
    },
    {
        "X": "The remainder of the dividend and divisor has the same sign as what?",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "Y": "dividend input"
    },
    {
        "X": "What is input (Tensor) \u2013?",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "the dividend"
    },
    {
        "X": "Returns what of (input - other) The shapes of input and other must be broadcastable?",
        "Z": "Returns the p-norm of (input - other) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (float, optional) \u2013 the norm to be computed Example: ",
        "Y": "p-norm"
    },
    {
        "X": "What is the other (Tensor)?",
        "Z": "Returns the p-norm of (input - other) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (float, optional) \u2013 the norm to be computed Example: ",
        "Y": "the Right-hand-side input tensor"
    },
    {
        "X": "What is reps treated as if input has shape and reps is 2?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "(1, 1, 2, 2)."
    },
    {
        "X": "If input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "if input has fewer dimensions than reps specifies"
    },
    {
        "X": "If input has shape (what is the number of dimensions) and reps is (3, 3, 2, 2), then input is treated as if it had",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "4, 2)"
    },
    {
        "X": "What is this function similar to?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "NumPy\u2019s tile function"
    },
    {
        "X": "What is the tensor whose elements to repeat?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "input"
    },
    {
        "X": "What does reps represent?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "the number of repetitions per dimension"
    },
    {
        "X": "What is an example of a tensor whose elements to repeat?",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "Y": "Example"
    },
    {
        "X": "What is another name for a 3D transposed convolution operator over an input image composed of several input planes?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "Y": "deconvolution"
    },
    {
        "X": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes called what?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "Y": "deconvolution"
    },
    {
        "X": "What type of transposed convolution operator is applied over an input image composed of several input planes?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "Y": "2D"
    },
    {
        "X": "What does deconvolution combine an array of sliding local blocks into?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "Y": "large containing tensor"
    },
    {
        "X": "What is a partial inverse of MaxPool3d?",
        "Z": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What does Compute a partial inverse of MaxPool2d?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What is extracted from a batched input tensor?",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "sliding local blocks"
    },
    {
        "X": "What does Computes a partial inverse of MaxPool1d?",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "Applies a 1D power-average pooling over an input signal composed of what?",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What does the array of sliding local blocks combine into?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "a large containing tensor"
    },
    {
        "X": "What does a 3D pooling over an input signal comprised of several input planes do?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What does the 3D pooling function do?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What pooling over an input signal composed of several input planes?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "1D power-average"
    },
    {
        "X": "Applies a 2D power-average pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "Applies a 1D max pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "What is a partial inverse of MaxPool2d?",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "Applies what operation in kTkHkWkT times kH times kWkT",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "3D average-pooling"
    },
    {
        "X": "What does the 3D pooling over an input signal comprised of several input planes do?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What is an input signal composed of?",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "several input planes"
    },
    {
        "X": "What is a 3D transposed convolution operator called?",
        "Z": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "Y": "deconvolution"
    },
    {
        "X": "What is a 1D adaptive max pooling over an input signal composed of?",
        "Z": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "A 3D adaptive max pooling over an input signal composed of what?",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "What is a 1D adaptive average pooling over an input signal composed of?",
        "Z": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "What does 3D fractional max pooling over an input signal composed of several input planes?",
        "Z": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "2D fractional max pooling"
    },
    {
        "X": "Applies 3D fractional max pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "Applies the element-wise function ReLU6(x)=min(max(0,x),6)textReLU6",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   ",
        "Y": "hardswish"
    },
    {
        "X": "What is the name of the element-wise function?",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   ",
        "Y": "ELU(x)"
    },
    {
        "X": "What is the in-place version of elu()?",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   ",
        "Y": "elu()"
    },
    {
        "X": "What is the scale of elu()?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "Y": "1.0507009873554804934193349852946"
    },
    {
        "X": "What is the name of the in-place version of elu()?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "Y": "LeakyReLU(x)"
    },
    {
        "X": "What is another term for Gaussian negative log likelihood loss?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Gaussian negative log likelihood loss"
    },
    {
        "X": "A squared term is used if the absolute element-wise error falls below what?",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "Y": "beta"
    },
    {
        "X": "What is the name of the loss that combines log_softmax and nll_loss in a single function?",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "Y": "Poisson negative log likelihood loss"
    },
    {
        "X": "Where does the absolute element-wise error fall?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "below beta and an L1 term otherwise"
    },
    {
        "X": "What function uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "Z": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "SoftMarginLoss"
    },
    {
        "X": "Upsamples the input, using nearest neighbours\u2019 pixel values. Upsamples the input using what?",
        "Z": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "Y": "bilinear upsampling"
    },
    {
        "X": "What is the first argument of the matrix-matrix product?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "Y": "1"
    },
    {
        "X": "If the first argument is 2-dimensional and the second argument is what, the matrix-vector product is returned?",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "Y": "1-dimensional"
    },
    {
        "X": "What is returned if both arguments are at least 1-dimensional and at least one argument is N-dimensional?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "batched matrix multiply"
    },
    {
        "X": "If the first argument is 2-dimensional and the second argument is 1-dimensional, what dimension is prepended to its dimension for the purpose of the batched matrix",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "Y": "1"
    },
    {
        "X": "If the second argument is what, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "1-dimensional"
    },
    {
        "X": "What type of dimensions are broadcasted?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "non-matrix"
    },
    {
        "X": "What is another term for non-matrix dimensions?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "batch"
    },
    {
        "X": "What dimension is the first argument in a batched matrix multiply?",
        "Z": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "Y": "1"
    },
    {
        "X": "If the second argument is at least what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after",
        "Z": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "Y": "1-dimensional"
    },
    {
        "X": "Return the singular value decomposition of a matrix, batches of matrices, or a sparse matrix AAA such that A",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "U, S, V"
    },
    {
        "X": "What is computed for the matrix AMA - MAM?",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "SVD"
    },
    {
        "X": "What is low-rank SVD useful for?",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "huge sparse matrices"
    },
    {
        "X": "What must niter be?",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "Y": "a nonnegative integer"
    },
    {
        "X": "What do NNN tensors create?",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "Y": "NNN N-dimensional grids"
    },
    {
        "X": "What is a torch.Tensor?",
        "Z": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: ",
        "Y": "multi-dimensional matrix containing elements of a single data type"
    },
    {
        "X": "What type of tensor does Torch define?",
        "Z": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean ",
        "Y": "LongTensor Boolean"
    },
    {
        "X": "What is the data type of CPU tensor GPU tensor 32-bit floating point torch?",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "Y": "dtype"
    },
    {
        "X": "What is the name of the 32-bit floating point torch?",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "Y": "dtype CPU tensor GPU tensor"
    },
    {
        "X": "What is the name of the floating point torch?",
        "Z": "32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "32-bit floating point torch"
    },
    {
        "X": "What is another name for torch?",
        "Z": "torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "Y": "float32"
    },
    {
        "X": "What is another name for a doubletensor torch?",
        "Z": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "Y": "double torch"
    },
    {
        "X": "What is the sign of an IntTensor 6?",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 ",
        "Y": "4-bit integer"
    },
    {
        "X": "What is a bfloat16 torch?",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "16-bit floating point 2 torch"
    },
    {
        "X": "What is the unsigned name of the IntTensor torch?",
        "Z": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "Y": "/ quantized 4-bit integer"
    },
    {
        "X": "What does binary16 use?",
        "Z": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "1 sign, 5 exponent, and 10 significand bits"
    },
    {
        "X": "When is binary16 useful?",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "Y": "when precision is important at the expense of range"
    },
    {
        "X": "What is binary16 sometimes referred to as?",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "When is Brain Floating Point useful?",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 ",
        "Y": "range is important"
    },
    {
        "X": "What is the sign for torch.int8 torch?",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 ",
        "Y": "8-bit integer"
    },
    {
        "X": "What is the quantized 4-bit integer stored as?",
        "Z": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. ",
        "Y": "8-bit signed integer"
    },
    {
        "X": "What is Brain Floating Point only supported in?",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "Y": "EmbeddingBag operator"
    },
    {
        "X": "What is float32 quantized 4-bit integer stored as?",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "Y": "8-bit signed integer"
    },
    {
        "X": "What is an alias for the default tensor type?",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What can be used to create a tensor?",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "requires_grad=True"
    },
    {
        "X": "What can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. ",
        "Y": "A tensor of specific data type"
    },
    {
        "X": "What are the contents of a tensor accessed and modified using?",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "Y": "indexing and slicing notation"
    },
    {
        "X": "What does torch.Storage do?",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "Y": "holds its data"
    },
    {
        "X": "What are the three attributes of a torch.Tensor?",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "Y": "torch.dtype, torch.device, and torch.layout attributes"
    },
    {
        "X": "What is Indexing, Slicing, Joining, Mutating Ops?",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "indexing"
    },
    {
        "X": "What holds each tensor's data?",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "associated torch.Storage"
    },
    {
        "X": "What does the tensor class provide of a storage?",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "multi-dimensional, strided view"
    },
    {
        "X": "What are some attributes of a torch.Tensor?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "torch.dtype, torch.device, and torch.layout attributes"
    },
    {
        "X": "What does to() method on a tensor do?",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What can a tensor be created with?",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "requires_grad=True"
    },
    {
        "X": "What holds the data of a tensor?",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "torch.Storage"
    },
    {
        "X": "What does the to() method on a tensor do?",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "Y": "Warning"
    },
    {
        "X": "What kind of view does the tensor class provide?",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "Y": "multi-dimensional"
    },
    {
        "X": "What Returns a new Tensor with data as the tensor data?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Tensor.new_tensor"
    },
    {
        "X": "Tensor.imag Returns a new tensor containing what values of the self tensor?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "Y": "imaginary values"
    },
    {
        "X": "What Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "Y": "abs()"
    },
    {
        "X": "Tensor.new_full Returns a Tensor of size size filled with what?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "fill_value"
    },
    {
        "X": "What is returned a Tensor of size size filled with?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "Y": "uninitialized data"
    },
    {
        "X": "In-place version of what Alias for abs() Tensor.absolute?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "Y": "abs() Tensor.absolute"
    },
    {
        "X": "What is the name of the device where the Tensor is stored?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ ",
        "Y": "torch.device"
    },
    {
        "X": "What can be added to a self tensor?",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "Y": "a scalar or tensor"
    },
    {
        "X": "What is the name of the tensor that is stored on the GPU?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ ",
        "Y": "addbmm"
    },
    {
        "X": "Add what to self tensor?",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() ",
        "Y": "scalar or tensor"
    },
    {
        "X": "What Add a scalar or tensor to self tensor?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "Y": "add"
    },
    {
        "X": "Where is the Tensor.device located?",
        "Z": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "Y": "torch.device"
    },
    {
        "X": "What does the Alias for dim() Tensor.ndim return?",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "What does abs() Tensor.absolute Alias for abs() Tensor.absolute Alias for ab",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() ",
        "Y": "abs() Tensor"
    },
    {
        "X": "What does Tensor.imag return?",
        "Z": "Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "Y": "imaginary values"
    },
    {
        "X": "What provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "PyTorch"
    },
    {
        "X": "What is stored contiguously in memory?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "array elements"
    },
    {
        "X": "What class of multi-dimensional arrays have a property of having a vast portion of elements being equal to zero?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "sparse arrays"
    },
    {
        "X": "What has a property of having a vast portion of elements being equal to zero?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "Sparse arrays"
    },
    {
        "X": "What are some sparse storage formats?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "COO, CSR/CSC, LIL"
    },
    {
        "X": "What are sparse storage formats optimized for?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "specific operations on the arrays"
    },
    {
        "X": "What is a sparse storage format?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note ",
        "Y": "Note"
    },
    {
        "X": "What do we use when talking about storing only non-zero elements of a sparse array?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "specified elements"
    },
    {
        "X": "What term is used to denote the unspecified elements of a sparse array?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "fill value"
    },
    {
        "X": "What format can be advantageous only when the size and sparsity levels of arrays are high?",
        "Z": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "Y": "sparse storage"
    },
    {
        "X": "What is the name of the storage format for sparse tensors implemented by PyTorch?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "Coordinate format"
    },
    {
        "X": "What is the indices of specified elements called?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "Y": "ndim"
    },
    {
        "X": "What is the number of specified elements in a sparse COO tensor?",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "Y": "nse"
    },
    {
        "X": "What is at least product(tensor shape>) * size of element type in bytes>?",
        "Z": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "Y": "strided tensor"
    },
    {
        "X": "How much memory saving does a sparse COO tensor have from using the COO storage format?",
        "Z": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "Y": "200 fold"
    },
    {
        "X": "What is the memory consumption of a 10 000 x 10 000 tensor with non-zero 32-bit floating point numbers?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "Y": "2 000 000 bytes"
    },
    {
        "X": "What is the default value of the fill value in a sparse tensor?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "zero"
    },
    {
        "X": "What is NOT a list of index tuples?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "the input i"
    },
    {
        "X": "If you want to write your indices this way, you should do what before passing them to the sparse constructor?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "Y": "transpose"
    },
    {
        "X": "What is the input i of a sparse COO tensor?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "Y": "i"
    },
    {
        "X": "What would we use to define a sparse tensor?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "Y": "i"
    },
    {
        "X": "What is the input i NOT a list of?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "index tuples"
    },
    {
        "X": "What extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional ten",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "Y": "PyTorch"
    },
    {
        "X": "How does PyTorch hybrid COO tensor extend the sparse COO tensor?",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "Y": "allowing the values tensor to be a multi-dimensional tensor"
    },
    {
        "X": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tens",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "multi-dimensional tensor"
    },
    {
        "X": "Where is the entry [5, 6] in a dimensional tensor?",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "(1, 0"
    },
    {
        "X": "What would we do to create a 2 + 1-dimensional tensor?",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "write"
    },
    {
        "X": "Where is the entry [5, 6] in the tensor?",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "(1, 0"
    },
    {
        "X": "What are the invariants of a sparse COO tensor?",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "Y": "s.sparse_dim(), K = s.dense_dim()"
    },
    {
        "X": "What is the location of the entry [5, 6] in a 2 + 1-dimensional tensor?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "(1, 0"
    },
    {
        "X": "What are the corresponding tensor values collected with?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, ",
        "Y": "arbitrary integer or floating point number element type"
    },
    {
        "X": "Where do we want to create a 2 + 1-dimensional tensor?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, ",
        "Y": "(1, 0"
    },
    {
        "X": "If s is a sparse COO tensor and M = s.sparse_dim(), what",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, ",
        "Y": "K"
    },
    {
        "X": "What are the values of a hybrid sparse tensor stored as?",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "strided tensors"
    },
    {
        "X": "What are the values of a hybrid tensor stored as?",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "strided tensors"
    },
    {
        "X": "What does PyTorch sparse COO tensor format permit?",
        "Z": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: ",
        "Y": "uncoalesced sparse tensors"
    },
    {
        "X": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors where there may",
        "Z": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, ",
        "Y": "duplicate coordinates"
    },
    {
        "X": "What will accumulate the multi-valued elements into a single value using summation?",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "Y": "the coalescing process"
    },
    {
        "X": "What are the indices of specified tensor elements?",
        "Z": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, ",
        "Y": "unique"
    },
    {
        "X": "PyTorch sparse COO tensor format permits what?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "uncoalesced sparse tensors"
    },
    {
        "X": "How many values can be specified for the same index?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "Y": "1"
    },
    {
        "X": "What are two values that lead to a 1-D uncoalesced tensor?",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "Y": "3 and 4"
    },
    {
        "X": "How many values can be specified for the same index 1?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "Y": "3 and 4"
    },
    {
        "X": "What are the properties of a sparse tensor?",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: ",
        "Y": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order"
    },
    {
        "X": "Why should you coalesce sparse tensors?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "to prevent them from growing too large"
    },
    {
        "X": "What tensor is coalesced?",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "Y": "sparse tensor"
    },
    {
        "X": "In what order are indices sorted?",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "lexicographical"
    },
    {
        "X": "What is coalesced?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "sparse tensor"
    },
    {
        "X": "How are sparse COO tensors implemented?",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "Y": "by simply concatenating the indices and values tensors"
    },
    {
        "X": "What is a sparse COO tensor a instance of?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What should you do to prevent sparse tensors from growing too large?",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: ",
        "Y": "coalesce"
    },
    {
        "X": "What type of COO tensor is a torch.Tensor instance?",
        "Z": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "sparse"
    },
    {
        "X": "What can be acquired using methods torch.Tensor.indices() and torch.Tensor.values()?",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "COO format data"
    },
    {
        "X": "What can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products?",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "lexicographical ordering"
    },
    {
        "X": "What dimension can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "sparse"
    },
    {
        "X": "How could the scalar multiplication on an uncoalesced sparse tensor be implemented?",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "multiplying all the uncoalesced values with the scalar"
    },
    {
        "X": "What does not hold in general?",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "sqrt(a + b) == sqrt(a) + sqrt(b)"
    },
    {
        "X": "The values of the same indices are what?",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. ",
        "Y": "terms of a sum"
    },
    {
        "X": "What must one take into account when working with uncoalesced sparse COO tensors?",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "the additive nature of uncoalesced data"
    },
    {
        "X": "Slicing (with positive step) of a sparse COO tensor is supported only for what?",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "dense dimensions"
    },
    {
        "X": "What is supported for both sparse and dense dimensions?",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "Indexing"
    },
    {
        "X": "What are the values of the same indices?",
        "Z": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "the terms of a sum"
    },
    {
        "X": "When working with uncoalesced sparse COO tensors, one must take into account what?",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "additive nature of uncoalesced data"
    },
    {
        "X": "What format implements the CSR format for storage of 2 dimensional tensors?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "CSR"
    },
    {
        "X": "The values tensor contains the values of the CSR tensor. This is a what?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "1-D tensor"
    },
    {
        "X": "What is the CSR tensor of size nnz?",
        "Z": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "1-D tensor"
    },
    {
        "X": "What is the default element type for index tensors crow_indices and col_indices?",
        "Z": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "torch.int64"
    },
    {
        "X": "What can be directly constructed by using the torch._sparse_csr_tensor() method?",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "Sparse CSR matrices"
    },
    {
        "X": "What will be interpreted as missing values in the sparse tensor?",
        "Z": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "Y": "zeros"
    },
    {
        "X": "The sparse matrix-vector multiplication is currently the only math operation supported on what?",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "CSR tensors"
    },
    {
        "X": "What is a PyTorch operation Sparse grad?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "PyTorch operation Sparse grad"
    },
    {
        "X": "What function is no longer used for Linear Algebra operations on sparse matrices?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "Y": "addmm()"
    },
    {
        "X": "What does svd_lowrank() yes indicate if the PyTorch operation supports backward with respect to sparse matrix argument",
        "Z": "torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "SVD"
    },
    {
        "X": "What does the PyTorch operation support backward with respect to sparse matrix argument?",
        "Z": "no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "All PyTorch operations"
    },
    {
        "X": "What is the only PyTorch operation that supports backward with respect to sparse matrix argument?",
        "Z": "M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "torch"
    },
    {
        "X": "All PyTorch operations, except torch.smm(), support backward with respect to what?",
        "Z": "torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "Y": "strided matrix arguments"
    },
    {
        "X": "PyTorch does not support matrix multiplication with what layout signature?",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "Y": "M[strided] @ M[sparse_coo]"
    },
    {
        "X": "PyTorch applications can still compute this using what?",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "Y": "matrix relation"
    },
    {
        "X": "Where are the values of a sparse tensor filtered by the indices of the sparse tensor mask",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "a strided tensor self"
    },
    {
        "X": "What _ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.sparse_resize"
    },
    {
        "X": "What does Tensor.sparse_mask return?",
        "Z": "Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. ",
        "Y": "a new sparse tensor"
    },
    {
        "X": "What is the name of the method that resizes a sparse tensor?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What is the name of the number of sparse dimensions in a sparse tensor self?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "sparse"
    },
    {
        "X": "What does the Tensor.sparse_mask return?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "Y": "a new sparse tensor"
    },
    {
        "X": "Return the number of dense dimensions in a what self?",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "Y": "sparse tensor"
    },
    {
        "X": "What does Tensor.to_sparse Return?",
        "Z": "Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. ",
        "Y": "a sparse copy of the tensor"
    },
    {
        "X": "What Returns a new sparse tensor with values from a strided tensor self filtered by the ",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense ",
        "Y": "Tensor.sparse_mask"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, return a coalesced copy of self",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "uncoalesced"
    },
    {
        "X": "Returns what with values from a strided tensor self filtered by the indices of the sparse tens",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: ",
        "Y": "a new sparse tensor"
    },
    {
        "X": "What Returns a sparse copy of the tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.to_sparse"
    },
    {
        "X": "If self is a sparse COO tensor, what type of tensor is returned?",
        "Z": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "uncoalesced"
    },
    {
        "X": "What returns the indices of the self tensor when self is a sparse CSR tensor of layout spars",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "Y": "Tensor.col_indices"
    },
    {
        "X": "Convert a tensor to what format?",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "Y": "compressed row storage format"
    },
    {
        "X": "Return the tensor of a sparse COO tensor. Tensor.values Return the values tens",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "Y": "indices"
    },
    {
        "X": "If self is a sparse COO tensor, return a coalesced copy of self if self is what?",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "uncoalesced"
    },
    {
        "X": "What Returns the tensor containing the compressed row indices of the self tensor when self is a sparse",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.crow_indices"
    },
    {
        "X": "Return the values of what of a sparse COO tensor?",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "Y": "tensor"
    },
    {
        "X": "The following methods are specific to what CSR tensors?",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "sparse"
    },
    {
        "X": "What does a sparse tensor self do?",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Removes all specified elements"
    },
    {
        "X": "What Returns the tensor containing the column indices of the self tensor when self is a sparse C",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.col_indices"
    },
    {
        "X": "Performs a matrix multiplication of what?",
        "Z": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "Y": "sparse matrix input with the dense matrix mat"
    },
    {
        "X": "Default: if None, what does torch.full_like(input.size(), fill_value, input.dtype",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "defaults to the dtype of input"
    },
    {
        "X": "If None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned ten",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "None"
    },
    {
        "X": "What is the desired memory format of returned tensor?",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "Y": "memory_format"
    },
    {
        "X": "What is the name of the short-time Fourier transform function that will only return complex tensors?",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. ",
        "Y": "return_complex=True"
    },
    {
        "X": "What does STFT compute of short overlapping windows of the input?",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. ",
        "Y": "The STFT computes the Fourier transform"
    },
    {
        "X": "The interface of the STFT computes the Fourier transform of short overlapping windows of the input is modeled after what?",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "Y": "librosa stft function"
    },
    {
        "X": "What must the input be?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "Y": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "X": "What can a window be?",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "1-D tensor of size win_length"
    },
    {
        "X": "If window is None (default), it is treated as if having what everywhere in the window?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "111"
    },
    {
        "X": "If win_length is None, window will be padded on both sides to length n_fft before being applied.",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "n_fft"
    },
    {
        "X": "If win_length is None, window will be padded on both sides to length n_fft before being applied?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "Y": "n_fft"
    },
    {
        "X": "If center is what, input will be padded on both sides so that the ttt-th frame is centered?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "True"
    },
    {
        "X": "What is the default value for input when center is True?",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "reflect"
    },
    {
        "X": "If True, input will be padded on both sides so that the ttt-th frame is centered at time thop_",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "center"
    },
    {
        "X": "What is the default value for the padding method used on input when center is True?",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "reflect"
    },
    {
        "X": "If what is the default, input will be padded on both sides so that the ttt-th frame is centered?",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "center is True"
    },
    {
        "X": "What is the default setting for a window when center is True?",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "reflect"
    },
    {
        "X": "If win_length  textn_fftwin_lengthn_fft, window will be",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "n_fft"
    },
    {
        "X": "What is the name of all available options?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "torch.nn.functional.pad()"
    },
    {
        "X": "What is the default setting for the padding method used on input when center is True?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "\"reflect\""
    },
    {
        "X": "If normalized is True, what is the default?",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "If normalized is True"
    },
    {
        "X": "When does the function return the normalized STFT results?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "Y": "If normalized is True"
    },
    {
        "X": "Returns what if return_complex is true?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "Y": "a complex tensor of size"
    },
    {
        "X": "What is the number of frequencies where STFT is applied?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "Y": "NNN"
    },
    {
        "X": "Calling with what may cause error or return incorrect result?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "Y": "previous signature"
    },
    {
        "X": "The input tensor n_fft (int) \u2013 the input tensor n_fft (",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "Y": "Fourier transform"
    },
    {
        "X": "Returns either a complex tensor of size (NT)(* times N times T ",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "a real tensor of size"
    },
    {
        "X": "What is the default value for the window function?",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "Y": "None"
    },
    {
        "X": "What is the default value for window of all 111 s?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "None"
    },
    {
        "X": "What frame is centered at time thop_lengtht times texthop_lengththop_",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "ttt-th"
    },
    {
        "X": "What is the default value for return normalized STFT results?",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "Y": "False"
    },
    {
        "X": "What is the default to return half of results to avoid redundancy for real inputs?",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "False onesided"
    },
    {
        "X": "What is the optional window function?",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "Y": "window"
    },
    {
        "X": "What is the default for STFT results?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "\"reflect\" normalized"
    },
    {
        "X": "What is the name of the tensor that returns a complex tensor?",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "return_complex"
    },
    {
        "X": "What is one way to count the frequency of each value in an array of non-negative ints?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "Count the frequency of each value in an array of non-negative ints"
    },
    {
        "X": "What is the result if the number of bins is one larger than the largest value in input unless input is empty?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "a tensor of size 0."
    },
    {
        "X": "If what is specified, the number of bins is at least minlength and if input is empty, the result is a tensor",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "minlength"
    },
    {
        "X": "Input (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "1-d int tensor weights"
    },
    {
        "X": "What should the weight for each value in the input tensor be of?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "same size"
    },
    {
        "X": "Minlength (int) \u2013 what?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "optional, minimum number of bins"
    },
    {
        "X": "Minlength (int) \u2013 optional, minimum number of bins. Should be what?",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "Y": "non-negative"
    },
    {
        "X": "What does torch.tensor() construct with data?",
        "Z": "Constructs a tensor with data. Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "Y": "a tensor"
    },
    {
        "X": "What type of data does torch.as_tensor() avoid a copy of?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "Y": "NumPy ndarray"
    },
    {
        "X": "What does torch.tensor() construct when data is a tensor x?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "Y": "a leaf variable"
    },
    {
        "X": "What do you use if you have a Tensor data and want to avoid a copy?",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "Y": "torch.Tensor.requires_grad_() or torch.Tensor.detach()"
    },
    {
        "X": "What is defined as the elements on and above the diagonal?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "The upper triangular part"
    },
    {
        "X": "What controls which diagonal to consider?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "offset"
    },
    {
        "X": "What value controls which diagonal to consider?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "offset = 0,"
    },
    {
        "X": "What excludes just as many diagonals above the main diagonal?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "A positive value"
    },
    {
        "X": "What is the set of indices (i,i)lbrace (i, i) rbrace",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "main diagonal"
    },
    {
        "X": "What does row (int) mean?",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "Y": "row (int) \u2013 number of rows in the 2-D matrix"
    },
    {
        "X": "Returns a 2-D tensor with ones on the diagonal and what else?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "zeros"
    },
    {
        "X": "What is the default number of columns in a 2-D tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "n out"
    },
    {
        "X": "What is the global default of a 2-D tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "Default"
    },
    {
        "X": "What is the default setting of a 2-D tensor?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "torch.strided"
    },
    {
        "X": "What is the default value for autograd?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "False"
    },
    {
        "X": "A 2-D tensor with ones on the diagonal and what else?",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "Y": "zeros"
    },
    {
        "X": "Computes the Kaiser window with what?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "Y": "window length"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L, B, periodic=True)[:-1])?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "torch.kaiser_window(L, B, periodic=True)"
    },
    {
        "X": "What is the name of the window suitable for use in spectral analysis?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "periodic"
    },
    {
        "X": "What type of window is returned if False?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "Y": "symmetric"
    },
    {
        "X": "Default: if None, uses what?",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "Y": "global default"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1])?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "torch.kaiser_window(L, B, periodic=True)"
    },
    {
        "X": "What is the global default of the returned tensor?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "Default"
    },
    {
        "X": "What is the default layout of a returned window tensor?",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "torch.strided"
    },
    {
        "X": "What device will be the CPU for CPU tensor types?",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "Y": "current CUDA device"
    },
    {
        "X": "If autograd should record operations on the returned tensor, what is the default?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "False"
    },
    {
        "X": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes the eigenvalue decom",
        "Z": "Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "Y": "eigenvalues"
    },
    {
        "X": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.",
        "Z": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "Y": "eigenvalues of a square matrix"
    },
    {
        "X": "Computes the singular value decomposition (SVD) of a matrix. Computes the solution of a square system of linear equation",
        "Z": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "Y": "singular values of a matrix"
    },
    {
        "X": "Computes the inverse of a square matrix if it exists. Computes the eigenvalues of a square matrix.",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   ",
        "Y": "eigenvalue decomposition"
    },
    {
        "X": "What does Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix?",
        "Z": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "Y": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "What is the singular value decomposition of a matrix?",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "Y": "Computes the singular values of a matrix"
    },
    {
        "X": "Computes the what of a square matrix for an integer n?",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "n-th power"
    },
    {
        "X": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes the pseudoinverse (Moore-",
        "Z": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "Y": "inverse"
    },
    {
        "X": "Computes the multiplicative inverse of what?",
        "Z": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "Y": "torch.tensordot()"
    },
    {
        "X": "Computes the solution to the system torch.tensordot(A, X) = B.",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "Y": "X"
    },
    {
        "X": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix?",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "Y": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix"
    },
    {
        "X": "What function replaces torch.solve()?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.linalg.solve()"
    },
    {
        "X": "What function returns the LU factorization of the input?",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "Y": "torch.lu()"
    },
    {
        "X": "What is the name of a device. Gets the properties of a device?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "Y": "Gets the cuda capability of a device"
    },
    {
        "X": "What does Gets the properties of a device?",
        "Z": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   ",
        "Y": "Gets the properties of a device"
    },
    {
        "X": "What does Context-manager that changes the selected device return?",
        "Z": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "Y": "the number of GPUs available"
    },
    {
        "X": "Returns a bool indicating if CUDA is currently available.",
        "Z": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "Checks if what is possible. Returns cublasHandle_t pointer to current cuBLAS handle. Returns the index",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "Y": "peer access between two devices"
    },
    {
        "X": "Gets the cuda capability of a device. Gets the properties of a device.",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "Y": "name"
    },
    {
        "X": "What indicating if CUDA is currently available?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "a bool"
    },
    {
        "X": "Returns a bool indicating if PyTorch\u2019s CUDA state has been initialized.",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "Sets the current device. This is a wrapper API to set the stream.",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "Y": "current stream"
    },
    {
        "X": "Sets the current device. This is a wrapper API to what?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "Y": "set the stream"
    },
    {
        "X": "What is the wrapper around that selects a given stream?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "StreamContext"
    },
    {
        "X": "What does Returns a bool indicating if PyTorch\u2019s CUDA state has been initialized?",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "What is the cuda capability of a device?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Gets the cuda capability of a device"
    },
    {
        "X": "What happens when a CUDA device is selected?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "Y": "Waits for all kernels in all streams on a CUDA device to complete"
    },
    {
        "X": "What is the random number generator state of the specified GPU?",
        "Z": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "Y": "ByteTensor"
    },
    {
        "X": "What does Return?",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "Y": "the number of GPUs available"
    },
    {
        "X": "What is returned when a bool indicating if CUDA is currently available?",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "What does Set the random number generator state of the specified GPU do?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "What does Returns a bool indicating if CUDA is currently available?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "What happens when all kernels in all streams on a CUDA device complete?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "Waits for all kernels in all streams on a CUDA device to complete"
    },
    {
        "X": "What does Sets the random number generator state of the specified GPU do?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "Sets what of all devices?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   ",
        "Y": "random number generator state"
    },
    {
        "X": "What does the bool indicating if CUDA is currently available return?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "What does Sets the seed for generating random numbers for the current GPU do?",
        "Z": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   ",
        "Y": "Sets the random number generator state of all devices"
    },
    {
        "X": "What does Sets the seed for generating random numbers for the current GPU?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "Sets the seed for generating random numbers for the current GPU"
    },
    {
        "X": "What does Sets the seed for generating random numbers on all GPUs?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Sets the seed for generating random numbers on all GPUs"
    },
    {
        "X": "What does it do when a ByteTensor returns a list of ByteTensors representing the random number",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "Sets the seed for what to a random number on all GPUs?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "generating random numbers"
    },
    {
        "X": "What does Sets the seed for generating random numbers on all GPUs return?",
        "Z": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "Y": "current random seed of the current GPU"
    },
    {
        "X": "Returns the current GPU memory occupied by what in bytes for a given device?",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "tensors"
    },
    {
        "X": "Returns the maximum GPU memory managed by the caching allocator for a given device.",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "current GPU memory managed by the caching allocator in bytes"
    },
    {
        "X": "What does the caching allocator do for a process?",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "Y": "Set memory fraction"
    },
    {
        "X": "Returns the starting point in tracking maximum GPU memory occupied by tensors for a given device.",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors"
    },
    {
        "X": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.",
        "Z": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "Y": "current GPU memory managed by the caching allocator in bytes"
    },
    {
        "X": "Returns the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Resets the starting point in tracking maximum GPU memory occupied by tensors"
    },
    {
        "X": "What does Returns the current GPU memory managed by the caching allocator in bytes for a given device?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "the maximum GPU memory managed by the caching allocator in bytes for a given device"
    },
    {
        "X": "What does memory_reserved() do?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "Set memory fraction for a process"
    },
    {
        "X": "Resets what stats tracked by the CUDA memory allocator?",
        "Z": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "\u201cpeak\u201d stats"
    },
    {
        "X": "What is the name of each function that may change in future PyTorch releases?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "documentation"
    },
    {
        "X": "What does Computes the error function of input do?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "What does Computes the exponential of the elements minus 1 of input?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "Computes the exponential of the elements minus 1 of input"
    },
    {
        "X": "What is an example of a complementary error function?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "What is the complementary error function defined as?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is the function that provides greater precision than exp(x) - 1 for small values of x?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "Computes the exponential of the elements minus 1 of input"
    },
    {
        "X": "What does Computes the exponential of the elements minus 1 of input provide?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "Y": "greater precision"
    },
    {
        "X": "What is an example of a function that provides greater precision than exp(x) - 1 for small values of x?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "Computes the exponential of the elements minus 1 of input"
    },
    {
        "X": "What is the exponentially scaled zeroth order modified Bessel function?",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "Y": "the first kind"
    },
    {
        "X": "What value does addcdiv multiply the result of the element-wise division of tensor1 by tensor2?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "scalar"
    },
    {
        "X": "What is no longer supported with addcdiv?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "Warning Integer division"
    },
    {
        "X": "For what type of inputs can addcdiv be implemented as (input + value * tensor1 / tensor2)",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "float inputs"
    },
    {
        "X": "What is just the latter implementation of addcdiv?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "future addcdiv behavior"
    },
    {
        "X": "What is the tensor to be added tensor1 (Tensor)?",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "Y": "input"
    },
    {
        "X": "What does the torch._C.Future expose to add callback functions and set results?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "APIs"
    },
    {
        "X": "What does the torch._C.Future encapsulate?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. ",
        "Y": "asynchronous execution of a callable"
    },
    {
        "X": "What does the torch._C.Future expose?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. ",
        "Y": "APIs"
    },
    {
        "X": "What will be run when the Future is completed?",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "Append the given callback function to this Future"
    },
    {
        "X": "What API does add_done_callback use?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. ",
        "Y": "callback registration"
    },
    {
        "X": "Append the given callback function to what?",
        "Z": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "this Future"
    },
    {
        "X": "What version of GPU support is subject to changes?",
        "Z": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "beta"
    },
    {
        "X": "What is the name of the callback function that will be run when the Future is completed?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "Append"
    },
    {
        "X": "If this Future is already completed, the given callback will be run what?",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "inline"
    },
    {
        "X": "What is another name for callback?",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Future"
    },
    {
        "X": "What is the user responsible for handling if a callback later completes additional futures?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "completion/waiting"
    },
    {
        "X": "What happens if a Future is done?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Return True"
    },
    {
        "X": "What happens if this Future is done?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Return True"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, what will return True even if the asynchronous kernels that are popul",
        "Z": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "Future.done()"
    },
    {
        "X": "What does a Future not do?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "a Future cannot be marked completed twice"
    },
    {
        "X": "If the result is already usable, what can be performed?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "synchronizations"
    },
    {
        "X": "What happens when the result is set for this Future?",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "mark this Future as completed and trigger all attached callbacks"
    },
    {
        "X": "If the result contains what that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those ",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "tensors"
    },
    {
        "X": "What is safe to call this method immediately after launching kernels?",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "change streams in between"
    },
    {
        "X": "This method will record events on what?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "all the relevant current streams"
    },
    {
        "X": "Set the result for this Future, which will do what?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "mark this Future as completed and trigger all attached callbacks"
    },
    {
        "X": "Is a Future able to be completed twice?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "a Future cannot be marked completed twice"
    },
    {
        "X": "What are the streams on which asynchronous kernels were enqueued set as when this method is called?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "current ones"
    },
    {
        "X": "When is it safe to call this method?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "immediately after launching those kernels"
    },
    {
        "X": "What will this method record events on?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "all the relevant current streams"
    },
    {
        "X": "What is a way to enforce a certain order?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "chaining"
    },
    {
        "X": "If the Future is already completed, the given callback will be run immediately what?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "inline"
    },
    {
        "X": "What does the Future's value contain that reside on GPUs?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "tensors"
    },
    {
        "X": "What are the dedicated streams set as?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "current"
    },
    {
        "X": "When will any operation performed by the callback on tensors be scheduled on the device?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "after the kernels complete"
    },
    {
        "X": "What does the callback not do?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "switch streams"
    },
    {
        "X": "What non-blocking behavior is similar to the non-blocking behavior of?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "wait()"
    },
    {
        "X": "What holds the return value of the callback?",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "A new Future object"
    },
    {
        "X": "What does a callback do?",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "Obtain the value of an already-completed future"
    },
    {
        "X": "When should Obtain the value of an already-completed future be called?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "after a call to wait() has completed, or inside a callback function passed to then()"
    },
    {
        "X": "What could cause calling value() to fail?",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "this Future may not yet hold a value"
    },
    {
        "X": "What is a method that should only be called after a call to wait() has completed?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "Obtain the value of an already-completed future"
    },
    {
        "X": "What could cause value() to fail?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "this Future may not yet hold a value"
    },
    {
        "X": "What does this method do to obtain the value of an already-completed future?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "Obtain the value of an already-completed future"
    },
    {
        "X": "When should the Obtain the value of an already-completed future be called?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "after a call to wait() has completed"
    },
    {
        "X": "What could happen if the method is called after a call to wait() has completed or inside a callback function passed to then()?",
        "Z": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "Future may not yet hold a value"
    },
    {
        "X": "What could happen if a call to value() fails?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "Future may not yet hold a value"
    },
    {
        "X": "If the value contains what that reside on GPUs, this method will not perform any additional synchronization?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "Y": "tensors"
    },
    {
        "X": "What does the value() method do?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "Block until the value of this Future is ready"
    },
    {
        "X": "How long should the value() method be blocked?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "until the value of this Future is ready"
    },
    {
        "X": "What could cause the call to value() to fail?",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "this Future may not yet hold a value"
    },
    {
        "X": "How long should the value() method be called?",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "until the value of this Future is ready"
    },
    {
        "X": "What method will throw an error if the function (callback or RPC) creating the value has thrown an error?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "this wait method"
    },
    {
        "X": "When does Block occur?",
        "Z": "Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. ",
        "Y": "until the value of this Future is ready"
    },
    {
        "X": "If the value contains what that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "tensors"
    },
    {
        "X": "What will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the as",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "wait()"
    },
    {
        "X": "What is required when accessing and using the values, as long as one doesn't change streams?",
        "Z": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "No further synchronization"
    },
    {
        "X": "If the function (callback or RPC) creating the value has thrown an error, what method will also throw an error?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "this wait method"
    },
    {
        "X": "What is performed if the value contains tensors that reside on GPUs?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "synchronization"
    },
    {
        "X": "What is required when accessing and using the values, as long as one doesn\u2019t change streams?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "No further synchronization"
    },
    {
        "X": "What happens when all of the sub-futures are completed?",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "Collects the provided Future objects into a single combined Future"
    },
    {
        "X": "What is required when accessing and using the values?",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "Y": "No further synchronization is required"
    },
    {
        "X": "What does the wait method throw an error on?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "The value held by this Future"
    },
    {
        "X": "What does the wait method do?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "Waits for all provided futures to be complete"
    },
    {
        "X": "If any of the futures encounter an error, the method will report the error not waiting for other futures to complete?",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "Y": "exit early"
    },
    {
        "X": "What type of convolution is applied to an input signal composed of several input planes?",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "1D convolution"
    },
    {
        "X": "Combines an array of sliding local blocks into what?",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "large containing tensor"
    },
    {
        "X": "Applies what pooling over an input signal composed of several input planes?",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "Y": "1D average"
    },
    {
        "X": "Applies what operation in kTkHkWkT times kWkTkHkW regions by step",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "Y": "3D average-pooling"
    },
    {
        "X": "Applies a pooling over an input signal composed of several input planes. Applies a pooling over an input signal composed of several input",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "3D max"
    },
    {
        "X": "What is the result of a 2D max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What is the name of the pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "2D adaptive max"
    },
    {
        "X": "Applies what type of convolution over an input image composed of several input planes?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "3D"
    },
    {
        "X": "What is another name for a 2D transposed convolution operator over an input image composed of several input planes?",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "Y": "deconvolution"
    },
    {
        "X": "Applies what operation in kHkWkH times kWkHkW regions by step size sHs",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "Y": "2D average-pooling"
    },
    {
        "X": "Applies what over an input image composed of several input planes?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "3D convolution"
    },
    {
        "X": "What operation in kTkHkWkT times kWkTkHkW regions by step size ",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "3D average-pooling"
    },
    {
        "X": "What does a 3D max pooling over an input signal consist of?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "several input planes"
    },
    {
        "X": "What is the result of a 3D max pooling over an input signal composed of several input planes?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What is the name of the pooling over an input signal?",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "3D adaptive max"
    },
    {
        "X": "What operation in kHkWkH times kWkHkW regions by step size sHsWs",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "Y": "2D average-pooling"
    },
    {
        "X": "What is the result of a partial inverse of MaxPool1d?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What type of pooling over an input signal composed of several input planes?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "1D power-average"
    },
    {
        "X": "What does a 3D transposed convolution operator combine an array of sliding local blocks into?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "Y": "large containing tensor"
    },
    {
        "X": "What does a 2D max pooling over an input signal consist of?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "several input planes"
    },
    {
        "X": "2D power-average pooling over an input signal composed of what?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "What is a 2D fractional max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "Y": "2D fractional max pooling over"
    },
    {
        "X": "What does an array of sliding local blocks combine into?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "a large containing tensor"
    },
    {
        "X": "What is the input signal composed of?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 1D adaptive max pooling over an input signal consist of?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 2D adaptive max pooling over an input signal consist of?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "several input planes"
    },
    {
        "X": "What type of fractional max pooling over an input signal composed of several input planes?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "3D"
    },
    {
        "X": "What does each element of the input Tensor have?",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "Y": "Thresholds"
    },
    {
        "X": "What is each element of the input Tensor?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "Thresholds"
    },
    {
        "X": "What is the In-place version of threshold()?",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "Y": "In-place version of threshold()"
    },
    {
        "X": "What type of linear unit function does threshold() apply?",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "Y": "rectified linear unit function element-wise"
    },
    {
        "X": "What is the rectified linear unit function element-wise?",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "Y": "In-place"
    },
    {
        "X": "Computes a partial inverse of MaxPool2d. Computes a partial inverse of MaxPool",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "Computes a partial inverse of MaxPool2d"
    },
    {
        "X": "What is element-wise?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "rectified linear unit function"
    },
    {
        "X": "What function is element-wise?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "HardTanh"
    },
    {
        "X": "Applies what element-wise function?",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "Y": "hardswish function"
    },
    {
        "X": "What type of max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "3D"
    },
    {
        "X": "What is the result of the 2D max pooling over an input signal composed of several input planes?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "2D adaptive max pooling over an input signal composed of what?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 3D adaptive max pooling over an input signal consist of?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 1D adaptive average pooling over an input signal consist of?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "2D adaptive average pooling over an input signal composed of what?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "Which function is element-wise?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "HardTanh"
    },
    {
        "X": "What function element-wise. In-place version of hardtanh(). Applies the hardswish function, element-wise?",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "HardTanh"
    },
    {
        "X": "3D adaptive max pooling over an input signal composed of what?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "3D adaptive average pooling over an input signal composed of what?",
        "Z": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "Y": "several input planes"
    },
    {
        "X": "What does a 3D pooling over an input signal composed of several input planes do?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "Computes a partial inverse of MaxPool1d"
    },
    {
        "X": "What does a partial inverse of MaxPool1d. Computes a partial inverse of MaxPool2",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "Computes a partial inverse of MaxPool3d"
    },
    {
        "X": "What is applied to an input signal composed of several input planes?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "1D power-average pooling"
    },
    {
        "X": "Applies 2D fractional max pooling over an input signal composed of what?",
        "Z": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "Y": "several input planes"
    },
    {
        "X": "In-place version of what?",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.   ",
        "Y": "leaky_relu()"
    },
    {
        "X": "What is a function that measures the Binary Cross Entropy between target and output logits?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "Poisson negative log likelihood loss"
    },
    {
        "X": "What is the name of the function that measures the Poisson negative log likelihood loss?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "CosineEmbeddingLoss"
    },
    {
        "X": "What function uses a squared term if the absolute element-wise error falls below delta and an L1 term otherwise?",
        "Z": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "Y": "SoftMarginLoss"
    },
    {
        "X": "What is the result of two tensors?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "Matrix product of two tensors"
    },
    {
        "X": "If both tensors are what dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional,",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "1-dimensional"
    },
    {
        "X": "If both arguments are 2-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, what product is returned?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "matrix-matrix"
    },
    {
        "X": "If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of what?",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "matrix multiply"
    },
    {
        "X": "After the matrix multiply, what happens to the prepended dimension?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "the prepended dimension is removed"
    },
    {
        "X": "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), what is returned?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "batched matrix multiply"
    },
    {
        "X": "If the first argument is what dimension, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "1-dimensional"
    },
    {
        "X": "If the second argument is what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "1-dimensional"
    },
    {
        "X": "The non-matrix dimensions are what?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "broadcasted"
    },
    {
        "X": "What only looks at the batch dimensions when determining if the inputs are broadcastable?",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "Y": "the broadcasting logic"
    },
    {
        "X": "What defines 10 tensor types with CPU and GPU variants?",
        "Z": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: ",
        "Y": "Torch"
    },
    {
        "X": "What is the name of the data type dtype CPU tensor GPU tensor?",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "Y": "Data type dtype CPU tensor GPU tensor"
    },
    {
        "X": "What is a dtype CPU tensor or a GPU tensor?",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "Y": "dtype CPU tensor GPU tensor"
    },
    {
        "X": "What type of tensor is 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "Y": "CPU tensor GPU tensor"
    },
    {
        "X": "How many bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.Hal",
        "Z": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "Y": "16"
    },
    {
        "X": "Useful when what is important at the expense of range?",
        "Z": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "precision"
    },
    {
        "X": "What is another term for binary16?",
        "Z": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "Sometimes referred"
    },
    {
        "X": "What is torch.float64 or torch.double torch?",
        "Z": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "64-bit floating point"
    },
    {
        "X": "What is the term used to describe a device that uses 1 sign, 5 exponent, and 10 significand bits?",
        "Z": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "What is another term for the use of 1 sign, 8 exponent, and 7 significand bits?",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of",
        "Z": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "Useful"
    },
    {
        "X": "How many bits does a doubletensor have?",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "Y": "16"
    },
    {
        "X": "What is important at the expense of range?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "precision"
    },
    {
        "X": "Useful when precision is important at the expense of range.",
        "Z": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "Y": "range is important"
    },
    {
        "X": "What is another name for a sign, 8 exponent, and 7 significand bits?",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "What is torch.cuda.HalfTensor?",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "16-bit floating point 2"
    },
    {
        "X": "What is the term for a sign, 8 exponent, and 7 significand bits?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "Brain Floating Point"
    },
    {
        "X": "What is Brain Floating Point stored as?",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "8-bit signed integer"
    },
    {
        "X": "Currently it\u2019s only supported in what?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "EmbeddingBag operator"
    },
    {
        "X": "What is torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "16-bit floating point 2"
    },
    {
        "X": "What is the same number of exponent bits as float32 quantized 4-bit integer?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "8-bit signed integer"
    },
    {
        "X": "What is the name of the torch.Tensor?",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What is an alias?",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What is a Brain Floating Point stored as?",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "Y": "8-bit signed integer"
    },
    {
        "X": "Currently it\u2019s only supported in what operator?",
        "Z": "torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "EmbeddingBag operator"
    },
    {
        "X": "What is torch.Tensor an alias for?",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "default tensor type (torch.FloatTensor)"
    },
    {
        "X": "What is the term for ByteTensor?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "binary16"
    },
    {
        "X": "What is stored as a quantized 4-bit integer?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "8-bit signed integer"
    },
    {
        "X": "When precision is important at the expense of range?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "precision is important at the expense of range"
    },
    {
        "X": "A tensor can be constructed from a what?",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "Y": "Python list or sequence using the torch"
    },
    {
        "X": "What can be constructed from a Python list or sequence using the torch.tensor type?",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "Y": "A tensor"
    },
    {
        "X": "What is torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.By",
        "Z": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "128-bit complex"
    },
    {
        "X": "What can be constructed from a Python list or sequence using the torch.tensor() constructor?",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "A tensor"
    },
    {
        "X": "What is the name of the tensor that uses 1 sign, 5 exponent, and 10 significand bits?",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "binary16"
    },
    {
        "X": "What is unsigned?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "8-bit integer"
    },
    {
        "X": "What type of Tensor?",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "Tens"
    },
    {
        "X": "What is another name for ByteTensor?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "binary16"
    },
    {
        "X": "What does a Tensor data need to change?",
        "Z": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "requires_grad flag"
    },
    {
        "X": "If you want to change the required_grad flag of a Tensor, use what?",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "requires_grad_() or detach()"
    },
    {
        "X": "What type of integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "8-bit"
    },
    {
        "X": "What can a tensor be constructed from?",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "Y": "Python list or sequence using the torch"
    },
    {
        "X": "What are two ways to avoid a copy of a Tensor data?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "requires_grad_() or detach()"
    },
    {
        "X": "If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach()",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "numpy"
    },
    {
        "X": "In what operator is Brain Floating Point only supported?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "EmbeddingBag operator"
    },
    {
        "X": "If you have a Tensor data and want to avoid a copy, use requires_grad_() or detach() to avoid ",
        "Z": "torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "numpy array"
    },
    {
        "X": "What is the signature of torch.cuda.CharTensor?",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "16-bit integer"
    },
    {
        "X": "What type of integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "16-bit"
    },
    {
        "X": "What is a tensor of?",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "Y": "specific data type"
    },
    {
        "X": "A tensor of specific data type can be constructed by passing what to a constructor?",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "Y": "torch.dtype and/or a torch.device"
    },
    {
        "X": "To create a tensor with specific size, use what?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "Y": "torch"
    },
    {
        "X": "What does torch. * tensor do?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "Y": "creation ops"
    },
    {
        "X": "To create a tensor with the same size (and similar types) as another tensor, use what?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "torch"
    },
    {
        "X": "What does torch. *_like tensor do?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "Y": "creation ops"
    },
    {
        "X": "Is this Tensor with what?",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin ",
        "Y": "its dimensions reversed"
    },
    {
        "X": "Is the Tensor.is_cuda true?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "True if the Tensor is stored on the GPU"
    },
    {
        "X": "What is True if the Tensor is quantized?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "Y": "Tensor.is_quantized"
    },
    {
        "X": "Is Tensor.is_meta True if the Tensor is a meta tensor?",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin ",
        "Y": "True if the Tensor is a meta tensor"
    },
    {
        "X": "Tensor.grad This attribute is what by default?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "None"
    },
    {
        "X": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing what?",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "Tensor.imag Returns a new tensor containing what?",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "imaginary values of the self tensor"
    },
    {
        "X": "What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.abs_ In-place version of",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What is another name for creation ops?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "Creation Ops"
    },
    {
        "X": "What does Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing?",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "Is the Tensor.is_cuda True?",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "Y": "True if the Tensor is stored on the GPU"
    },
    {
        "X": "What does torch.abs() Tensor.abs stand for?",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin ",
        "Y": "Tensor.abs"
    },
    {
        "X": "Tensor.new_empty Returns a Tensor of size size filled with what?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "uninitialized data"
    },
    {
        "X": "Tensor.is_cuda Is what?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "Y": "True if the Tensor is stored on the GPU"
    },
    {
        "X": "Tensor.is_meta Is True if the Tensor is a what?",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "Y": "meta tensor"
    },
    {
        "X": "Where is the Tensor.device?",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin ",
        "Y": "torch.device"
    },
    {
        "X": "What does a scalar or tensor do to self tensor?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Add a scalar or tensor to self tensor"
    },
    {
        "X": "What is another name for add() Tensor?",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "addbmm"
    },
    {
        "X": "What does new_empty return a Tensor of size size filled with?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "uninitialized data"
    },
    {
        "X": "What is the name of the tensor that returns a new Tensor?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What does addbmm stand for?",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin ",
        "Y": "addbmm"
    },
    {
        "X": "Tensor.imag Returns a new tensor containing what of the self tensor?",
        "Z": "Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() ",
        "Y": "imaginary values"
    },
    {
        "X": "What is the name of the feature that adds a scalar or tensor to self tensor?",
        "Z": "Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() ",
        "Y": "addbmm"
    },
    {
        "X": "What is another name for addbmm() Tensor?",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 ",
        "Y": "addbmm"
    },
    {
        "X": "What type of data does a Tensor.new_empty return a Tensor of size size filled with?",
        "Z": "Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() ",
        "Y": "uninitialized data"
    },
    {
        "X": "What is another name for Tensor.abs?",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "Y": "torch.abs"
    },
    {
        "X": "Is True if the Tensor is a meta tensor, False otherwise?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "Y": "meta"
    },
    {
        "X": "Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing what?",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "Tensor.is_meta Is what?",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 ",
        "Y": "True if the Tensor is a meta tensor"
    },
    {
        "X": "What does Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "What does the tensor.ndim Alias for dim() Tensor.real Returns a new tensor ",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "Y": "real values of the self tensor"
    },
    {
        "X": "What attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self?",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What is the name of the attribute that becomes a Tensor the first time a call to backward() computes gradients for self?",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "Y": "addbmm"
    },
    {
        "X": "Returns a new tensor containing imaginary values of the self tensor. What Returns a new tensor",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "Tensor.imag"
    },
    {
        "X": "What is added to the tensor?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "a scalar or tensor to self tensor"
    },
    {
        "X": "What is the In-place version of add()?",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "Tensor.addbmm"
    },
    {
        "X": "What is another name for Tensor.argmax?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "Tensor.argmax"
    },
    {
        "X": "What is added to the self tensor?",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "a scalar or tensor to self tensor"
    },
    {
        "X": "What is the name of the tensor that returns a scalar or tensor to self tensor?",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "Y": "Tensor.addbmm"
    },
    {
        "X": "Which tensor returns a new tensor containing imaginary values of the self tensor?",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "Y": "Tensor"
    },
    {
        "X": "What is the Tensor.add_ In-place version of?",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "add() Tensor.addbmm"
    },
    {
        "X": "What is a Tensor.add_ In-place version of?",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). ",
        "Y": "add() Tensor.addbmm"
    },
    {
        "X": "What _ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut",
        "Z": "Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "Tensor.abs"
    },
    {
        "X": "What does add add?",
        "Z": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "a scalar or tensor to self tensor"
    },
    {
        "X": "What is added to the Tensor.absolute Alias for abs() Tensor.absolute Alias for ab",
        "Z": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "a scalar or tensor to self tensor"
    },
    {
        "X": "Tensor.add_ In-place version of what add() Tensor.addbmm?",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "add() Tensor.addbmm"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "Y": "Tensor.absolute"
    },
    {
        "X": "What type of elements can be stored in a sparse array?",
        "Z": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "Y": "non-zero"
    },
    {
        "X": "What is the consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "memory"
    },
    {
        "X": "How much memory saving does a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers have?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "Y": "200 fold"
    },
    {
        "X": "What is the function that provides the size of a sparse COO tensor?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "torch.sparse_coo_tensor()"
    },
    {
        "X": "What is the value of entry 4 in a sparse tensor?",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "Y": "1, 0"
    },
    {
        "X": "What is the fill value of a sparse tensor?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "zero by default"
    },
    {
        "X": "What is the value of entry 4 in the sparse tensor?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "Y": "1, 0"
    },
    {
        "X": "What are the numbers of sparse and dense dimensions, respectively?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "M and K"
    },
    {
        "X": "What type of tensor would we want to create?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "Y": "(2 + 1)-dimensional"
    },
    {
        "X": "Suppose we want to define a sparse tensor with the entry 4 at location?",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "1, 0"
    },
    {
        "X": "Suppose we want to create a what -dimensional tensor with the entry [3, 4] at location (0, 2), entry [",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "Y": "(2 + 1)"
    },
    {
        "X": "What does M stand for?",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "s.sparse_dim()"
    },
    {
        "X": "What is the location of the entry [5, 6] in a 2 + 1)-dimensional tensor?",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "(1, 0"
    },
    {
        "X": "What type of sparse COO tensor can be constructed by specifying its size only?",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "Y": "empty"
    },
    {
        "X": "Where is the entry [5, 6] in a 2 + 1)-dimensional tensor?",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: ",
        "Y": "(1, 0"
    },
    {
        "X": "Pytorch implements an extension of sparse tensors with scalar values to sparse tensor",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "Y": "hybrid tensors"
    },
    {
        "X": "What is a dimensional tensor?",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "Y": "(2 + 1)"
    },
    {
        "X": "What is the element type of the indices of specified elements?",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "torch.int64"
    },
    {
        "X": "What is a sparse COO tensor?",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "Y": "torch"
    },
    {
        "X": "What are the corresponding (tensor) values collected in?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "arbitrary integer or floating point number element type"
    },
    {
        "X": "What sparse COO tensor format permits uncoalesced sparse tensors?",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "Y": "PyTorch"
    },
    {
        "X": "What leads to an 1-D uncoalesced tensor?",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "Y": "multiple values"
    },
    {
        "X": "What is the name of the tensor we want to create?",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "Y": "(2 + 1)"
    },
    {
        "X": "What always follow sparse dimensions?",
        "Z": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "Y": "Dense dimensions"
    },
    {
        "X": "What format permits uncoalesced sparse tensors?",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "Y": "PyTorch sparse COO tensor"
    },
    {
        "X": "What can lead to an 1-D uncoalesced tensor?",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "Y": "multiple values"
    },
    {
        "X": "What is the location of a 2 + 1-dimensional tensor?",
        "Z": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "Y": "(1, 0"
    },
    {
        "X": "What is coalesced or not?",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "sparse tensor"
    },
    {
        "X": "Values are stored as what?",
        "Z": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "Y": "strided tensors"
    },
    {
        "X": "What type of uncoalesced tensor is created when multiple values are specified for the same index 1?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "Y": "1-D"
    },
    {
        "X": "What is the lexicographical ordering of indices?",
        "Z": "s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "Y": "lexicographical ordering of indices"
    },
    {
        "X": "What type of ordering of indices can be advantageous for implementing algorithms that involve many element selection operations?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "lexicographical"
    },
    {
        "X": "Most operations will work identically given a what?",
        "Z": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "coalesced or uncoalesced sparse tensor"
    },
    {
        "X": "The lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as what?",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "slicing or matrix products"
    },
    {
        "X": "What is a sparse COO tensor a part of?",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What instance of a sparse COO tensor is a sparse COO tensor?",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What type of data can a sparse COO tensor acquire?",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: ",
        "Y": "COO format data"
    },
    {
        "X": "What is the output of torch.Tensor.coalesce() method?",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "a sparse tensor"
    },
    {
        "X": "What can one acquire when the tensor instance is coalesced?",
        "Z": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "COO format data"
    },
    {
        "X": "What of specified tensor elements are unique?",
        "Z": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "indices"
    },
    {
        "X": "What instance does a sparse COO tensor belong to?",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "torch.Tensor"
    },
    {
        "X": "What format data can one acquire only when the tensor instance is coalesced?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "COO"
    },
    {
        "X": "The indices are sorted in what order?",
        "Z": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "lexicographical order"
    },
    {
        "X": "Why should sparse tensors be coalesced?",
        "Z": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "to prevent them from growing too large"
    },
    {
        "X": "How will most operations work with a coalesced or uncoalesced sparse tensor?",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "Y": "most operations will work identically"
    },
    {
        "X": "What is the usage of adjective \"non-zero\"?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "not strict"
    },
    {
        "X": "What does PyTorch use for those array elements that are actually stored?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "specified elements"
    },
    {
        "X": "When can using a sparse storage format for storing sparse arrays be advantageous?",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "when the size and sparsity levels of arrays are high"
    },
    {
        "X": "What beta is the PyTorch API of sparse tensors in?",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "Y": "beta"
    },
    {
        "X": "What can one acquire only when the tensor instance is coalesced?",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "COO format data"
    },
    {
        "X": "What could be implemented by multiplying all the uncoalesced values with the scalar?",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "scalar multiplication"
    },
    {
        "X": "What type of operation cannot be implemented by a square root?",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "Y": "nonlinear operation"
    },
    {
        "X": "What type of support does not exist as of now?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "CUDA"
    },
    {
        "X": "How many 1-D tensors does a CSR sparse tensor consist of?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "three"
    },
    {
        "X": "What is subtracted by the number before it denotes the number of elements in a given row?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "Each successive number in the tensor"
    },
    {
        "X": "How can Sparse CSR matrices be directly constructed?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "torch._sparse_csr_tensor() method"
    },
    {
        "X": "The user must supply the row and column indices and values tensors separately or separately?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "separately"
    },
    {
        "X": "What is the simplest way of constructing a sparse CSR matrices?",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "Y": "simplest way of constructing a sparse CSR"
    },
    {
        "X": "What is True if the Tensor uses sparse storage layout?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.is_sparse"
    },
    {
        "X": "Tensor.dense_dim Return the number of sparse dimensions in a sparse tensor self. Tens",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "sparse"
    },
    {
        "X": "What is returned by the indices of the sparse tensor mask?",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "a new sparse tensor with values from a strided tensor self"
    },
    {
        "X": "What Return the indices tensor of a sparse COO tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.indices"
    },
    {
        "X": "What Return the values tensor of a sparse COO tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.values"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced Returns what?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "True if self is a sparse COO tensor that is coalesced"
    },
    {
        "X": "Tensor.is_sparse Is what?",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "True if the Tensor uses sparse storage layout"
    },
    {
        "X": "What removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.sparse_resize_and_clear"
    },
    {
        "X": "What Return the number of sparse dimensions in a sparse tensor self?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "Tensor.sparse_dim"
    },
    {
        "X": "Is True if self is a sparse COO tensor that is coalesced?",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "Y": "True if self is a sparse COO tensor that is coalesced"
    },
    {
        "X": "Tensor.sparse_mask Returns what with values from a strided tensor self filtered by the",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "a new sparse tensor"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what does Tensor.is_coales",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "True if self is a sparse COO tensor that is coalesced"
    },
    {
        "X": "What Returns the tensor containing the compressed row indices of the self tensor?",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "Y": "Tensor.col_indices"
    },
    {
        "X": "What is strongly prefered as in a future pytorch release, this function will only return complex tensors?",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "return_complex=True"
    },
    {
        "X": "The STFT computes what of short overlapping windows of the input?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "Y": "Fourier transform"
    },
    {
        "X": "The Fourier transform of short overlapping windows of the input gives what of the signal as they change over time?",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "Y": "frequency components"
    },
    {
        "X": "Ignoring what, this method computes the following expression: where mmm is the index of the sliding window, and omega",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "optional batch dimension"
    },
    {
        "X": "input must be either a what?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "1-D time sequence or a 2-D batch of time sequences"
    },
    {
        "X": "Window can be a what?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "1-D tensor"
    },
    {
        "X": "If what is true, input will be padded on both sides to length n_fft before being applied?",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "center is True"
    },
    {
        "X": "When was return_complex required to be given explicitly for real inputs?",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "1.8.0"
    },
    {
        "X": "What is the name of the function that will only return complex tensors?",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "return_complex=True"
    },
    {
        "X": "If win_length is None (default), it is treated as equal to n_fft. If center is True (default), input will",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "Y": "n_fft"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the n_fft is padded before being applied?",
        "Z": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "center is True"
    },
    {
        "X": "From what version did return_complex have to always be given explicitly for real inputs?",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "1.8.0"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the t is the t?",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "center is True"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the ttt-th frame is centered at time t",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "center is True"
    },
    {
        "X": "What begins at time thop_lengtht times texthop_length?",
        "Z": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "Y": "ttt-th frame"
    },
    {
        "X": "If win_length is None, window will be padded on both sides to length n_fft before being applied. If center is True",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "n_fft"
    },
    {
        "X": "For all available options, see what for all available options. Default is \"reflect\"?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "torch.nn.functional.pad()"
    },
    {
        "X": "If what is true (default for real input) only values for omega are used?",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "onesided is True"
    },
    {
        "X": "If win_length is None (default), window will be padded on both sides to length n_fft before being applied. If center",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "n_fft"
    },
    {
        "X": "What is the default setting for the padding method used when center is True?",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "Y": "\"reflect\""
    },
    {
        "X": "If onesided is what?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "Y": "True"
    },
    {
        "X": "If win_length is None (default), it is treated as equal to n_fft?",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "n_fft"
    },
    {
        "X": "If what is true, input will be padded on both sides so that the ttt-th frame is centered?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "center is True"
    },
    {
        "X": "What is the default setting for input when center is True?",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "\"reflect\""
    },
    {
        "X": "If True (default for real input), only values for omega are returned because the real-to-complex Fourier transform ",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "Y": "onesided"
    },
    {
        "X": "If what is True (default for real input), only values for omega are returned because the real-to-complex Fourier",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "onesided"
    },
    {
        "X": "If the input or window tensors are complex, what is not possible?",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "onesided output is not possible"
    },
    {
        "X": "What is win_length?",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "n_fft"
    },
    {
        "X": "If normalized is True, the function returns the normalized STFT results, i.e., normalized STFT results, i.",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "Y": "False"
    },
    {
        "X": "If center is True, only values for omega are returned because the real-to-complex Fourier transform satisfie",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "onesided"
    },
    {
        "X": "If normalized is True, the function returns what?",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "Y": "normalized STFT results"
    },
    {
        "X": "If return_complex is True, the return is a input.dim() + what?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "Y": "1 dimensional complex tensor"
    },
    {
        "X": "If return_complex is True, the output is what?",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "input.dim() + 2 dimensional real tensor"
    },
    {
        "X": "Returns either a complex tensor of size or what?",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "Y": "a complex tensor of size"
    },
    {
        "X": "What is the output if return_complex is True?",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "Y": "input.dim() + 2 dimensional real tensor"
    },
    {
        "X": "At what version did the function change signature?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "version 0.4.1"
    },
    {
        "X": "n_fft (int) \u2013 the input tensor n_fft (int) \u2013 size",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "Fourier transform"
    },
    {
        "X": "What is the default value for n_fft?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "None"
    },
    {
        "X": "What is the default?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "True for real input and window"
    },
    {
        "X": "What is the output if return_complex is False?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "input.dim() + 2 dimensional real tensor"
    },
    {
        "X": "What is the default value for return half of results to avoid redundancy for real inputs?",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "Y": "False onesided"
    },
    {
        "X": "What is the window length of the Kaiser window?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "window_length"
    },
    {
        "X": "What argument is intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "periodic"
    },
    {
        "X": "What is the default layout of the returned window tensor?",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "Y": "torch.strided"
    },
    {
        "X": "What does this library do?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Gets the cuda capability of a device"
    },
    {
        "X": "Gets the cuda capability of a device. Gets the name of a device. Gets what of a device?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "properties"
    },
    {
        "X": "What does Sets the current stream. This is a wrapper API to set the stream?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "current device"
    },
    {
        "X": "What does Sets the seed for generating random numbers on all GPUs do?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Sets the seed for generating random numbers for the current GPU"
    },
    {
        "X": "What does a bool indicating if CUDA is currently available return?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized"
    },
    {
        "X": "What happens when all kernels in all streams on a CUDA device are selected?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Waits for all kernels in all streams on a CUDA device to complete"
    },
    {
        "X": "What does Sets the seed for generating random numbers to a random number on all GPUs?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "Sets the seed for generating random numbers to a random number for the current GPU"
    },
    {
        "X": "What does comm.broadcast return?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "current random seed of the current GPU"
    },
    {
        "X": "What is the name of the command that returns the current random seed of the current GPU?",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "Y": "comm.broadcast"
    },
    {
        "X": "Who selects a given stream?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "Y": "Context-manager"
    },
    {
        "X": "Returns the currently selected Stream for a given device. Returns what for a given device?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "default Stream"
    },
    {
        "X": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device. Gets the capability",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "Y": "cuda"
    },
    {
        "X": "Gets the name of a device. Gets the properties of a device. Returns NVCC gencode flags this library was",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "Gets the name of a device"
    },
    {
        "X": "Gets the properties of a device. Gets the cuda capability of a device. Gets the name of a device. Get",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "properties"
    },
    {
        "X": "Force collects GPU memory after it has been released by CUDA IPC. Returns whether PyTorch\u2019s CUDA state has",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "Initialize PyTorch\u2019s CUDA state"
    },
    {
        "X": "Sets the current stream.This is a what?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "wrapper API to set the stream"
    },
    {
        "X": "What wrapper wraps around the Context-manager StreamContext that selects a given stream?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "Wrapper around the Context-manager StreamContext"
    },
    {
        "X": "What happens to all kernels in all streams on a CUDA device to complete?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "Waits for all kernels in all streams on a CUDA device to complete"
    },
    {
        "X": "Sets what of the specified GPU. Sets the random number generator state of all devices. Sets the seed for generating random numbers for the current",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "Y": "random number generator state"
    },
    {
        "X": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers on all GPUs. Returns the",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "Sets the seed for generating random numbers on all GPUs"
    },
    {
        "X": "Returns what this library was compiled for. Gets the cuda capability of a device. Gets the name of a device.",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "list CUDA architectures"
    },
    {
        "X": "Returns what flags this library was compiled with. Initialize PyTorch\u2019s CUDA state. Force collects GPU memory after",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "Waits for all kernels in all streams on a CUDA device to complete?",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "Y": "Waits for all kernels in all streams on a CUDA device to complete"
    },
    {
        "X": "Sets the random number generator state of the specified GPU. Sets the seed for generating random numbers for the current GPU. Sets the seed for",
        "Z": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers on all GPUs. Sets the",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "current random seed of the current GPU"
    },
    {
        "X": "What does comm.reduce_add Sums tensors from multiple GPUs?",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather ",
        "Y": "comm"
    },
    {
        "X": "Gets the cuda capability of a device. Gets what?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "name of a device"
    },
    {
        "X": "What Scatters tensors from multiple GPUs?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "comm.scatter"
    },
    {
        "X": "Returns the index of a what?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "currently selected device"
    },
    {
        "X": "Returns the index of a currently selected device. Returns what for a given device?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "currently selected Stream"
    },
    {
        "X": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device.",
        "Z": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "cuda"
    },
    {
        "X": "Gets the cuda capability of a device. Gets the what of a device?",
        "Z": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "name"
    },
    {
        "X": "Gets what of a device. Returns NVCC gencode flags this library was compiled with. Initialize PyTorch",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "properties"
    },
    {
        "X": "Sets the seed for generating random numbers on all GPUs. Sets the seed for what?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "generating random numbers"
    },
    {
        "X": "What Gathers tensors?",
        "Z": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "comm.gather"
    },
    {
        "X": "What does Gets the name of a device?",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "Y": "Gets the name of a device"
    },
    {
        "X": "Wrapper around a CUDA stream <sep>",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "Wrapper around a CUDA stream"
    },
    {
        "X": "Wrapper around what event?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "CUDA event"
    },
    {
        "X": "Releases all unoccupied cached memory currently held by CUDA IPC. Releases all unoccupied cached memory currently held by CUDA",
        "Z": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "Y": "CUDA event"
    },
    {
        "X": "What release releases all unoccupied cached memory currently held by CUDA?",
        "Z": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "Y": "Releases all unoccupied cached memory"
    },
    {
        "X": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers to a random number on all GPU",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "generating random numbers"
    },
    {
        "X": "What releases all unoccupied cached memory currently held by the caching allocator?",
        "Z": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "Y": "Releases all unoccupied cached memory"
    },
    {
        "X": "What does Sets the random number generator state of the specified GPU?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "Gets what of a device. Gets the name of a device. Gets the properties of a device. Returns NVCC",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "cuda capability"
    },
    {
        "X": "What does Force collects GPU memory after it has been released by CUDA IPC?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "Initialize PyTorch\u2019s CUDA state"
    },
    {
        "X": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "Wrapper around a CUDA stream"
    },
    {
        "X": "Returns a what?",
        "Z": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "Y": "human"
    },
    {
        "X": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator. Releases",
        "Z": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "Wrapper around a CUDA stream"
    },
    {
        "X": "What does it do?",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "Y": "Sets the random number generator state of the specified GPU"
    },
    {
        "X": "Returns what of the running processes and their GPU memory use for a given device?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "a human-readable printout"
    },
    {
        "X": "Gets what?",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "Y": "the properties of a device"
    },
    {
        "X": "Returns what flags this library was compiled with. Initialize PyTorch\u2019s CUDA state.",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "NVCC gencode flags"
    },
    {
        "X": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator so that those",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "Wrapper around a CUDA stream"
    },
    {
        "X": "Returns a dictionary of what?",
        "Z": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "Y": "CUDA memory allocator statistics"
    },
    {
        "X": "Resets the \u201cpeak\u201d stats tracked by what?",
        "Z": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "Y": "CUDA memory allocator"
    },
    {
        "X": "What is the current state of this module?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "BETA"
    },
    {
        "X": "Some functions may change in future what?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "PyTorch releases"
    },
    {
        "X": "For details, see what for details. Computes the entropy on input (as defined below), elementwise?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "documentation of each function"
    },
    {
        "X": "Computes the entropy on input (as defined below), elementwise. Computes the entropy on input (a",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "Computes the error function of input. The error function is defined as follows: what is the input tensor?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is the name of the function that Computes the error function of input?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "Computes the complementary error function of input. The complementary error function is defined as follows: what is the input tensor?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is the equivalent of Computes the exponential of the elements minus 1 of input?",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "Computes the exponential of the elements minus 1 of input"
    },
    {
        "X": "Computes the natural logarithm of the absolute value of the gamma function on input?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "Y": "Computes the natural logarithm of the absolute value of the gamma function on input"
    },
    {
        "X": "Computes the natural logarithm of the absolute value of the gamma function on input. Example: Computes the inverse",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "Y": "Computes"
    },
    {
        "X": "What is the error function defined as?",
        "Z": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note ",
        "Y": "input (Tensor) \u2013 the input tensor"
    },
    {
        "X": "What is an example of a function that computes the inverse error function of input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "Computes the inverse error function of input"
    },
    {
        "X": "What is also known as the logistic sigmoid function?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "expit"
    },
    {
        "X": "What is an example of a computation?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "Computes the natural logarithm of the absolute value of the gamma function on input"
    },
    {
        "X": "Computes the first kind (as defined below) for each element of input. input (Tensor) \u2013 the input ten",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "exponentially scaled zeroth order modified Bessel function"
    },
    {
        "X": "What does input (Tensor) refer to?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "input tensor"
    },
    {
        "X": "Out (Tensor) is what?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "Y": "optional"
    },
    {
        "X": "What does the output tensor do?",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "Y": "Computes the error function of input"
    },
    {
        "X": "What is the name of the function that computes the complementary error function of input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "Computes the complementary error function of input"
    },
    {
        "X": "What is an example?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "Computes the base two exponential function of input"
    },
    {
        "X": "Returns what with the logit of the elements of input?",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "Y": "a new tensor"
    },
    {
        "X": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects?",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "Y": "Future type"
    },
    {
        "X": "What does torch._C.Future expose?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "APIs"
    },
    {
        "X": "Warning GPU support is a what?",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "Y": "beta"
    },
    {
        "X": "If the Future is already completed, the given callback will be run what way?",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. ",
        "Y": "inline"
    },
    {
        "X": "GPU support is a what?",
        "Z": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "Y": "beta feature"
    },
    {
        "X": "When calling wait()/value() on this Future, the exception set here will be raised what way?",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "inline"
    },
    {
        "X": "What method behaves in the same way with respect to GPU tensors?",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "Y": "then()"
    },
    {
        "X": "What are the streams on which the kernels were enqueued set as when this method is called?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "current ones"
    },
    {
        "X": "What is safe to call this method immediately after launching kernels without any additional synchronization?",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "change streams in between"
    },
    {
        "X": "This method will use events on all the relevant current streams to ensure proper scheduling for all the consumers of what?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "this Future"
    },
    {
        "X": "What is the reference to this Future?",
        "Z": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "this Future"
    },
    {
        "X": "This method will use the events to ensure proper scheduling for all the consumers of what?",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "Y": "this Future"
    },
    {
        "X": "When is it safe to call this method if the result contains tensors that reside on GPUs?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "immediately after launching those kernels"
    },
    {
        "X": "What does Future.done() do?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "record events on all the relevant current streams"
    },
    {
        "X": "What is used to enforce a certain order?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "chaining"
    },
    {
        "X": "How many arguments must the callback take?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "one argument"
    },
    {
        "X": "What will be run immediately inline if this Future is already completed?",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "the given callback"
    },
    {
        "X": "When will the given callback be run immediately inline?",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "this Future is already completed"
    },
    {
        "X": "What does setting the result for this Future do?",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "mark this Future as completed and trigger all attached callbacks"
    },
    {
        "X": "If this Future is already completed, the given callback will be run immediately what?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "inline"
    },
    {
        "X": "What does a Future not have to be done twice?",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "Y": "a Future cannot be marked completed twice"
    },
    {
        "X": "If the Future's value contains tensors that reside on GPUs, the callback might be invoked when?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device"
    },
    {
        "X": "What are the dedicated streams set to when the callback is invoked?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "current"
    },
    {
        "X": "When will any operation performed by the callback on these tensors be scheduled on the device?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "after the kernels complete"
    },
    {
        "X": "If the callback doesn't do what, it can safely manipulate the result without any additional synchronization?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "switch streams"
    },
    {
        "X": "The non-blocking behavior of what method is similar to the non-blocking behavior of?",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "Y": "wait()"
    },
    {
        "X": "If the Future's value contains what that reside on GPUs, the callback might be invoked while the async kernels that are",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "tensors"
    },
    {
        "X": "The non-blocking behavior of what is similar to the non-blocking behavior of?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "wait()"
    },
    {
        "X": "What happens if the callback returns a value that contains tensors that reside on a GPU?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "if the callback returns a value that contains tensors that reside on a GPU"
    },
    {
        "X": "What does a callback take as the only argument?",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "Future"
    },
    {
        "X": "What does a callback do to an already-completed future?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "Obtain the value of an already-completed future"
    },
    {
        "X": "What could happen if the callback is called after a call to wait() has completed?",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "Y": "Future may not yet hold a value"
    },
    {
        "X": "If the value contains what that reside on GPUs, this method will not perform any additional synchronization.",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "tensors"
    },
    {
        "X": "What method does not perform any additional synchronization if the value contains tensors that reside on GPUs?",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "Y": "wait()"
    },
    {
        "X": "What does fut.wait() do?",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "Obtain the value of an already-completed future"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, this method will not perform any additional synchronization. This should be done separately",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "Y": "wait()"
    },
    {
        "X": "What method will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "Y": "wait()"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, what is performed with the kernels?",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "synchronization"
    },
    {
        "X": "What method inserts the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "Y": "wait()"
    },
    {
        "X": "How to use torch.atanh, give an example?",
        "Z": "  >>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])\n",
        "Y": ">>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])\n"
    },
    {
        "X": "How to use torch.logaddexp, give an example?",
        "Z": " This op should be disambiguated with torch.logsumexp() which performs a\nreduction on a single tensor. >>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))\ntensor([-0.3069, -0.6867, -0.8731])\n>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))\ntensor([-1., -2., -3.])\n>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))\ntensor([1.1269e+00, 2.0000e+03, 3.0000e+04])\n",
        "Y": ">>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))\ntensor([-0.3069, -0.6867, -0.8731])\n>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))\ntensor([-1., -2., -3.])\n>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))\ntensor([1.1269e+00, 2.0000e+03, 3.0000e+04])\n"
    },
    {
        "X": "How to use torch.testing.assert_close, give an example?",
        "Z": " For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs. >>> # tensor to tensor comparison\n>>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n>>> actual = torch.acos(torch.cos(expected))\n>>> torch.testing.assert_close(actual, expected)\n",
        "Y": ">>> # tensor to tensor comparison\n>>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n>>> actual = torch.acos(torch.cos(expected))\n>>> torch.testing.assert_close(actual, expected)\n"
    },
    {
        "X": "How  For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs., give an example?",
        "Z": " For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs. >>> expected = torch.tensor([1.0, 2.0, 3.0])\n>>> actual = torch.tensor([1.0, 4.0, 5.0])\n>>> # The default mismatch message can be overwritten.\n>>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\nAssertionError: Argh, the tensors are not close!\n>>> # The error message can also created at runtime by passing a callable.\n>>> def custom_msg(actual, expected, diagnostic_info):\n...     return (\n...         f\"Argh, we found {diagnostic_info.total_mismatches} mismatches! \"\n...         f\"That is {diagnostic_info.mismatch_ratio:.1%}!\"\n...     )\n>>> torch.testing.assert_close(actual, expected, msg=custom_msg)\nAssertionError: Argh, we found 2 mismatches! That is 66.7%!\n",
        "Y": ">>> expected = torch.tensor([1.0, 2.0, 3.0])\n>>> actual = torch.tensor([1.0, 4.0, 5.0])\n>>> # The default mismatch message can be overwritten.\n>>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\nAssertionError: Argh, the tensors are not close!\n>>> # The error message can also created at runtime by passing a callable.\n>>> def custom_msg(actual, expected, diagnostic_info):\n...     return (\n...         f\"Argh, we found {diagnostic_info.total_mismatches} mismatches! \"\n...         f\"That is {diagnostic_info.mismatch_ratio:.1%}!\"\n...     )\n>>> torch.testing.assert_close(actual, expected, msg=custom_msg)\nAssertionError: Argh, we found 2 mismatches! That is 66.7%!\n"
    },
    {
        "X": "How to use torch.diag, give an example?",
        "Z": " Get the square matrix where the input vector is the diagonal: >>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\n",
        "Y": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])\n"
    },
    {
        "X": "How  Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix:, give an example?",
        "Z": " Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])\n"
    },
    {
        "X": "How to use The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc., give an example?",
        "Z": "  >>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n",
        "Y": ">>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n"
    },
    {
        "X": "How to use torch.overrides.get_ignored_functions, give an example?",
        "Z": "  >>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse\n",
        "Y": ">>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse\n"
    },
    {
        "X": "How to use torch.overrides.get_testing_overrides, give an example?",
        "Z": "  >>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>\n",
        "Y": ">>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>\n"
    },
    {
        "X": "How to use torch.overrides.handle_torch_function, give an example?",
        "Z": " :raises TypeError : if no implementation is found.: >>> def func(a):\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0\n",
        "Y": ">>> def func(a):\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0\n"
    },
    {
        "X": "How to use torch.overrides.is_tensor_like, give an example?",
        "Z": " A subclass of tensor is generally a Tensor-like. >>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue\n",
        "Y": ">>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue\n"
    },
    {
        "X": "How  A subclass of tensor is generally a Tensor-like.Built-in or user types aren\u2019t usually Tensor-like., give an example?",
        "Z": " A subclass of tensor is generally a Tensor-like.Built-in or user types aren\u2019t usually Tensor-like. >>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse\n",
        "Y": ">>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse\n"
    },
    {
        "X": "How  Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__., give an example?",
        "Z": " Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue\n",
        "Y": ">>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue\n"
    },
    {
        "X": "How to use torch.overrides.is_tensor_method_or_property, give an example?",
        "Z": " This may be needed, in particular, for the following reasons: >>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse\n",
        "Y": ">>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse\n"
    },
    {
        "X": "How to use torch.overrides.wrap_torch_function, give an example?",
        "Z": "  >>> def dispatcher(a): # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a): # This will make func dispatchable by __torch_function__\n...     return a + 0\n",
        "Y": ">>> def dispatcher(a): # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a): # This will make func dispatchable by __torch_function__\n...     return a + 0\n"
    },
    {
        "X": "How to use torch.mvlgamma, give an example?",
        "Z": " All elements must be greater than p\u221212\\frac{p - 1}{2}2p\u22121\u200b, otherwise an error would be thrown. >>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])\n",
        "Y": ">>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])\n"
    },
    {
        "X": "How to use torch.fake_quantize_per_channel_affine, give an example?",
        "Z": "  >>> x = torch.randn(2, 2, 2)\n>>> x\ntensor([[[-0.2525, -0.0466],\n         [ 0.3491, -0.2168]],\n\n        [[-0.5906,  1.6258],\n         [ 0.6444, -0.0542]]])\n>>> scales = (torch.randn(2) + 1) * 0.05\n>>> scales\ntensor([0.0475, 0.0486])\n>>> zero_points = torch.zeros(2).to(torch.long)\n>>> zero_points\ntensor([0, 0])\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\ntensor([[[0.0000, 0.0000],\n         [0.3405, 0.0000]],\n\n        [[0.0000, 1.6134],\n        [0.6323, 0.0000]]])\n",
        "Y": ">>> x = torch.randn(2, 2, 2)\n>>> x\ntensor([[[-0.2525, -0.0466],\n         [ 0.3491, -0.2168]],\n\n        [[-0.5906,  1.6258],\n         [ 0.6444, -0.0542]]])\n>>> scales = (torch.randn(2) + 1) * 0.05\n>>> scales\ntensor([0.0475, 0.0486])\n>>> zero_points = torch.zeros(2).to(torch.long)\n>>> zero_points\ntensor([0, 0])\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\ntensor([[[0.0000, 0.0000],\n         [0.3405, 0.0000]],\n\n        [[0.0000, 1.6134],\n        [0.6323, 0.0000]]])\n"
    },
    {
        "X": "How to use torch.atleast_1d, give an example?",
        "Z": "  >>> x = torch.randn(2)\n>>> x\ntensor([1.4584, 0.7583])\n>>> torch.atleast_1d(x)\ntensor([1.4584, 0.7583])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x,y))\n(tensor([0.5000]), tensor([1.]))\n",
        "Y": ">>> x = torch.randn(2)\n>>> x\ntensor([1.4584, 0.7583])\n>>> torch.atleast_1d(x)\ntensor([1.4584, 0.7583])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x,y))\n(tensor([0.5000]), tensor([1.]))\n"
    },
    {
        "X": "How to use torch.heaviside, give an example?",
        "Z": "  >>> input = torch.tensor([-1.5, 0, 2.0])\n>>> values = torch.tensor([0.5])\n>>> torch.heaviside(input, values)\ntensor([0.0000, 0.5000, 1.0000])\n>>> values = torch.tensor([1.2, -2.0, 3.5])\n>>> torch.heaviside(input, values)\ntensor([0., -2., 1.])\n",
        "Y": ">>> input = torch.tensor([-1.5, 0, 2.0])\n>>> values = torch.tensor([0.5])\n>>> torch.heaviside(input, values)\ntensor([0.0000, 0.5000, 1.0000])\n>>> values = torch.tensor([1.2, -2.0, 3.5])\n>>> torch.heaviside(input, values)\ntensor([0., -2., 1.])\n"
    },
    {
        "X": "How to use torch.atleast_2d, give an example?",
        "Z": "  >>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.randn(2,2)\n>>> x\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> torch.atleast_2d(x)\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x,y))\n(tensor([[0.5000]]), tensor([[1.]]))\n",
        "Y": ">>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.randn(2,2)\n>>> x\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> torch.atleast_2d(x)\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x,y))\n(tensor([[0.5000]]), tensor([[1.]]))\n"
    },
    {
        "X": "How to use torch.ldexp, give an example?",
        "Z": " Typically this function is used to construct floating point numbers by multiplying\nmantissas in input with integral powers of two created from the exponents\nin :attr:\u2019other\u2019. >>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))\ntensor([2.])\n>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))\ntensor([ 2.,  4.,  8., 16.])\n",
        "Y": ">>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))\ntensor([2.])\n>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))\ntensor([ 2.,  4.,  8., 16.])\n"
    },
    {
        "X": "How to use torch.promote_types, give an example?",
        "Z": "  >>> torch.promote_types(torch.int32, torch.float32)\ntorch.float32\n>>> torch.promote_types(torch.uint8, torch.long)\ntorch.long\n",
        "Y": ">>> torch.promote_types(torch.int32, torch.float32)\ntorch.float32\n>>> torch.promote_types(torch.uint8, torch.long)\ntorch.long\n"
    },
    {
        "X": "How to use torch.nextafter, give an example?",
        "Z": " The shapes of input and other must be\nbroadcastable. >>> eps = torch.finfo(torch.float32).eps\n>>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])\ntensor([True, True])\n",
        "Y": ">>> eps = torch.finfo(torch.float32).eps\n>>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])\ntensor([True, True])\n"
    },
    {
        "X": "How to use torch.set_default_tensor_type, give an example?",
        "Z": " The default floating point tensor type is initially torch.FloatTensor. >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_tensor_type(torch.DoubleTensor)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n",
        "Y": ">>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_tensor_type(torch.DoubleTensor)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n"
    },
    {
        "X": "How to use torch.diagflat, give an example?",
        "Z": "  >>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\n",
        "Y": ">>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])\n"
    },
    {
        "X": "How to use torch.lu_unpack, give an example?",
        "Z": " Returns a tuple of tensors as (the P tensor (permutation matrix), the L tensor, the U tensor). >>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>>\n>>> # can recover A from factorization\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n>>> # LU factorization of a rectangular matrix:\n>>> A = torch.randn(2, 3, 2)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>> P\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]])\n>>> A_L\ntensor([[[ 1.0000,  0.0000],\n         [ 0.4763,  1.0000],\n         [ 0.3683,  0.1135]],\n\n        [[ 1.0000,  0.0000],\n         [ 0.2957,  1.0000],\n         [-0.9668, -0.3335]]])\n>>> A_U\ntensor([[[ 2.1962,  1.0881],\n         [ 0.0000, -0.8681]],\n\n        [[-1.0947,  0.3736],\n         [ 0.0000,  0.5718]]])\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n>>> torch.norm(A_ - A)\ntensor(2.9802e-08)\n",
        "Y": ">>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>>\n>>> # can recover A from factorization\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n>>> # LU factorization of a rectangular matrix:\n>>> A = torch.randn(2, 3, 2)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>> P\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]])\n>>> A_L\ntensor([[[ 1.0000,  0.0000],\n         [ 0.4763,  1.0000],\n         [ 0.3683,  0.1135]],\n\n        [[ 1.0000,  0.0000],\n         [ 0.2957,  1.0000],\n         [-0.9668, -0.3335]]])\n>>> A_U\ntensor([[[ 2.1962,  1.0881],\n         [ 0.0000, -0.8681]],\n\n        [[-1.0947,  0.3736],\n         [ 0.0000,  0.5718]]])\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n>>> torch.norm(A_ - A)\ntensor(2.9802e-08)\n"
    },
    {
        "X": "How to use torch.addmv, give an example?",
        "Z": " For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers >>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n",
        "Y": ">>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])\n"
    },
    {
        "X": "How to use torch.nanmedian, give an example?",
        "Z": " This function is identical to torch.median() when there are no NaN values in input.\nWhen input has one or more NaN values, torch.median() will always return NaN,\nwhile this function will return the median of the non-NaN elements in input.\nIf all the elements in input are NaN it will also return NaN. >>> a = torch.tensor([1, float('nan'), 3, 2])\n>>> a.median()\ntensor(nan)\n>>> a.nanmedian()\ntensor(2.)\n",
        "Y": ">>> a = torch.tensor([1, float('nan'), 3, 2])\n>>> a.median()\ntensor(nan)\n>>> a.nanmedian()\ntensor(2.)\n"
    },
    {
        "X": "How  This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has\none or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the\nmedian of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too., give an example?",
        "Z": " This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has\none or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the\nmedian of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too. >>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])\n>>> a\ntensor([[2., 3., 1.],\n        [nan, 1., nan]])\n>>> a.median(0)\ntorch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))\n>>> a.nanmedian(0)\ntorch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))\n",
        "Y": ">>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])\n>>> a\ntensor([[2., 3., 1.],\n        [nan, 1., nan]])\n>>> a.median(0)\ntorch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))\n>>> a.nanmedian(0)\ntorch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))\n"
    },
    {
        "X": "How to use torch.empty_strided, give an example?",
        "Z": "  >>> a = torch.empty_strided((2, 3), (1, 2))\n>>> a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n>>> a.stride()\n(1, 2)\n>>> a.size()\ntorch.Size([2, 3])\n",
        "Y": ">>> a = torch.empty_strided((2, 3), (1, 2))\n>>> a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n>>> a.stride()\n(1, 2)\n>>> a.size()\ntorch.Size([2, 3])\n"
    },
    {
        "X": "How to use torch.isnan, give an example?",
        "Z": "  >>> torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([False, True, False])\n",
        "Y": ">>> torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([False, True, False])\n"
    },
    {
        "X": "How to use torch.atleast_3d, give an example?",
        "Z": "  >>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))\n",
        "Y": ">>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))\n"
    },
    {
        "X": "How to use torch.bitwise_xor, give an example?",
        "Z": "  >>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])\n",
        "Y": ">>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])\n"
    },
    {
        "X": "How to use torch.sinh, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n>>> torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n>>> torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])\n"
    },
    {
        "X": "How to use torch.roll, give an example?",
        "Z": "  >>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n>>> x\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n>>> torch.roll(x, 1, 0)\ntensor([[7, 8],\n        [1, 2],\n        [3, 4],\n        [5, 6]])\n>>> torch.roll(x, -1, 0)\ntensor([[3, 4],\n        [5, 6],\n        [7, 8],\n        [1, 2]])\n>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\ntensor([[6, 5],\n        [8, 7],\n        [2, 1],\n        [4, 3]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n>>> x\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n>>> torch.roll(x, 1, 0)\ntensor([[7, 8],\n        [1, 2],\n        [3, 4],\n        [5, 6]])\n>>> torch.roll(x, -1, 0)\ntensor([[3, 4],\n        [5, 6],\n        [7, 8],\n        [1, 2]])\n>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\ntensor([[6, 5],\n        [8, 7],\n        [2, 1],\n        [4, 3]])\n"
    },
    {
        "X": "How to use torch.tile, give an example?",
        "Z": " Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). >>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])\n"
    },
    {
        "X": "How to use torch.topk, give an example?",
        "Z": " The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted >>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.topk(x, 3)\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))\n",
        "Y": ">>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.topk(x, 3)\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))\n"
    },
    {
        "X": "How to use torch.sqrt, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])\n"
    },
    {
        "X": "How to use torch.minimum, give an example?",
        "Z": "  >>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.minimum(a, b)\ntensor([1, 0, -1])\n",
        "Y": ">>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.minimum(a, b)\ntensor([1, 0, -1])\n"
    },
    {
        "X": "How to use torch.floor_divide, give an example?",
        "Z": " Supports broadcasting to a common shape, type promotion, and integer and float inputs. >>> a = torch.tensor([4.0, 3.0])\n>>> b = torch.tensor([2.0, 2.0])\n>>> torch.floor_divide(a, b)\ntensor([2.0, 1.0])\n>>> torch.floor_divide(a, 1.4)\ntensor([2.0, 2.0])\n",
        "Y": ">>> a = torch.tensor([4.0, 3.0])\n>>> b = torch.tensor([2.0, 2.0])\n>>> torch.floor_divide(a, b)\ntensor([2.0, 1.0])\n>>> torch.floor_divide(a, 1.4)\ntensor([2.0, 2.0])\n"
    },
    {
        "X": "How to use torch.sgn, give an example?",
        "Z": "  >>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])\n>>> t.sgn()\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])\n",
        "Y": ">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])\n>>> t.sgn()\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])\n"
    },
    {
        "X": "How to use torch.ones_like, give an example?",
        "Z": "  >>> input = torch.empty(2, 3)\n>>> torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n",
        "Y": ">>> input = torch.empty(2, 3)\n>>> torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n"
    },
    {
        "X": "How to use torch.no_grad, give an example?",
        "Z": " Also functions as a decorator. (Make sure to instantiate with parenthesis.) >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n",
        "Y": ">>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse\n"
    },
    {
        "X": "How to use torch.view_as_complex, give an example?",
        "Z": "  >>> x=torch.randn(4, 2)\n>>> x\ntensor([[ 1.6116, -0.5772],\n        [-1.4606, -0.9120],\n        [ 0.0786, -1.7497],\n        [-0.6561, -1.6623]])\n>>> torch.view_as_complex(x)\ntensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])\n",
        "Y": ">>> x=torch.randn(4, 2)\n>>> x\ntensor([[ 1.6116, -0.5772],\n        [-1.4606, -0.9120],\n        [ 0.0786, -1.7497],\n        [-0.6561, -1.6623]])\n>>> torch.view_as_complex(x)\ntensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])\n"
    },
    {
        "X": "How to use torch.swapaxes, give an example?",
        "Z": " This function is equivalent to NumPy\u2019s swapaxes function. >>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])\n",
        "Y": ">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])\n"
    },
    {
        "X": "How to use torch.logical_xor, give an example?",
        "Z": "  >>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([False, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_xor(a, b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b.double())\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True, False, False])\n",
        "Y": ">>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([False, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_xor(a, b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b.double())\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True, False, False])\n"
    },
    {
        "X": "How to use A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported., give an example?",
        "Z": " A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported. >>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n",
        "Y": ">>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n"
    },
    {
        "X": "How to use torch dtype, give an example?",
        "Z": "  # allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor\n",
        "Y": "# allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor\n"
    },
    {
        "X": "How to use A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?",
        "Z": " A torch.device can be constructed via a string or via a string and device ordinalVia a string: >>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n",
        "Y": ">>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n"
    },
    {
        "X": "How to use Via a string:Via a string and device ordinal:, give an example?",
        "Z": " Via a string:Via a string and device ordinal: >>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n",
        "Y": ">>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n"
    },
    {
        "X": "How to use NoteThe torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code., give an example?",
        "Z": " The torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. >>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)\n",
        "Y": ">>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)\n"
    },
    {
        "X": "How  The torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code., give an example?",
        "Z": " The torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. >>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')\n",
        "Y": ">>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')\n"
    },
    {
        "X": "How to use NoteFor legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors., give an example?",
        "Z": " For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. >>> torch.device(1)\ndevice(type='cuda', index=1)\n",
        "Y": ">>> torch.device(1)\ndevice(type='cuda', index=1)\n"
    },
    {
        "X": "How to use NoteMethods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?",
        "Z": " Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: >>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy\n",
        "Y": ">>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy\n"
    },
    {
        "X": "How to use torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently., give an example?",
        "Z": " torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n",
        "Y": ">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n"
    },
    {
        "X": "How to use torch.median, give an example?",
        "Z": "  >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n>>> torch.median(a)\ntensor(0.2202)\n",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n>>> torch.median(a)\ntensor(0.2202)\n"
    },
    {
        "X": "How  If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input., give an example?",
        "Z": " If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. >>> a = torch.randn(4, 5)\n>>> a\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n>>> torch.median(a, 1)\ntorch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))\n",
        "Y": ">>> a = torch.randn(4, 5)\n>>> a\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n>>> torch.median(a, 1)\ntorch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))\n"
    },
    {
        "X": "How to use Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:, give an example?",
        "Z": " Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: import torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device='cuda')\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\n# Providing input and output names sets the display names for values\n# within the model's graph. Setting these does not change the semantics\n# of the graph; it is only for readability.\n#\n# The inputs to the network consist of the flat list of inputs (i.e.\n# the values you would pass to the forward() method) followed by the\n# flat list of parameters. You can partially specify names, i.e. provide\n# a list here shorter than the number of inputs to the model, and we will\n# only set that subset of names, starting from the beginning.\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n",
        "Y": "import torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device='cuda')\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\n# Providing input and output names sets the display names for values\n# within the model's graph. Setting these does not change the semantics\n# of the graph; it is only for readability.\n#\n# The inputs to the network consist of the flat list of inputs (i.e.\n# the values you would pass to the forward() method) followed by the\n# flat list of parameters. You can partially specify names, i.e. provide\n# a list here shorter than the number of inputs to the model, and we will\n# only set that subset of names, starting from the beginning.\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)\n"
    },
    {
        "X": "How to use Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:, give an example?",
        "Z": " The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: # These are the inputs and parameters to the network, which have taken on\n# the names we specified earlier.\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\n      %learned_0 : Float(64, 3, 11, 11)\n      %learned_1 : Float(64)\n      %learned_2 : Float(192, 64, 5, 5)\n      %learned_3 : Float(192)\n      # ---- omitted for brevity ----\n      %learned_14 : Float(1000, 4096)\n      %learned_15 : Float(1000)) {\n  # Every statement consists of some output tensors (and their types),\n  # the operator to be run (with its attributes, e.g., kernels, strides,\n  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  # ---- omitted for brevity ----\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  # Dynamic means that the shape is not known. This may be because of a\n  # limitation of our implementation (which we would like to fix in a\n  # future release) or shapes which are truly dynamic.\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n  # ---- omitted for brevity ----\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%output1);\n}\n",
        "Y": "# These are the inputs and parameters to the network, which have taken on\n# the names we specified earlier.\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\n      %learned_0 : Float(64, 3, 11, 11)\n      %learned_1 : Float(64)\n      %learned_2 : Float(192, 64, 5, 5)\n      %learned_3 : Float(192)\n      # ---- omitted for brevity ----\n      %learned_14 : Float(1000, 4096)\n      %learned_15 : Float(1000)) {\n  # Every statement consists of some output tensors (and their types),\n  # the operator to be run (with its attributes, e.g., kernels, strides,\n  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  # ---- omitted for brevity ----\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  # Dynamic means that the shape is not known. This may be because of a\n  # limitation of our implementation (which we would like to fix in a\n  # future release) or shapes which are truly dynamic.\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n  # ---- omitted for brevity ----\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%output1);\n}\n"
    },
    {
        "X": "How to use The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda:, give an example?",
        "Z": " The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: conda install -c conda-forge onnx\n",
        "Y": "conda install -c conda-forge onnx\n"
    },
    {
        "X": "How to use You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda:Then, you can run:, give an example?",
        "Z": " You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda:Then, you can run: import onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)\n",
        "Y": "import onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)\n"
    },
    {
        "X": "How to use To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions.Once these are installed, you can use the backend for Caffe2:, give an example?",
        "Z": " To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions.Once these are installed, you can use the backend for Caffe2: # ...continuing from above\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\nprint(outputs[0])\n",
        "Y": "# ...continuing from above\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\nprint(outputs[0])\n"
    },
    {
        "X": "How to use You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions.Once these are installed, you can use the backend for ONNX Runtime:, give an example?",
        "Z": " You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions.Once these are installed, you can use the backend for ONNX Runtime: # ...continuing from above\nimport onnxruntime as ort\n\nort_session = ort.InferenceSession('alexnet.onnx')\n\noutputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\n\nprint(outputs[0])\n",
        "Y": "# ...continuing from above\nimport onnxruntime as ort\n\nort_session = ort.InferenceSession('alexnet.onnx')\n\noutputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\n\nprint(outputs[0])\n"
    },
    {
        "X": "How to use The ONNX exporter can be both trace-based and script-based exporter.We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example:, give an example?",
        "Z": " We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: import torch\n\n# Trace-based only\n\nclass LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        for i in range(y):\n            x = x + i\n        return x\n\nmodel = LoopModel()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\n\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)\n",
        "Y": "import torch\n\n# Trace-based only\n\nclass LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        for i in range(y):\n            x = x + i\n        return x\n\nmodel = LoopModel()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\n\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)\n"
    },
    {
        "X": "How to use We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example:With trace-based exporter, we get the result ONNX graph which unrolls the for loop:, give an example?",
        "Z": " We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example:With trace-based exporter, we get the result ONNX graph which unrolls the for loop: graph(%0 : Long(2, 3),\n      %1 : Long()):\n  %2 : Tensor = onnx::Constant[value={1}]()\n  %3 : Tensor = onnx::Add(%0, %2)\n  %4 : Tensor = onnx::Constant[value={2}]()\n  %5 : Tensor = onnx::Add(%3, %4)\n  %6 : Tensor = onnx::Constant[value={3}]()\n  %7 : Tensor = onnx::Add(%5, %6)\n  %8 : Tensor = onnx::Constant[value={4}]()\n  %9 : Tensor = onnx::Add(%7, %8)\n  return (%9)\n",
        "Y": "graph(%0 : Long(2, 3),\n      %1 : Long()):\n  %2 : Tensor = onnx::Constant[value={1}]()\n  %3 : Tensor = onnx::Add(%0, %2)\n  %4 : Tensor = onnx::Constant[value={2}]()\n  %5 : Tensor = onnx::Add(%3, %4)\n  %6 : Tensor = onnx::Constant[value={3}]()\n  %7 : Tensor = onnx::Add(%5, %6)\n  %8 : Tensor = onnx::Constant[value={4}]()\n  %9 : Tensor = onnx::Add(%7, %8)\n  return (%9)\n"
    },
    {
        "X": "How to use With trace-based exporter, we get the result ONNX graph which unrolls the for loop:To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:, give an example?",
        "Z": " With trace-based exporter, we get the result ONNX graph which unrolls the for loop:To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: # Mixing tracing and scripting\n\n@torch.jit.script\ndef loop(x, y):\n    for i in range(int(y)):\n        x = x + i\n    return x\n\nclass LoopModel2(torch.nn.Module):\n    def forward(self, x, y):\n        return loop(x, y)\n\nmodel = LoopModel2()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,\n                  input_names=['input_data', 'loop_range'])\n",
        "Y": "# Mixing tracing and scripting\n\n@torch.jit.script\ndef loop(x, y):\n    for i in range(int(y)):\n        x = x + i\n    return x\n\nclass LoopModel2(torch.nn.Module):\n    def forward(self, x, y):\n        return loop(x, y)\n\nmodel = LoopModel2()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,\n                  input_names=['input_data', 'loop_range'])\n"
    },
    {
        "X": "How to use To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes:, give an example?",
        "Z": " To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes: graph(%input_data : Long(2, 3),\n      %loop_range : Long()):\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\n  %3 : Tensor = onnx::Cast[to=9](%2)\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop\n      %9 : Tensor = onnx::Cast[to=9](%2)\n      -> (%9, %8)\n  return (%4)\n",
        "Y": "graph(%input_data : Long(2, 3),\n      %loop_range : Long()):\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\n  %3 : Tensor = onnx::Cast[to=9](%2)\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop\n      %9 : Tensor = onnx::Cast[to=9](%2)\n      -> (%9, %8)\n  return (%4)\n"
    },
    {
        "X": "How to use Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range., give an example?",
        "Z": " Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range. import caffe2.python.onnx.backend as backend\nimport numpy as np\nimport onnx\nmodel = onnx.load('loop.onnx')\n\nrep = backend.prepare(model)\noutputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))\nprint(outputs[0])\n#[[37 37 37]\n# [37 37 37]]\n\n\nimport onnxruntime as ort\nort_sess = ort.InferenceSession('loop.onnx')\noutputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\n                              'loop_range': np.array(9).astype(np.int64)})\nprint(outputs)\n#[array([[37, 37, 37],\n#       [37, 37, 37]], dtype=int64)]\n",
        "Y": "import caffe2.python.onnx.backend as backend\nimport numpy as np\nimport onnx\nmodel = onnx.load('loop.onnx')\n\nrep = backend.prepare(model)\noutputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))\nprint(outputs[0])\n#[[37 37 37]\n# [37 37 37]]\n\n\nimport onnxruntime as ort\nort_sess = ort.InferenceSession('loop.onnx')\noutputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\n                              'loop_range': np.array(9).astype(np.int64)})\nprint(outputs)\n#[array([[37, 37, 37],\n#       [37, 37, 37]], dtype=int64)]\n"
    },
    {
        "X": "How to use The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\navoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\nE.g.:, give an example?",
        "Z": " The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\navoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\nE.g.: class LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        res = []\n        arr = x.split(2, 0)\n        for i in range(int(y)):\n            res += [arr[i].sum(0, False)]\n        return torch.stack(res)\n\nmodel = torch.jit.script(LoopModel())\ninputs = (torch.randn(16), torch.tensor(8))\n\nout = model(*inputs)\ntorch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n",
        "Y": "class LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        res = []\n        arr = x.split(2, 0)\n        for i in range(int(y)):\n            res += [arr[i].sum(0, False)]\n        return torch.stack(res)\n\nmodel = torch.jit.script(LoopModel())\ninputs = (torch.randn(16), torch.tensor(8))\n\nout = model(*inputs)\ntorch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n"
    },
    {
        "X": "How to use TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere.Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations., give an example?",
        "Z": " Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. import torch\n\nclass Module(torch.nn.Module):\n    def forward(self, x, tup):\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n        t0, t1 = tup\n        return t0 + t1 + x\n",
        "Y": "import torch\n\nclass Module(torch.nn.Module):\n    def forward(self, x, tup):\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n        t0, t1 = tup\n        return t0 + t1 + x\n"
    },
    {
        "X": "How to use Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below., give an example?",
        "Z": " Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below. RuntimeError:\nTensor (inferred) cannot be used as a tuple:\n  File <filename>\n        def forward(self, x, tup):\n            t0, t1 = tup\n                     ~~~ <--- HERE\n            return t0 + t1 + x\n",
        "Y": "RuntimeError:\nTensor (inferred) cannot be used as a tuple:\n  File <filename>\n        def forward(self, x, tup):\n            t0, t1 = tup\n                     ~~~ <--- HERE\n            return t0 + t1 + x\n"
    },
    {
        "X": "How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors:, give an example?",
        "Z": " PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: np.concatenate((x, y, z), axis=1)\n",
        "Y": "np.concatenate((x, y, z), axis=1)\n"
    },
    {
        "X": "How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors:do not convert to numpy types:, give an example?",
        "Z": " do not convert to numpy types: y = x.astype(np.int)\n",
        "Y": "y = x.astype(np.int)\n"
    },
    {
        "X": "How to use do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,, give an example?",
        "Z": " do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., class MyModule(nn.Module):\n    def __init__(self):\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)\n",
        "Y": "class MyModule(nn.Module):\n    def __init__(self):\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)\n"
    },
    {
        "X": "How to use There are two ways to handle models which consist of named parameters or keyword arguments as inputs:For example, in the model:, give an example?",
        "Z": " For example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)\n",
        "Y": "class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)\n"
    },
    {
        "X": "How to use Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model, give an example?",
        "Z": " Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model torch.onnx.export(model, (x, None, z), \u2018test.onnx\u2019)\n",
        "Y": "torch.onnx.export(model, (x, None, z), \u2018test.onnx\u2019)\n"
    },
    {
        "X": "How to use Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple., give an example?",
        "Z": " Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. torch.onnx.export(model, (x, {'y': None, 'z': z}), \u2018test.onnx\u2019)\n",
        "Y": "torch.onnx.export(model, (x, {'y': None, 'z': z}), \u2018test.onnx\u2019)\n"
    },
    {
        "X": "How to use There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example,, give an example?",
        "Z": " There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, torch.onnx.export(model, (x, {}), \u2018test.onnx\u2019)\nor\ntorch.onnx.export(model, (x, ), \u2018test.onnx\u2019)\n",
        "Y": "torch.onnx.export(model, (x, {}), \u2018test.onnx\u2019)\nor\ntorch.onnx.export(model, (x, ), \u2018test.onnx\u2019)\n"
    },
    {
        "X": "How to use For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,, give an example?",
        "Z": " For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, class Model(torch.nn.Module):\n  def forward(self, k, x):\n    ...\n    return x\nm = Model()\nk =\u202ftorch.randn(2, 3)\nx = {torch.tensor(1.):\u202ftorch.randn(2, 3)}\n",
        "Y": "class Model(torch.nn.Module):\n  def forward(self, k, x):\n    ...\n    return x\nm = Model()\nk =\u202ftorch.randn(2, 3)\nx = {torch.tensor(1.):\u202ftorch.randn(2, 3)}\n"
    },
    {
        "X": "How to use An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this., give an example?",
        "Z": " An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)\n",
        "Y": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)\n"
    },
    {
        "X": "How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:, give an example?",
        "Z": " This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: data = torch.randn(3, 4)\nindex = torch.tensor([1, 2])\n\n# RHS indexing is supported in ONNX opset >= 11.\nclass RHSIndexing(torch.nn.Module):\n    def forward(self, data, index):\n        return data[index]\n\nout = RHSIndexing()(data, index)\n\ntorch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('indexing.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: data.numpy(),\n    sess.get_inputs()[1].name: index.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\n",
        "Y": "data = torch.randn(3, 4)\nindex = torch.tensor([1, 2])\n\n# RHS indexing is supported in ONNX opset >= 11.\nclass RHSIndexing(torch.nn.Module):\n    def forward(self, data, index):\n        return data[index]\n\nout = RHSIndexing()(data, index)\n\ntorch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('indexing.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: data.numpy(),\n    sess.get_inputs()[1].name: index.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\n"
    },
    {
        "X": "How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:Below is the list of supported patterns for RHS indexing., give an example?",
        "Z": " Below is the list of supported patterns for RHS indexing. # Scalar indices\ndata[0, 1]\n\n# Slice indices\ndata[:3]\n\n# Tensor indices\ndata[torch.tensor([[1, 2], [2, 3]])]\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])]\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])]\n\n# Ellipsis followed by tensor indexing\n# Not supported in scripting\n# i.e. torch.jit.script(model) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])]\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]\n\n# Boolean mask (supported for ONNX opset version >= 11)\ndata[data != 1]\n",
        "Y": "# Scalar indices\ndata[0, 1]\n\n# Slice indices\ndata[:3]\n\n# Tensor indices\ndata[torch.tensor([[1, 2], [2, 3]])]\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])]\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])]\n\n# Ellipsis followed by tensor indexing\n# Not supported in scripting\n# i.e. torch.jit.script(model) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])]\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]\n\n# Boolean mask (supported for ONNX opset version >= 11)\ndata[data != 1]\n"
    },
    {
        "X": "How to use Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing., give an example?",
        "Z": " Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing. # Tensor indices that includes negative values.\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\n",
        "Y": "# Tensor indices that includes negative values.\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]\n"
    },
    {
        "X": "How to use In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.:, give an example?",
        "Z": " In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: data = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\n\n# LHS indexing is supported in ONNX opset >= 11.\nclass LHSIndexing(torch.nn.Module):\n    def forward(self, data, new_data):\n        data[1] = new_data\n        return data\n\nout = LHSIndexing()(data, new_data)\n\ndata = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\ntorch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('inplace_assign.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),\n    sess.get_inputs()[1].name: new_data.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\n",
        "Y": "data = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\n\n# LHS indexing is supported in ONNX opset >= 11.\nclass LHSIndexing(torch.nn.Module):\n    def forward(self, data, new_data):\n        data[1] = new_data\n        return data\n\nout = LHSIndexing()(data, new_data)\n\ndata = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\ntorch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('inplace_assign.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),\n    sess.get_inputs()[1].name: new_data.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))\n"
    },
    {
        "X": "How to use In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.:Below is the list of supported patterns for LHS indexing., give an example?",
        "Z": " Below is the list of supported patterns for LHS indexing. # Scalar indices\ndata[0, 1] = new_data\n\n# Slice indices\ndata[:3] = new_data\n\n# Tensor indices\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\n\n# Ellipsis followed by tensor indexing\n# Not supported to export in script modules\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])] = new_data\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\n\n# Boolean mask\ndata[data != 1] = new_data\n",
        "Y": "# Scalar indices\ndata[0, 1] = new_data\n\n# Slice indices\ndata[:3] = new_data\n\n# Tensor indices\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\n\n# Ellipsis followed by tensor indexing\n# Not supported to export in script modules\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])] = new_data\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\n\n# Boolean mask\ndata[data != 1] = new_data\n"
    },
    {
        "X": "How to use Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing., give an example?",
        "Z": " Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing. # Multiple tensor indices if any has rank >= 2\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n\n# Multiple tensor indices that are not consecutive\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n\n# Tensor indices that includes negative values.\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\n",
        "Y": "# Multiple tensor indices if any has rank >= 2\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n\n# Multiple tensor indices that are not consecutive\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n\n# Tensor indices that includes negative values.\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data\n"
    },
    {
        "X": "How to use If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions:Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:, give an example?",
        "Z": " Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: def operator/symbolic(g, *inputs):\n  \"\"\"\n  Modifies Graph (e.g., using \"op\"), adding the ONNX operations representing\n  this PyTorch function, and returning a Value or tuple of Values specifying the\n  ONNX outputs whose values correspond to the original PyTorch return values\n  of the autograd Function (or None if an output is not supported by ONNX).\n\n  Args:\n    g (Graph): graph to write the ONNX representation into\n    inputs (Value...): list of values representing the variables which contain\n        the inputs for this function\n  \"\"\"\n\nclass Value(object):\n  \"\"\"Represents an intermediate tensor value computed in ONNX.\"\"\"\n  def type(self):\n    \"\"\"Returns the Type of the value.\"\"\"\n\nclass Type(object):\n  def sizes(self):\n    \"\"\"Returns a tuple of ints representing the shape of a tensor this describes.\"\"\"\n\nclass Graph(object):\n  def op(self, opname, *inputs, **attrs):\n    \"\"\"\n    Create an ONNX operator 'opname', taking 'args' as inputs\n    and attributes 'kwargs' and add it as a node to the current graph,\n    returning the value representing the single output of this\n    operator (see the `outputs` keyword argument for multi-return\n    nodes).\n\n    The set of operators and the inputs/attributes they take\n    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md\n\n    Args:\n        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.\n        args (Value...): The inputs to the operator; usually provided\n            as arguments to the `symbolic` definition.\n        kwargs: The attributes of the ONNX operator, with keys named\n            according to the following convention: `alpha_f` indicates\n            the `alpha` attribute with type `f`.  The valid type specifiers are\n            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute\n            specified with type float accepts either a single float, or a\n            list of floats (e.g., you would say `dims_i` for a `dims` attribute\n            that takes a list of integers).\n        outputs (int, optional):  The number of outputs this operator returns;\n            by default an operator is assumed to return a single output.\n            If `outputs` is greater than one, this functions returns a tuple\n            of output `Value`, representing each output of the ONNX operator\n            in positional.\n    \"\"\"\n",
        "Y": "def operator/symbolic(g, *inputs):\n  \"\"\"\n  Modifies Graph (e.g., using \"op\"), adding the ONNX operations representing\n  this PyTorch function, and returning a Value or tuple of Values specifying the\n  ONNX outputs whose values correspond to the original PyTorch return values\n  of the autograd Function (or None if an output is not supported by ONNX).\n\n  Args:\n    g (Graph): graph to write the ONNX representation into\n    inputs (Value...): list of values representing the variables which contain\n        the inputs for this function\n  \"\"\"\n\nclass Value(object):\n  \"\"\"Represents an intermediate tensor value computed in ONNX.\"\"\"\n  def type(self):\n    \"\"\"Returns the Type of the value.\"\"\"\n\nclass Type(object):\n  def sizes(self):\n    \"\"\"Returns a tuple of ints representing the shape of a tensor this describes.\"\"\"\n\nclass Graph(object):\n  def op(self, opname, *inputs, **attrs):\n    \"\"\"\n    Create an ONNX operator 'opname', taking 'args' as inputs\n    and attributes 'kwargs' and add it as a node to the current graph,\n    returning the value representing the single output of this\n    operator (see the `outputs` keyword argument for multi-return\n    nodes).\n\n    The set of operators and the inputs/attributes they take\n    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md\n\n    Args:\n        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.\n        args (Value...): The inputs to the operator; usually provided\n            as arguments to the `symbolic` definition.\n        kwargs: The attributes of the ONNX operator, with keys named\n            according to the following convention: `alpha_f` indicates\n            the `alpha` attribute with type `f`.  The valid type specifiers are\n            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute\n            specified with type float accepts either a single float, or a\n            list of floats (e.g., you would say `dims_i` for a `dims` attribute\n            that takes a list of integers).\n        outputs (int, optional):  The number of outputs this operator returns;\n            by default an operator is assumed to return a single output.\n            If `outputs` is greater than one, this functions returns a tuple\n            of output `Value`, representing each output of the ONNX operator\n            in positional.\n    \"\"\"\n"
    },
    {
        "X": "How to use The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:, give an example?",
        "Z": " The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator elu\n",
        "Y": "UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator elu\n"
    },
    {
        "X": "How to use Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:, give an example?",
        "Z": " Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: def elu(g, input, alpha, inplace=False):\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))\n",
        "Y": "def elu(g, input, alpha, inplace=False):\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))\n"
    },
    {
        "X": "How to use Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:, give an example?",
        "Z": " Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: # Create custom symbolic function\nfrom torch.onnx.symbolic_helper import parse_args\n@parse_args('v', 'v', 'f', 'i')\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\n    return g.op(\"Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\n# Register custom symbolic function\nfrom torch.onnx import register_custom_op_symbolic\nregister_custom_op_symbolic('custom_ops::foo_forward', symbolic_foo_forward, 9)\n\nclass FooModel(torch.nn.Module):\n    def __init__(self, attr1, attr2):\n        super(FooModule, self).__init__()\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def forward(self, input1, input2):\n        # Calling custom op\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n\nmodel = FooModel(attr1, attr2)\ntorch.onnx.export(model, (dummy_input1, dummy_input2), 'model.onnx', custom_opsets={\"custom_domain\": 2})\n",
        "Y": "# Create custom symbolic function\nfrom torch.onnx.symbolic_helper import parse_args\n@parse_args('v', 'v', 'f', 'i')\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\n    return g.op(\"Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\n# Register custom symbolic function\nfrom torch.onnx import register_custom_op_symbolic\nregister_custom_op_symbolic('custom_ops::foo_forward', symbolic_foo_forward, 9)\n\nclass FooModel(torch.nn.Module):\n    def __init__(self, attr1, attr2):\n        super(FooModule, self).__init__()\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def forward(self, input1, input2):\n        # Calling custom op\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n\nmodel = FooModel(attr1, attr2)\ntorch.onnx.export(model, (dummy_input1, dummy_input2), 'model.onnx', custom_opsets={\"custom_domain\": 2})\n"
    },
    {
        "X": "How to use This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode., give an example?",
        "Z": " This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)\n    return (%2)\n",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)\n    return (%2)\n"
    },
    {
        "X": "How to use This mode is used to export all operators as ATen ops, and avoid conversion to ONNX., give an example?",
        "Z": " This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"exp\"](%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"div\"](%0, %1)\n    return (%2)\n",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"exp\"](%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"div\"](%0, %1)\n    return (%2)\n"
    },
    {
        "X": "How to use To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator., give an example?",
        "Z": " To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. Example torch ir graph:\n\n  graph(%0 : Float):\n    %3 : int = prim::Constant[value=0]()\n    %4 : Float = aten::triu(%0, %3) # unsupported op\n    %5 : Float = aten::mul(%4, %0) # registered op\n    return (%5)\n\nis exported as:\n\n  graph(%0 : Float):\n    %1 : Long() = onnx::Constant[value={0}]()\n    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1) # unsupported op\n    %3 : Float = onnx::Mul(%2, %0) # registered op\n    return (%3)\n",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float):\n    %3 : int = prim::Constant[value=0]()\n    %4 : Float = aten::triu(%0, %3) # unsupported op\n    %5 : Float = aten::mul(%4, %0) # registered op\n    return (%5)\n\nis exported as:\n\n  graph(%0 : Float):\n    %1 : Long() = onnx::Constant[value={0}]()\n    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1) # unsupported op\n    %3 : Float = onnx::Mul(%2, %0) # registered op\n    return (%3)\n"
    },
    {
        "X": "How to use To export a raw ir., give an example?",
        "Z": " To export a raw ir. Example torch ir graph:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n\nis exported as:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n",
        "Y": "Example torch ir graph:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n\nis exported as:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n"
    },
    {
        "X": "How to use This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\nExported falls through and exports the operator as is, as custom op. Exporting custom operators\nenables users to register and implement the operator as part of their runtime backend., give an example?",
        "Z": " This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\nExported falls through and exports the operator as is, as custom op. Exporting custom operators\nenables users to register and implement the operator as part of their runtime backend. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op\n    return (%7))\n\nis exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op\n    return (%3\n",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op\n    return (%7))\n\nis exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op\n    return (%3\n"
    },
    {
        "X": "How to use The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api., give an example?",
        "Z": " The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. layer_count = 4\n\nmodel = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)\nmodel.eval()\n\nwith torch.no_grad():\n    input = torch.randn(5, 3, 10)\n    h0 = torch.randn(layer_count * 2, 3, 20)\n    c0 = torch.randn(layer_count * 2, 3, 20)\n    output, (hn, cn) = model(input, (h0, c0))\n\n    # default export\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape [5, 3, 10]\n    print(onnx_model.graph.input[0])\n\n    # export with `dynamic_axes`\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',\n                    input_names=['input', 'h0', 'c0'],\n                    output_names=['output', 'hn', 'cn'],\n                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape ['sequence', 3, 10]\n    print(onnx_model.graph.input[0])\n",
        "Y": "layer_count = 4\n\nmodel = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)\nmodel.eval()\n\nwith torch.no_grad():\n    input = torch.randn(5, 3, 10)\n    h0 = torch.randn(layer_count * 2, 3, 20)\n    c0 = torch.randn(layer_count * 2, 3, 20)\n    output, (hn, cn) = model(input, (h0, c0))\n\n    # default export\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape [5, 3, 10]\n    print(onnx_model.graph.input[0])\n\n    # export with `dynamic_axes`\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',\n                    input_names=['input', 'h0', 'c0'],\n                    output_names=['output', 'hn', 'cn'],\n                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape ['sequence', 3, 10]\n    print(onnx_model.graph.input[0])\n"
    },
    {
        "X": "How to use No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future., give an example?",
        "Z": " No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. class ImplicitCastType(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        # Exporter knows x is float32, will export '2' as float32 as well.\n        y = x + 2\n        # Without type propagation, exporter doesn't know the datatype of y.\n        # Thus '3' is exported as int64 by default.\n        return y + 3\n        # The following will export correctly.\n        # return y + torch.tensor([3], dtype=torch.float32)\n\nx = torch.tensor([1.0], dtype=torch.float32)\ntorch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',\n                  example_outputs=ImplicitCastType()(x))\n",
        "Y": "class ImplicitCastType(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        # Exporter knows x is float32, will export '2' as float32 as well.\n        y = x + 2\n        # Without type propagation, exporter doesn't know the datatype of y.\n        # Thus '3' is exported as int64 by default.\n        return y + 3\n        # The following will export correctly.\n        # return y + torch.tensor([3], dtype=torch.float32)\n\nx = torch.tensor([1.0], dtype=torch.float32)\ntorch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',\n                  example_outputs=ImplicitCastType()(x))\n"
    },
    {
        "X": "How to use Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\nSimilar to list, Sequence is a data type that contains arbitrary number of Tensors.\nAssociated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\nHowever, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\nadd operator.\nE.g.:, give an example?",
        "Z": " Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\nSimilar to list, Sequence is a data type that contains arbitrary number of Tensors.\nAssociated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\nHowever, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\nadd operator.\nE.g.: class ListLoopModel(torch.nn.Module):\n    def forward(self, x):\n        res = []\n        res1 = []\n        arr = x.split(2, 0)\n        res2 = torch.zeros(3, 4, dtype=torch.long)\n        for i in range(len(arr)):\n            res += [arr[i].sum(0, False)]\n            res1 += [arr[-1 - i].sum(0, False)]\n            res2 += 1\n        return torch.stack(res), torch.stack(res1), res2\n\nmodel = torch.jit.script(ListLoopModel())\ninputs = torch.randn(16)\n\nout = model(inputs)\ntorch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('loop_and_list.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: inputs.numpy(),\n})\n\nassert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]\n",
        "Y": "class ListLoopModel(torch.nn.Module):\n    def forward(self, x):\n        res = []\n        res1 = []\n        arr = x.split(2, 0)\n        res2 = torch.zeros(3, 4, dtype=torch.long)\n        for i in range(len(arr)):\n            res += [arr[i].sum(0, False)]\n            res1 += [arr[-1 - i].sum(0, False)]\n            res2 += 1\n        return torch.stack(res), torch.stack(res1), res2\n\nmodel = torch.jit.script(ListLoopModel())\ninputs = torch.randn(16)\n\nout = model(inputs)\ntorch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('loop_and_list.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: inputs.numpy(),\n})\n\nassert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]\n"
    },
    {
        "X": "How to use use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model., give an example?",
        "Z": " use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. model = torchvision.models.mobilenet_v2(pretrained=True)\ninput = torch.randn(2, 3, 224, 224, requires_grad=True)\ntorch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)\n",
        "Y": "model = torchvision.models.mobilenet_v2(pretrained=True)\ninput = torch.randn(2, 3, 224, 224, requires_grad=True)\ntorch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)\n"
    },
    {
        "X": "How to use torch.onnx.export, give an example?",
        "Z": " ONLY A TUPLE OF ARGUMENTS or torch.Tensor: \"args = (x, y, z)\"\n",
        "Y": "\"args = (x, y, z)\"\n"
    },
    {
        "X": "How  A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:, give an example?",
        "Z": " A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: \"args = (x,\n        {\n        'y': input_y,\n        'z': input_z\n        })\"\n",
        "Y": "\"args = (x,\n        {\n        'y': input_y,\n        'z': input_z\n        })\"\n"
    },
    {
        "X": "How  , give an example?",
        "Z": "  >>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n",
        "Y": ">>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n"
    },
    {
        "X": "How  is exported as:, give an example?",
        "Z": " is exported as: graph(%x.1 : Long(1, strides=[1]))::\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\n  %y.1 : Long() = prim::ListConstruct(%1)\n  return (%y.1)\n",
        "Y": "graph(%x.1 : Long(1, strides=[1]))::\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\n  %y.1 : Long() = prim::ListConstruct(%1)\n  return (%y.1)\n"
    },
    {
        "X": "How to use Functions, give an example?",
        "Z": "  shape(input_1) = ('b', 3, 'w', 'h')\nand shape(input_2) = ('b', 4)\nand shape(output)  = ('b', 'd', 5)\n",
        "Y": "shape(input_1) = ('b', 3, 'w', 'h')\nand shape(input_2) = ('b', 4)\nand shape(output)  = ('b', 'd', 5)\n"
    },
    {
        "X": "How  ONLY INDICES:, give an example?",
        "Z": " ONLY INDICES: ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':[0],\n                  'output':[0, 1]}``\nwhere automatic names will be generated for exported dynamic axes\n",
        "Y": "``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':[0],\n                  'output':[0, 1]}``\nwhere automatic names will be generated for exported dynamic axes\n"
    },
    {
        "X": "How  INDICES WITH CORRESPONDING NAMES:, give an example?",
        "Z": " INDICES WITH CORRESPONDING NAMES: ``dynamic_axes = {'input_1':{0:'batch',\n                             1:'width',\n                             2:'height'},\n                  'input_2':{0:'batch'},\n                  'output':{0:'batch',\n                            1:'detections'}}``\nwhere provided names will be applied to exported dynamic axes\n",
        "Y": "``dynamic_axes = {'input_1':{0:'batch',\n                             1:'width',\n                             2:'height'},\n                  'input_2':{0:'batch'},\n                  'output':{0:'batch',\n                            1:'detections'}}``\nwhere provided names will be applied to exported dynamic axes\n"
    },
    {
        "X": "How  MIXED MODE OF (1) and (2):, give an example?",
        "Z": " MIXED MODE OF (1) and (2): ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':{0:'batch'},\n                  'output':[0,1]}``\n",
        "Y": "``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':{0:'batch'},\n                  'output':[0,1]}``\n"
    },
    {
        "X": "How to use torch.tril_indices, give an example?",
        "Z": " The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and below the main diagonal are\nretained. A positive value includes just as many diagonals above the main\ndiagonal, and similarly a negative value excludes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.tril_indices(3, 3)\n>>> a\ntensor([[0, 1, 1, 2, 2, 2],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, -1)\n>>> a\ntensor([[1, 2, 2, 3, 3, 3],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
        "Y": ">>> a = torch.tril_indices(3, 3)\n>>> a\ntensor([[0, 1, 1, 2, 2, 2],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, -1)\n>>> a\ntensor([[1, 2, 2, 3, 3, 3],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])\n"
    },
    {
        "X": "How to use torch.sub, give an example?",
        "Z": " Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs. >>> a = torch.tensor((1, 2))\n>>> b = torch.tensor((0, 1))\n>>> torch.sub(a, b, alpha=2)\ntensor([1, 0])\n",
        "Y": ">>> a = torch.tensor((1, 2))\n>>> b = torch.tensor((0, 1))\n>>> torch.sub(a, b, alpha=2)\ntensor([1, 0])\n"
    },
    {
        "X": "How to use torch.cuda.amp.autocast, give an example?",
        "Z": " autocast should wrap only the forward pass(es) of your network, including the loss\ncomputation(s).  Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward ops. # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n",
        "Y": "# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()\n"
    },
    {
        "X": "How to use autocast can also be used as a decorator, e.g., on the forward method of your model:, give an example?",
        "Z": " See the Automatic Mixed Precision examples for usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n",
        "Y": "class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...\n"
    },
    {
        "X": "How to use Floating-point Tensors produced in an autocast-enabled region may be float16.\nAfter returning to an autocast-disabled region, using them with floating-point\nTensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\nproduced in the autocast region back to float32 (or other dtype if desired).\nIf a Tensor from the autocast region is already float32, the cast is a no-op,\nand incurs no additional overhead.  Example:, give an example?",
        "Z": " autocast can also be used as a decorator, e.g., on the forward method of your model: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n",
        "Y": "# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())\n"
    },
    {
        "X": "How to use autocast(enabled=False) subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particular dtype.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast to dtype before use:, give an example?",
        "Z": " Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe,\nplease file an issue.autocast(enabled=False) subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particular dtype.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n",
        "Y": "# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)\n"
    },
    {
        "X": "How to use torch.cuda.amp.GradScaler.unscale_, give an example?",
        "Z": " unscale_() is optional, serving cases where you need to\nmodify or inspect gradients\nbetween the backward pass(es) and step().\nIf unscale_() is not called explicitly,  gradients will be unscaled  automatically during step().Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n",
        "Y": "...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()\n"
    },
    {
        "X": "How to use torch.logdet, give an example?",
        "Z": "  >>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(0.2611)\n>>> torch.logdet(A)\ntensor(-1.3430)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n>>> A.det().log()\ntensor([ 0.1815, -0.8917, -0.3031])\n",
        "Y": ">>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(0.2611)\n>>> torch.logdet(A)\ntensor(-1.3430)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n>>> A.det().log()\ntensor([ 0.1815, -0.8917, -0.3031])\n"
    },
    {
        "X": "How to use torch.lt, give an example?",
        "Z": " The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, False], [True, False]])\n",
        "Y": ">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, False], [True, False]])\n"
    },
    {
        "X": "How to use torch.quantize_per_tensor, give an example?",
        "Z": "  >>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)\n",
        "Y": ">>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)\n"
    },
    {
        "X": "How to use torch.square, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.square(a)\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.square(a)\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])\n"
    },
    {
        "X": "How to use torch.take, give an example?",
        "Z": "  >>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])\n",
        "Y": ">>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])\n"
    },
    {
        "X": "How to use torch.normal, give an example?",
        "Z": " The shapes of mean and std don\u2019t need to match, but the\ntotal number of elements in each tensor need to be the same. >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])\n",
        "Y": ">>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])\n"
    },
    {
        "X": "How  Similar to the function above, but the means are shared among all drawn\nelements., give an example?",
        "Z": " Similar to the function above, but the means are shared among all drawn\nelements. >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n",
        "Y": ">>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n"
    },
    {
        "X": "How  Similar to the function above, but the standard deviations are shared among\nall drawn elements., give an example?",
        "Z": " Similar to the function above, but the standard deviations are shared among\nall drawn elements. >>> torch.normal(mean=torch.arange(1., 6.))\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n",
        "Y": ">>> torch.normal(mean=torch.arange(1., 6.))\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n"
    },
    {
        "X": "How  Similar to the function above, but the means and standard deviations are shared\namong all drawn elements. The resulting tensor has size given by size., give an example?",
        "Z": " Similar to the function above, but the means and standard deviations are shared\namong all drawn elements. The resulting tensor has size given by size. >>> torch.normal(2, 3, size=(1, 4))\ntensor([[-1.3987, -1.9544,  3.6048,  0.7909]])\n",
        "Y": ">>> torch.normal(2, 3, size=(1, 4))\ntensor([[-1.3987, -1.9544,  3.6048,  0.7909]])\n"
    },
    {
        "X": "How to use torch.std_mean, give an example?",
        "Z": " If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std_mean(a, unbiased=False)\n(tensor(0.4188), tensor(-0.8509))\n",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std_mean(a, unbiased=False)\n(tensor(0.4188), tensor(-0.8509))\n"
    },
    {
        "X": "How to use torch.swapdims, give an example?",
        "Z": " This function is equivalent to NumPy\u2019s swapaxes function. >>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])\n",
        "Y": ">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])\n"
    },
    {
        "X": "How to use torch.dstack, give an example?",
        "Z": " This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d(). >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.dstack((a,b))\ntensor([[[1, 4],\n         [2, 5],\n         [3, 6]]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.dstack((a,b))\ntensor([[[1, 4]],\n        [[2, 5]],\n        [[3, 6]]])\n",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.dstack((a,b))\ntensor([[[1, 4],\n         [2, 5],\n         [3, 6]]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.dstack((a,b))\ntensor([[[1, 4]],\n        [[2, 5]],\n        [[3, 6]]])\n"
    },
    {
        "X": "How to use torch.special.entr, give an example?",
        "Z": "  >>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])\n",
        "Y": ">>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])\n"
    },
    {
        "X": "How to use torch.special.erf, give an example?",
        "Z": "  >>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n",
        "Y": ">>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])\n"
    },
    {
        "X": "How to use torch.special.erfc, give an example?",
        "Z": "  >>> torch.special.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n",
        "Y": ">>> torch.special.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])\n"
    },
    {
        "X": "How to use torch.special.erfinv, give an example?",
        "Z": "  >>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n",
        "Y": ">>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])\n"
    },
    {
        "X": "How to use torch.special.expit, give an example?",
        "Z": "  >>> t = torch.randn(4)\n>>> t\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.special.expit(t)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n",
        "Y": ">>> t = torch.randn(4)\n>>> t\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.special.expit(t)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])\n"
    },
    {
        "X": "How to use torch.special.expm1, give an example?",
        "Z": "  >>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n",
        "Y": ">>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])\n"
    },
    {
        "X": "How to use torch.special.exp2, give an example?",
        "Z": "  >>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])\n",
        "Y": ">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])\n"
    },
    {
        "X": "How to use torch.special.gammaln, give an example?",
        "Z": "  >>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n",
        "Y": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n"
    },
    {
        "X": "How to use torch.special.i0e, give an example?",
        "Z": "  >>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\n",
        "Y": ">>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])\n"
    },
    {
        "X": "How to use torch.special.logit, give an example?",
        "Z": "  >>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\n",
        "Y": ">>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])\n"
    },
    {
        "X": "How to use torch.special.xlog1py, give an example?",
        "Z": " Similar to SciPy\u2019s scipy.special.xlog1py. >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])\n",
        "Y": ">>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])\n"
    },
    {
        "X": "How to use torch.isneginf, give an example?",
        "Z": "  >>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isneginf(a)\ntensor([ True, False, False])\n",
        "Y": ">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isneginf(a)\ntensor([ True, False, False])\n"
    },
    {
        "X": "How to use torch.tensordot, give an example?",
        "Z": " When called with dims of the list form, the given dimensions will be contracted\nin place of the last ddd of a and the first ddd of bbb. The sizes\nin these dimensions must match, but tensordot() will deal with broadcasted\ndimensions. >>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[  7.7193,  -2.4867, -10.3204],\n        [  1.5513, -14.4737,  -6.5113],\n        [ -0.2850,   4.2573,  -3.5997]])\n",
        "Y": ">>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[  7.7193,  -2.4867, -10.3204],\n        [  1.5513, -14.4737,  -6.5113],\n        [ -0.2850,   4.2573,  -3.5997]])\n"
    },
    {
        "X": "How to use torch.clamp, give an example?",
        "Z": " If min is None, there is no lower bound.\nOr, if max is None there is no upper bound. >>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n>>> min = torch.linspace(-1, 1, steps=4)\n>>> torch.clamp(a, min=min)\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n>>> min = torch.linspace(-1, 1, steps=4)\n>>> torch.clamp(a, min=min)\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])\n"
    },
    {
        "X": "How to use torch.renorm, give an example?",
        "Z": "  >>> x = torch.ones(3, 3)\n>>> x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n>>> x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n>>> x\ntensor([[ 1.,  1.,  1.],\n        [ 2.,  2.,  2.],\n        [ 3.,  3.,  3.]])\n>>> torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n        [ 1.6667,  1.6667,  1.6667],\n        [ 1.6667,  1.6667,  1.6667]])\n",
        "Y": ">>> x = torch.ones(3, 3)\n>>> x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n>>> x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n>>> x\ntensor([[ 1.,  1.,  1.],\n        [ 2.,  2.,  2.],\n        [ 3.,  3.,  3.]])\n>>> torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n        [ 1.6667,  1.6667,  1.6667],\n        [ 1.6667,  1.6667,  1.6667]])\n"
    },
    {
        "X": "How to use torch.logcumsumexp, give an example?",
        "Z": " For summation index jjj given by dim and other indices iii, the result is >>> a = torch.randn(10)\n>>> torch.logcumsumexp(a, dim=0)\ntensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,\n         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))\n",
        "Y": ">>> a = torch.randn(10)\n>>> torch.logcumsumexp(a, dim=0)\ntensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,\n         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))\n"
    },
    {
        "X": "How to use torch.cat, give an example?",
        "Z": " torch.cat() can be best understood via examples. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])\n",
        "Y": ">>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])\n"
    },
    {
        "X": "How to use torch.cdist, give an example?",
        "Z": " This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p)\nif p\u2208(0,\u221e)p \\in (0, \\infty)p\u2208(0,\u221e). When p=0p = 0p=0 it is equivalent to\nscipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\inftyp=\u221e, the closest\nscipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()). >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])\n",
        "Y": ">>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])\n"
    },
    {
        "X": "How to use torch.index_select, give an example?",
        "Z": " The returned tensor has the same number of dimensions as the original tensor\n(input).  The dimth dimension has the same size as the length\nof index; other dimensions have the same size as in the original tensor. >>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-0.4664,  0.2647, -0.1228, -1.1068],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> indices = torch.tensor([0, 2])\n>>> torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n        [-0.4664, -0.1228],\n        [-1.1734,  0.7230]])\n",
        "Y": ">>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-0.4664,  0.2647, -0.1228, -1.1068],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> indices = torch.tensor([0, 2])\n>>> torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n        [-0.4664, -0.1228],\n        [-1.1734,  0.7230]])\n"
    },
    {
        "X": "How  A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported., give an example?",
        "Z": " A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported. >>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n",
        "Y": ">>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32\n"
    },
    {
        "X": "How  A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?",
        "Z": " A torch.device can be constructed via a string or via a string and device ordinalVia a string: >>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n",
        "Y": ">>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')\n"
    },
    {
        "X": "How  Via a string:Via a string and device ordinal:, give an example?",
        "Z": " Via a string:Via a string and device ordinal: >>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n",
        "Y": ">>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)\n"
    },
    {
        "X": "How  For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors., give an example?",
        "Z": " For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. >>> torch.device(1)\ndevice(type='cuda', index=1)\n",
        "Y": ">>> torch.device(1)\ndevice(type='cuda', index=1)\n"
    },
    {
        "X": "How  Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?",
        "Z": " Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: >>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy\n",
        "Y": ">>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy\n"
    },
    {
        "X": "How  torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently., give an example?",
        "Z": " torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n",
        "Y": ">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)\n"
    },
    {
        "X": "How to use torch.cholesky_solve, give an example?",
        "Z": " Supports real-valued and complex-valued inputs.\nFor the complex-valued inputs the transpose operator above is the conjugate transpose. >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n"
    },
    {
        "X": "How to use torch.argmin, give an example?",
        "Z": " This is the second value returned by torch.min(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])\n"
    },
    {
        "X": "How to use where \u03b8\\theta\u03b8 are the parameters, \u03b1\\alpha\u03b1 is the learning rate,\nrrr is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))p(a\u2223\u03c0\u03b8(s)) is the probability of\ntaking action aaa in state sss given policy \u03c0\u03b8\\pi^\\theta\u03c0\u03b8.In practice we would sample an action from the output of a network, apply this\naction in an environment, and then use log_prob to construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows:, give an example?",
        "Z": " where \u03b8\\theta\u03b8 are the parameters, \u03b1\\alpha\u03b1 is the learning rate,\nrrr is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))p(a\u2223\u03c0\u03b8(s)) is the probability of\ntaking action aaa in state sss given policy \u03c0\u03b8\\pi^\\theta\u03c0\u03b8.In practice we would sample an action from the output of a network, apply this\naction in an environment, and then use log_prob to construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows: probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n",
        "Y": "probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()\n"
    },
    {
        "X": "How to use The other way to implement these stochastic/policy gradients would be to use the\nreparameterization trick from the\nrsample() method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The reparameterized\nsample therefore becomes differentiable. The code for implementing the pathwise\nderivative would be as follows:, give an example?",
        "Z": " The other way to implement these stochastic/policy gradients would be to use the\nreparameterization trick from the\nrsample() method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The reparameterized\nsample therefore becomes differentiable. The code for implementing the pathwise\nderivative would be as follows: params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n",
        "Y": "params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()\n"
    },
    {
        "X": "How to use torch.distributions.bernoulli.Bernoulli, give an example?",
        "Z": " Samples are binary (0 or 1). They take the value 1 with probability p\nand 0 with probability 1 - p. >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n",
        "Y": ">>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])\n"
    },
    {
        "X": "How to use torch.distributions.beta.Beta, give an example?",
        "Z": " Beta distribution parameterized by concentration1 and concentration0. >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n",
        "Y": ">>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use torch.distributions.binomial.Binomial, give an example?",
        "Z": " Creates a Binomial distribution parameterized by total_count and\neither probs or logits (but not both). total_count must be\nbroadcastable with probs/logits. >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n",
        "Y": ">>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])\n"
    },
    {
        "X": "How to use torch.distributions.categorical.Categorical, give an example?",
        "Z": " See also: torch.multinomial() >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n",
        "Y": ">>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)\n"
    },
    {
        "X": "How to use torch.distributions.cauchy.Cauchy, give an example?",
        "Z": " Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means 0 follows a\nCauchy distribution. >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n",
        "Y": ">>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])\n"
    },
    {
        "X": "How to use torch.distributions.chi2.Chi2, give an example?",
        "Z": " Creates a Chi2 distribution parameterized by shape parameter df.\nThis is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use torch.distributions.continuous_bernoulli.ContinuousBernoulli, give an example?",
        "Z": " The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to\nlog-odds, but the same names are used due to the similarity with the\nBernoulli. See [1] for more details. >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n",
        "Y": ">>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])\n"
    },
    {
        "X": "How to use torch.distributions.dirichlet.Dirichlet, give an example?",
        "Z": " Creates a Dirichlet distribution parameterized by concentration concentration. >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n",
        "Y": ">>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])\n"
    },
    {
        "X": "How to use torch.distributions.exponential.Exponential, give an example?",
        "Z": " Creates a Exponential distribution parameterized by rate. >>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use torch.distributions.fishersnedecor.FisherSnedecor, give an example?",
        "Z": " Creates a Fisher-Snedecor distribution parameterized by df1 and df2. >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n",
        "Y": ">>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])\n"
    },
    {
        "X": "How to use torch.distributions.gamma.Gamma, give an example?",
        "Z": " Creates a Gamma distribution parameterized by shape concentration and rate. >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use torch.distributions.geometric.Geometric, give an example?",
        "Z": " Samples are non-negative integers [0, inf\u2061\\infinf). >>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n",
        "Y": ">>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])\n"
    },
    {
        "X": "How to use torch.distributions.gumbel.Gumbel, give an example?",
        "Z": " Samples from a Gumbel Distribution. >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n",
        "Y": ">>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])\n"
    },
    {
        "X": "How to use Creates a half-Cauchy distribution parameterized by scale where:, give an example?",
        "Z": " Creates a half-Cauchy distribution parameterized by scale where: X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n",
        "Y": "X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)\n"
    },
    {
        "X": "How to use torch.distributions.half_cauchy.HalfCauchy, give an example?",
        "Z": " Creates a half-Cauchy distribution parameterized by scale where: >>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n",
        "Y": ">>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])\n"
    },
    {
        "X": "How to use Creates a half-normal distribution parameterized by scale where:, give an example?",
        "Z": " Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n",
        "Y": "X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)\n"
    },
    {
        "X": "How to use torch.distributions.half_normal.HalfNormal, give an example?",
        "Z": " Creates a half-normal distribution parameterized by scale where: >>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use This is mainly useful for changing the shape of the result of\nlog_prob(). For example to create a diagonal Normal distribution with\nthe same shape as a Multivariate Normal distribution (so they are\ninterchangeable), you can:, give an example?",
        "Z": " Reinterprets some of the batch dims of a distribution as event dims.This is mainly useful for changing the shape of the result of\nlog_prob(). For example to create a diagonal Normal distribution with\nthe same shape as a Multivariate Normal distribution (so they are\ninterchangeable), you can: >>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n",
        "Y": ">>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n"
    },
    {
        "X": "How to use torch.distributions.kumaraswamy.Kumaraswamy, give an example?",
        "Z": " Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n",
        "Y": ">>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])\n"
    },
    {
        "X": "How to use torch.distributions.lkj_cholesky.LKJCholesky, give an example?",
        "Z": " LKJ distribution for lower Cholesky factor of correlation matrices.\nThe distribution is controlled by concentration parameter \u03b7\\eta\u03b7\nto make the probability of the correlation matrix MMM generated from\na Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1}det(M)\u03b7\u22121. Because of that,\nwhen concentration == 1, we have a uniform distribution over Cholesky\nfactors of correlation matrices. Note that this distribution samples the\nCholesky factor of correlation matrices and not the correlation matrices\nthemselves and thereby differs slightly from the derivations in [1] for\nthe LKJCorr distribution. For sampling, this uses the Onion method from\n[1] Section 3. >>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n",
        "Y": ">>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])\n"
    },
    {
        "X": "How to use torch.distributions.laplace.Laplace, give an example?",
        "Z": " Creates a Laplace distribution parameterized by loc and scale. >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use Creates a log-normal distribution parameterized by\nloc and scale where:, give an example?",
        "Z": " Creates a log-normal distribution parameterized by\nloc and scale where: X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n",
        "Y": "X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)\n"
    },
    {
        "X": "How to use torch.distributions.log_normal.LogNormal, give an example?",
        "Z": " Creates a log-normal distribution parameterized by\nloc and scale where: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use Creates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized by cov_factor and cov_diag:, give an example?",
        "Z": " Creates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n",
        "Y": "covariance_matrix = cov_factor @ cov_factor.T + cov_diag\n"
    },
    {
        "X": "How to use torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal, give an example?",
        "Z": " Creates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized by cov_factor and cov_diag: >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n",
        "Y": ">>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])\n"
    },
    {
        "X": "How to use The computation for determinant and inverse of covariance matrix is avoided when\ncov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and\nmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix:, give an example?",
        "Z": " The computation for determinant and inverse of covariance matrix is avoided when\ncov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and\nmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n",
        "Y": "capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor\n"
    },
    {
        "X": "How to use torch.distributions.mixture_same_family.MixtureSameFamily, give an example?",
        "Z": " The MixtureSameFamily distribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of\nthe same distribution type. It is parameterized by a Categorical\n\u201cselecting distribution\u201d (over k component) and a component\ndistribution, i.e., a Distribution with a rightmost batch shape\n(equal to [k]) which indexes each (batch of) component. # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n",
        "Y": "# Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n"
    },
    {
        "X": "How to use torch.distributions.multinomial.Multinomial, give an example?",
        "Z": " Note that total_count need not be specified if only log_prob() is\ncalled (see example below) >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n",
        "Y": ">>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])\n"
    },
    {
        "X": "How to use torch.distributions.multivariate_normal.MultivariateNormal, give an example?",
        "Z": " The multivariate normal distribution can be parameterized either\nin terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}\u03a3\nor a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121\nor a lower-triangular matrix L\\mathbf{L}L with positive-valued\ndiagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4. This triangular matrix\ncan be obtained via e.g. Cholesky decomposition of the covariance. >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n",
        "Y": ">>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])\n"
    },
    {
        "X": "How to use torch.distributions.normal.Normal, give an example?",
        "Z": " Creates a normal (also called Gaussian) distribution parameterized by\nloc and scale. >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n",
        "Y": ">>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use torch.distributions.one_hot_categorical.OneHotCategorical, give an example?",
        "Z": " See also: torch.distributions.Categorical() for specifications of\nprobs and logits. >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n",
        "Y": ">>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])\n"
    },
    {
        "X": "How to use torch.distributions.pareto.Pareto, give an example?",
        "Z": " Samples from a Pareto Type 1 distribution. >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n",
        "Y": ">>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])\n"
    },
    {
        "X": "How to use torch.distributions.poisson.Poisson, give an example?",
        "Z": " Samples are nonnegative integers, with a pmf given by >>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n",
        "Y": ">>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])\n"
    },
    {
        "X": "How to use torch.distributions.relaxed_bernoulli.RelaxedBernoulli, give an example?",
        "Z": " Creates a RelaxedBernoulli distribution, parametrized by\ntemperature, and either probs or logits\n(but not both). This is a relaxed version of the Bernoulli distribution,\nso the values are in (0, 1), and has reparametrizable samples. >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n",
        "Y": ">>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])\n"
    },
    {
        "X": "How to use torch.distributions.relaxed_categorical.RelaxedOneHotCategorical, give an example?",
        "Z": " Creates a RelaxedOneHotCategorical distribution parametrized by\ntemperature, and either probs or logits.\nThis is a relaxed version of the OneHotCategorical distribution, so\nits samples are on simplex, and are reparametrizable. >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n",
        "Y": ">>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])\n"
    },
    {
        "X": "How to use torch.distributions.studentT.StudentT, give an example?",
        "Z": " Creates a Student\u2019s t-distribution parameterized by degree of\nfreedom df, mean loc and scale scale. >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n",
        "Y": ">>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])\n"
    },
    {
        "X": "How to use Extension of the Distribution class, which applies a sequence of Transforms\nto a base distribution.  Let f be the composition of transforms applied:, give an example?",
        "Z": " Extension of the Distribution class, which applies a sequence of Transforms\nto a base distribution.  Let f be the composition of transforms applied: X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n",
        "Y": "X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|\n"
    },
    {
        "X": "How to use An example for the usage of TransformedDistribution would be:, give an example?",
        "Z": " Note that the .event_shape of a TransformedDistribution is the\nmaximum shape of its base distribution and its transforms, since transforms\ncan introduce correlations among events.An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n",
        "Y": "# Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)\n"
    },
    {
        "X": "How to use torch.distributions.uniform.Uniform, give an example?",
        "Z": " Generates uniformly distributed random samples from the half-open interval\n[low, high). >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n",
        "Y": ">>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])\n"
    },
    {
        "X": "How to use VonMises, give an example?",
        "Z": "  >>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n",
        "Y": ">>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])\n"
    },
    {
        "X": "How to use torch.distributions.weibull.Weibull, give an example?",
        "Z": " Samples from a two-parameter Weibull distribution. >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n",
        "Y": ">>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])\n"
    },
    {
        "X": "How to use torch.distributions.kl.register_kl, give an example?",
        "Z": " Decorator to register a pairwise function with kl_divergence().\nUsage: @register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n",
        "Y": "@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here\n"
    },
    {
        "X": "How  Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation:, give an example?",
        "Z": " Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation: @register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n",
        "Y": "@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...\n"
    },
    {
        "X": "How  Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation:you should register a third most-specific implementation, e.g.:, give an example?",
        "Z": " Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation:you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n",
        "Y": "register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.\n"
    },
    {
        "X": "How to use Caching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching:, give an example?",
        "Z": " Caching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching: y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n",
        "Y": "y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.\n"
    },
    {
        "X": "How to use However the following will error when caching due to dependency reversal:, give an example?",
        "Z": " Caching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching:However the following will error when caching due to dependency reversal: y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n",
        "Y": "y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x\n"
    },
    {
        "X": "How to use PyTorch provides two global ConstraintRegistry objects that link\nConstraint objects to\nTransform objects. These objects both\ninput constraints and return transforms, but they have different guarantees on\nbijectivity.The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:, give an example?",
        "Z": " The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam: loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n",
        "Y": "loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()\n"
    },
    {
        "X": "How to use The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:, give an example?",
        "Z": " The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.: dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n",
        "Y": "dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()\n"
    },
    {
        "X": "How to use The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints:, give an example?",
        "Z": " The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints: transform_to.register(my_constraint, my_transform)\n",
        "Y": "transform_to.register(my_constraint, my_transform)\n"
    },
    {
        "X": "How to use The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints:or as a decorator on parameterized constraints:, give an example?",
        "Z": " The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints:or as a decorator on parameterized constraints: @transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n",
        "Y": "@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)\n"
    },
    {
        "X": "How to use torch.distributions.constraint_registry.ConstraintRegistry.register, give an example?",
        "Z": " Registers a Constraint\nsubclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n",
        "Y": "@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)\n"
    },
    {
        "X": "How to use torch.quasirandom.SobolEngine, give an example?",
        "Z": " References >>> soboleng = torch.quasirandom.SobolEngine(dimension=5)\n>>> soboleng.draw(3)\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])\n",
        "Y": ">>> soboleng = torch.quasirandom.SobolEngine(dimension=5)\n>>> soboleng.draw(3)\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])\n"
    },
    {
        "X": "How to use torch.unique_consecutive, give an example?",
        "Z": "  >>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n>>> output = torch.unique_consecutive(x)\n>>> output\ntensor([1, 2, 3, 1, 2])\n\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> counts\ntensor([2, 2, 1, 2, 1])\n",
        "Y": ">>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n>>> output = torch.unique_consecutive(x)\n>>> output\ntensor([1, 2, 3, 1, 2])\n\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> counts\ntensor([2, 2, 1, 2, 1])\n"
    },
    {
        "X": "How to use torch.signbit, give an example?",
        "Z": "  >>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> torch.signbit(a)\ntensor([ False, True,  False,  False])\n",
        "Y": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> torch.signbit(a)\ntensor([ False, True,  False,  False])\n"
    },
    {
        "X": "How to use torch.isinf, give an example?",
        "Z": "  >>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])\n",
        "Y": ">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])\n"
    },
    {
        "X": "How to use torch.log1p, give an example?",
        "Z": "  >>> a = torch.randn(5)\n>>> a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n>>> torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\n",
        "Y": ">>> a = torch.randn(5)\n>>> a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n>>> torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])\n"
    },
    {
        "X": "How to use torch.randperm, give an example?",
        "Z": "  >>> torch.randperm(4)\ntensor([2, 1, 0, 3])\n",
        "Y": ">>> torch.randperm(4)\ntensor([2, 1, 0, 3])\n"
    },
    {
        "X": "How to use torch.diff, give an example?",
        "Z": " The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order\ndifferences are calculated by using torch.diff() recursively. >>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])\n",
        "Y": ">>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])\n"
    },
    {
        "X": "How to use torch.bernoulli, give an example?",
        "Z": " out can have integral dtype, but input must have floating\npoint dtype. >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n",
        "Y": ">>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n"
    },
    {
        "X": "How to use torch.amin, give an example?",
        "Z": " If keepdim is True, the output tensors are of the same size as\ninput except in the dimension(s) dim where they are of size 1.\nOtherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in\nthe output tensors having fewer dimensions than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])\n"
    },
    {
        "X": "How to use torch.equal, give an example?",
        "Z": "  >>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue\n",
        "Y": ">>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue\n"
    },
    {
        "X": "How to use torch.logical_not, give an example?",
        "Z": "  >>> torch.logical_not(torch.tensor([True, False]))\ntensor([False,  True])\n>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\ntensor([1, 0, 0], dtype=torch.int16)\n",
        "Y": ">>> torch.logical_not(torch.tensor([True, False]))\ntensor([False,  True])\n>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\ntensor([1, 0, 0], dtype=torch.int16)\n"
    },
    {
        "X": "How to use torch.acos, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])\n"
    },
    {
        "X": "How to use torch.abs, give an example?",
        "Z": "  >>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n",
        "Y": ">>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])\n"
    },
    {
        "X": "How to use torch.from_numpy, give an example?",
        "Z": " It currently accepts ndarray with dtypes of numpy.float64,\nnumpy.float32, numpy.float16, numpy.complex64, numpy.complex128,\nnumpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8,\nand numpy.bool. >>> a = numpy.array([1, 2, 3])\n>>> t = torch.from_numpy(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n",
        "Y": ">>> a = numpy.array([1, 2, 3])\n>>> t = torch.from_numpy(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n"
    },
    {
        "X": "How to use torch.ravel, give an example?",
        "Z": "  >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.ravel(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "Y": ">>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.ravel(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n"
    },
    {
        "X": "How to use torch.sinc, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.sinc(a)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.sinc(a)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])\n"
    },
    {
        "X": "How to use String to distinguish measurements with identical label and\nsub_label. The principal use of description is to signal to\nCompare the columns of data. For instance one might set it\nbased on the input size  to create a table of the form:, give an example?",
        "Z": " String to distinguish measurements with identical label and\nsub_label. The principal use of description is to signal to\nCompare the columns of data. For instance one might set it\nbased on the input size  to create a table of the form:                         | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\n",
        "Y": "                        | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...\n"
    },
    {
        "X": "How to use torch.utils.benchmark.Timer.blocked_autorange, give an example?",
        "Z": " At a high level, blocked_autorange executes the following pseudo-code: `setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n",
        "Y": "`setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)\n"
    },
    {
        "X": "How to use torch.utils.benchmark.CallgrindStats.as_standardized, give an example?",
        "Z": " When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: 23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n",
        "Y": "23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)\n"
    },
    {
        "X": "How to use torch.save, give an example?",
        "Z": " See also: Saving and loading tensors >>> # Save to file\n>>> x = torch.tensor([0, 1, 2, 3, 4])\n>>> torch.save(x, 'tensor.pt')\n>>> # Save to io.BytesIO buffer\n>>> buffer = io.BytesIO()\n>>> torch.save(x, buffer)\n",
        "Y": ">>> # Save to file\n>>> x = torch.tensor([0, 1, 2, 3, 4])\n>>> torch.save(x, 'tensor.pt')\n>>> # Save to io.BytesIO buffer\n>>> buffer = io.BytesIO()\n>>> torch.save(x, buffer)\n"
    },
    {
        "X": "How to use torch.set_grad_enabled, give an example?",
        "Z": " This context manager is thread local; it will not affect computation\nin other threads. >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n",
        "Y": ">>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse\n"
    },
    {
        "X": "How to use torch.combinations, give an example?",
        "Z": "  >>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n",
        "Y": ">>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])\n"
    },
    {
        "X": "How to use torch.mv, give an example?",
        "Z": " If input is a (n\u00d7m)(n \\times m)(n\u00d7m) tensor, vec is a 1-D tensor of\nsize mmm, out will be 1-D of size nnn. >>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])\n",
        "Y": ">>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])\n"
    },
    {
        "X": "How to use torch.amax, give an example?",
        "Z": " If keepdim is ``True`, the output tensors are of the same size\nas input except in the dimension(s) dim where they are of size 1.\nOtherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting\nin the output tensors having fewer dimension than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])\n"
    },
    {
        "X": "How to use torch.numel, give an example?",
        "Z": "  >>> a = torch.randn(1, 2, 3, 4, 5)\n>>> torch.numel(a)\n120\n>>> a = torch.zeros(4,4)\n>>> torch.numel(a)\n16\n",
        "Y": ">>> a = torch.randn(1, 2, 3, 4, 5)\n>>> torch.numel(a)\n120\n>>> a = torch.zeros(4,4)\n>>> torch.numel(a)\n16\n"
    },
    {
        "X": "How to use torch.logspace, give an example?",
        "Z": "  >>> torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])\n",
        "Y": ">>> torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])\n"
    },
    {
        "X": "How to use torch.acosh, give an example?",
        "Z": "  >>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])\n",
        "Y": ">>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])\n"
    },
    {
        "X": "How to use The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?",
        "Z": " The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: $ unzip my_package.pt && tree my_package\nmy_package\n\u251c\u2500\u2500 .data\n\u2502   \u251c\u2500\u2500 94304870911616.storage\n\u2502   \u251c\u2500\u2500 94304900784016.storage\n\u2502   \u251c\u2500\u2500 extern_modules\n\u2502   \u2514\u2500\u2500 version\n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 model_1.pkl\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...\n",
        "Y": "$ unzip my_package.pt && tree my_package\nmy_package\n\u251c\u2500\u2500 .data\n\u2502   \u251c\u2500\u2500 94304870911616.storage\n\u2502   \u251c\u2500\u2500 94304900784016.storage\n\u2502   \u251c\u2500\u2500 extern_modules\n\u2502   \u2514\u2500\u2500 version\n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 model_1.pkl\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...\n"
    },
    {
        "X": "How  The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?",
        "Z": " The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: # add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt\n",
        "Y": "# add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt\n"
    },
    {
        "X": "How to use PackageImporter and PackageExporter provide a file_structure() method, which will return a printable\nand queryable Folder object. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments., give an example?",
        "Z": " The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments. with PackageExporter('my_package.pt', verbose=False) as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n    # can limit printed items with include/exclude args\n    print(pe.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"))\n\nimporter = PackageImporter('my_package.pt')\nprint(importer.file_structure()) # will print out all files\n",
        "Y": "with PackageExporter('my_package.pt', verbose=False) as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n    # can limit printed items with include/exclude args\n    print(pe.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"))\n\nimporter = PackageImporter('my_package.pt')\nprint(importer.file_structure()) # will print out all files\n"
    },
    {
        "X": "How to use The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments.Output:, give an example?",
        "Z": " The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments.Output: # filtered with glob pattern:\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u2514\u2500\u2500 utils.py\n\n# all files\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 .data\n    \u2502   \u251c\u2500\u2500 94304870911616.storage\n    \u2502   \u251c\u2500\u2500 94304900784016.storage\n    \u2502   \u251c\u2500\u2500 extern_modules\n    \u2502   \u2514\u2500\u2500 version\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 resnet.py\n            \u2514\u2500\u2500 utils.py\n",
        "Y": "# filtered with glob pattern:\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u2514\u2500\u2500 utils.py\n\n# all files\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 .data\n    \u2502   \u251c\u2500\u2500 94304870911616.storage\n    \u2502   \u251c\u2500\u2500 94304900784016.storage\n    \u2502   \u251c\u2500\u2500 extern_modules\n    \u2502   \u2514\u2500\u2500 version\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 resnet.py\n            \u2514\u2500\u2500 utils.py\n"
    },
    {
        "X": "How to use Output:You can also query Folder objects with the has_file() method., give an example?",
        "Z": " Output:You can also query Folder objects with the has_file() method. exporter_file_structure = exporter.file_structure()\nfound: bool = exporter_file_structure.has_file(\"package_a/subpackage.py\")\n",
        "Y": "exporter_file_structure = exporter.file_structure()\nfound: bool = exporter_file_structure.has_file(\"package_a/subpackage.py\")\n"
    },
    {
        "X": "How to use PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save\nPython objects, text, and binary data to a package., give an example?",
        "Z": " PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save\nPython objects, text, and binary data to a package. with torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tens.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)\n",
        "Y": "with torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tens.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)\n"
    },
    {
        "X": "How to use PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save\nPython objects, text, and binary data to a package.PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load\nPython objects, text and binary data from a package., give an example?",
        "Z": " PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load\nPython objects, text and binary data from a package. importer = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")\n",
        "Y": "importer = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")\n"
    },
    {
        "X": "How to use torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method\n__reduce_package__ on a class and by defining a corresponding de-packaging function. This is similar to defining __reduce__ for\nPython\u2019s normal pickling process.Steps:, give an example?",
        "Z": " Steps: # foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo\n",
        "Y": "# foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo\n"
    },
    {
        "X": "How  Steps:, give an example?",
        "Z": " Steps: # output of running above script\n\u2500\u2500\u2500 foo_package\n    \u251c\u2500\u2500 foo-generated\n    \u2502   \u251c\u2500\u2500 _0\n    \u2502   \u2502   \u2514\u2500\u2500 foo.txt\n    \u2502   \u2514\u2500\u2500 _1\n    \u2502       \u2514\u2500\u2500 foo.txt\n    \u251c\u2500\u2500 foo_collection\n    \u2502   \u251c\u2500\u2500 foo1.pkl\n    \u2502   \u2514\u2500\u2500 foo2.pkl\n    \u2514\u2500\u2500 foo.py\n\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\nfoo_1 export time: 9857706.650140837\nfoo_1 import time: 9857706.652698385\n",
        "Y": "# output of running above script\n\u2500\u2500\u2500 foo_package\n    \u251c\u2500\u2500 foo-generated\n    \u2502   \u251c\u2500\u2500 _0\n    \u2502   \u2502   \u2514\u2500\u2500 foo.txt\n    \u2502   \u2514\u2500\u2500 _1\n    \u2502       \u2514\u2500\u2500 foo.txt\n    \u251c\u2500\u2500 foo_collection\n    \u2502   \u251c\u2500\u2500 foo1.pkl\n    \u2502   \u2514\u2500\u2500 foo2.pkl\n    \u2514\u2500\u2500 foo.py\n\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\nfoo_1 export time: 9857706.650140837\nfoo_1 import time: 9857706.652698385\n"
    },
    {
        "X": "How to use A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not., give an example?",
        "Z": " A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. # In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException\n",
        "Y": "# In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException\n"
    },
    {
        "X": "How to use A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a\ntorch.package., give an example?",
        "Z": " Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a\ntorch.package. from foo.bar import is_in_package\n\nprint(is_in_package())  # False\n\nloaded_module = PackageImporter(my_pacakge).import_module(\"foo.bar\")\nloaded_module.is_in_package()  # True\n",
        "Y": "from foo.bar import is_in_package\n\nprint(is_in_package())  # False\n\nloaded_module = PackageImporter(my_pacakge).import_module(\"foo.bar\")\nloaded_module.is_in_package()  # True\n"
    },
    {
        "X": "How to use PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing., give an example?",
        "Z": " PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing. with PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\\\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'\n",
        "Y": "with PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\\\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'\n"
    },
    {
        "X": "How to use PackageImporter implements the\nimportlib.resources\nAPI for accessing resources from inside a package., give an example?",
        "Z": " PackageImporter implements the\nimportlib.resources\nAPI for accessing resources from inside a package. with PackageExporter(f) as exporter:\n    # saves text to one/a.txt in the archive\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n    # saves the tensor to my_pickle/obj.pkl\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n    # see below for module contents\n    exporter.save_module(\"foo\")\n    exporter.save_module(\"bar\")\n",
        "Y": "with PackageExporter(f) as exporter:\n    # saves text to one/a.txt in the archive\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n    # saves the tensor to my_pickle/obj.pkl\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n    # see below for module contents\n    exporter.save_module(\"foo\")\n    exporter.save_module(\"bar\")\n"
    },
    {
        "X": "How to use PackageImporter implements the\nimportlib.resources\nAPI for accessing resources from inside a package.The importlib.resources API allows access to resources from within packaged code., give an example?",
        "Z": " The importlib.resources API allows access to resources from within packaged code. # foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")\n",
        "Y": "# foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")\n"
    },
    {
        "X": "How to use The importlib.resources API allows access to resources from within packaged code.Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent PackageImporter instance itself from within\npackaged code., give an example?",
        "Z": " The importlib.resources API allows access to resources from within packaged code.Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent PackageImporter instance itself from within\npackaged code. # bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalient to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\n",
        "Y": "# bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalient to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")\n"
    },
    {
        "X": "How to use To tell if an object\u2019s code is from a torch.package, use the torch.package.is_from_package() function.\nNote: if an object is from a package but its definition is from a module marked extern or from stdlib,\nthis check will return False., give an example?",
        "Z": " To tell if an object\u2019s code is from a torch.package, use the torch.package.is_from_package() function.\nNote: if an object is from a package but its definition is from a module marked extern or from stdlib,\nthis check will return False. importer = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False\n",
        "Y": "importer = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False\n"
    },
    {
        "X": "How to use To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter\naware of the original PackageImporter so that it can find source code for your object\u2019s dependencies., give an example?",
        "Z": " To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter\naware of the original PackageImporter so that it can find source code for your object\u2019s dependencies. importer = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)\n",
        "Y": "importer = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)\n"
    },
    {
        "X": "How to use To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work., give an example?",
        "Z": " To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work. # save TorchScript just like any other object\nwith PackageExporter(file_name, verbose=True) as e:\n    e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\n    e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\n# load as normal\nimporter = PackageImporter(file_name)\nloaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\nloaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"\n",
        "Y": "# save TorchScript just like any other object\nwith PackageExporter(file_name, verbose=True) as e:\n    e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\n    e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\n# load as normal\nimporter = PackageImporter(file_name)\nloaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\nloaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"\n"
    },
    {
        "X": "How to use A torch.package file is a ZIP archive which conventionally uses the .pt extension. Inside the ZIP archive, there are two kinds of files:As an example, this is what a fully packaged ResNet model from torchvision looks like:, give an example?",
        "Z": " As an example, this is what a fully packaged ResNet model from torchvision looks like: resnet\n\u251c\u2500\u2500 .data  # All framework-specific data is stored here.\n\u2502   \u2502      # It's named to avoid conflicts with user-serialized code.\n\u2502   \u251c\u2500\u2500 94286146172688.storage  # tensor data\n\u2502   \u251c\u2500\u2500 94286146172784.storage\n\u2502   \u251c\u2500\u2500 extern_modules  # text file with names of extern modules (e.g. 'torch')\n\u2502   \u251c\u2500\u2500 version         # version metadata\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u2514\u2500\u2500 torchvision  # all code dependencies are captured as source files\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n",
        "Y": "resnet\n\u251c\u2500\u2500 .data  # All framework-specific data is stored here.\n\u2502   \u2502      # It's named to avoid conflicts with user-serialized code.\n\u2502   \u251c\u2500\u2500 94286146172688.storage  # tensor data\n\u2502   \u251c\u2500\u2500 94286146172784.storage\n\u2502   \u251c\u2500\u2500 extern_modules  # text file with names of extern modules (e.g. 'torch')\n\u2502   \u251c\u2500\u2500 version         # version metadata\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u2514\u2500\u2500 torchvision  # all code dependencies are captured as source files\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n"
    },
    {
        "X": "How to use The .data/ directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThe torch.package format makes no guarantees about the contents of .data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load older torch.packages).Currently, the .data/ directory contains the following items:, give an example?",
        "Z": " Currently, the .data/ directory contains the following items: .data\n\u251c\u2500\u2500 94286146172688.storage\n\u251c\u2500\u2500 94286146172784.storage\n\u251c\u2500\u2500 extern_modules\n\u251c\u2500\u2500 version\n\u251c\u2500\u2500 ...\n",
        "Y": ".data\n\u251c\u2500\u2500 94286146172688.storage\n\u251c\u2500\u2500 94286146172784.storage\n\u251c\u2500\u2500 extern_modules\n\u251c\u2500\u2500 version\n\u251c\u2500\u2500 ...\n"
    },
    {
        "X": "How to use All other files in the archive were put there by a user. The layout is identical to a Python\nregular package. For a deeper dive in how Python packaging works,\nplease consult this essay (it\u2019s slightly out of date, so double-check implementation details\nwith the Python reference documentation)., give an example?",
        "Z": " All other files in the archive were put there by a user. The layout is identical to a Python\nregular package. For a deeper dive in how Python packaging works,\nplease consult this essay (it\u2019s slightly out of date, so double-check implementation details\nwith the Python reference documentation). <package root>\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u251c\u2500\u2500 another_package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 foo.txt         # a resource file , see importlib.resources\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py   # torchvision.models.resnet\n        \u2514\u2500\u2500 utils.py    # torchvision.models.utils\n",
        "Y": "<package root>\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u251c\u2500\u2500 another_package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 foo.txt         # a resource file , see importlib.resources\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py   # torchvision.models.resnet\n        \u2514\u2500\u2500 utils.py    # torchvision.models.utils\n"
    },
    {
        "X": "How to use When you issue a save_pickle(obj, ...) call, PackageExporter will pickle the object normally. Then, it uses the\npickletools standard library module to parse the pickle bytecode.In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like:, give an example?",
        "Z": " In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: GLOBAL 'torchvision.models.resnet Resnet`\n",
        "Y": "GLOBAL 'torchvision.models.resnet Resnet`\n"
    },
    {
        "X": "How to use Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s what torch.package uses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like \"foo.**\"). You associate a pattern\nwith an action using methods on PackageImporter, e.g., give an example?",
        "Z": " Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s what torch.package uses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like \"foo.**\"). You associate a pattern\nwith an action using methods on PackageImporter, e.g. my_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")\n",
        "Y": "my_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")\n"
    },
    {
        "X": "How to use When specifying actions, you can pass multiple patterns, e.g., give an example?",
        "Z": " When specifying actions, you can pass multiple patterns, e.g. exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\n",
        "Y": "exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])\n"
    },
    {
        "X": "How to use A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g., give an example?",
        "Z": " A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g. exporter.mock(\"**\", exclude=[\"torchvision.**\"])\n",
        "Y": "exporter.mock(\"**\", exclude=[\"torchvision.**\"])\n"
    },
    {
        "X": "How to use Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example:, give an example?",
        "Z": " Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example: from foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!\n",
        "Y": "from foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!\n"
    },
    {
        "X": "How to use In this example, MyClass and import_MyClass are not the same type. In this specific example, MyClass and import_MyClass have exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation where import_MyClass is coming from an\nolder package with an entirely different implementation of MyClass \u2014 in that case, it\u2019s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes:, give an example?",
        "Z": " In this example, MyClass and import_MyClass are not the same type. In this specific example, MyClass and import_MyClass have exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation where import_MyClass is coming from an\nolder package with an entirely different implementation of MyClass \u2014 in that case, it\u2019s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes: print(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\n",
        "Y": "print(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass\n"
    },
    {
        "X": "How to use torch.package.PackageExporter.close, give an example?",
        "Z": " Write the package to the filesystem. Any calls after close() are now invalid.\nIt is preferable to use resource guard syntax instead: with PackageExporter(\"file.zip\") as e:\n    ...\n",
        "Y": "with PackageExporter(\"file.zip\") as e:\n    ...\n"
    },
    {
        "X": "How to use torch.package.PackageExporter.register_extern_hook, give an example?",
        "Z": " The hook will be called each time a module matches against an extern() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> None\n",
        "Y": "hook(exporter: PackageExporter, module_name: str) -> None\n"
    },
    {
        "X": "How to use torch.package.PackageExporter.register_intern_hook, give an example?",
        "Z": " The hook will be called each time a module matches against an intern() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> None\n",
        "Y": "hook(exporter: PackageExporter, module_name: str) -> None\n"
    },
    {
        "X": "How to use torch.package.PackageExporter.register_mock_hook, give an example?",
        "Z": " The hook will be called each time a module matches against a mock() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> None\n",
        "Y": "hook(exporter: PackageExporter, module_name: str) -> None\n"
    },
    {
        "X": "How to use torch.package.PackageImporter.id, give an example?",
        "Z": " Returns internal identifier that torch.package uses to distinguish PackageImporter instances.\nLooks like: <torch_package_0>\n",
        "Y": "<torch_package_0>\n"
    },
    {
        "X": "How to use torch.cumsum, give an example?",
        "Z": " For example, if input is a vector of size N, the result will also be\na vector of size N, with elements. >>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])\n",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])\n"
    },
    {
        "X": "How to use torch.linalg.matrix_power, give an example?",
        "Z": " Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by\na negative power as, if n> 0: matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B\n",
        "Y": "matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B\n"
    },
    {
        "X": "How  If n= 0, it returns the identity matrix (or batch) of the same shape\nas A. If n is negative, it returns the inverse of each matrix\n(if invertible) raised to the power of abs(n)., give an example?",
        "Z": " If n= 0, it returns the identity matrix (or batch) of the same shape\nas A. If n is negative, it returns the inverse of each matrix\n(if invertible) raised to the power of abs(n). >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.2270,  0.6663, -1.3515],\n        [-0.9838, -0.4002, -1.9313],\n        [-0.7886, -0.0450,  0.0528]])\n>>> torch.linalg.matrix_power(a, 0)\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n>>> torch.linalg.matrix_power(a, 3)\ntensor([[ 1.0756,  0.4980,  0.0100],\n        [-1.6617,  1.4994, -1.9980],\n        [-0.4509,  0.2731,  0.8001]])\n>>> torch.linalg.matrix_power(a.expand(2, -1, -1), -2)\ntensor([[[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]],\n        [[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]]])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.2270,  0.6663, -1.3515],\n        [-0.9838, -0.4002, -1.9313],\n        [-0.7886, -0.0450,  0.0528]])\n>>> torch.linalg.matrix_power(a, 0)\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n>>> torch.linalg.matrix_power(a, 3)\ntensor([[ 1.0756,  0.4980,  0.0100],\n        [-1.6617,  1.4994, -1.9980],\n        [-0.4509,  0.2731,  0.8001]])\n>>> torch.linalg.matrix_power(a.expand(2, -1, -1), -2)\ntensor([[[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]],\n        [[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]]])\n"
    },
    {
        "X": "How to use torch.profiler.profile, give an example?",
        "Z": "  with torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile()\nprint(p.key_averages().table(\n    sort_by=\"self_cuda_time_total\", row_limit=-1))\n",
        "Y": "with torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile()\nprint(p.key_averages().table(\n    sort_by=\"self_cuda_time_total\", row_limit=-1))\n"
    },
    {
        "X": "How to use Using the profiler\u2019s schedule, on_trace_ready and step functions:, give an example?",
        "Z": " Using the profiler\u2019s schedule, on_trace_ready and step functions: # Non-default profiler schedule allows user to turn profiler on and off\n# on different iterations of the training loop;\n# trace_handler is called every time a new trace becomes available\ndef trace_handler(prof):\n    print(prof.key_averages().table(\n        sort_by=\"self_cuda_time_total\", row_limit=-1))\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n\n    # In this example with wait=1, warmup=1, active=2,\n    # profiler will skip the first step/iteration,\n    # start warming up on the second, record\n    # the third and the forth iterations,\n    # after which the trace will become available\n    # and on_trace_ready (when set) is called;\n    # the cycle repeats starting with the next step\n\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n    # used when outputting for tensorboard\n    ) as p:\n        for iter in range(N):\n            code_iteration_to_profile(iter)\n            # send a signal to the profiler that the next iteration has started\n            p.step()\n",
        "Y": "# Non-default profiler schedule allows user to turn profiler on and off\n# on different iterations of the training loop;\n# trace_handler is called every time a new trace becomes available\ndef trace_handler(prof):\n    print(prof.key_averages().table(\n        sort_by=\"self_cuda_time_total\", row_limit=-1))\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n\n    # In this example with wait=1, warmup=1, active=2,\n    # profiler will skip the first step/iteration,\n    # start warming up on the second, record\n    # the third and the forth iterations,\n    # after which the trace will become available\n    # and on_trace_ready (when set) is called;\n    # the cycle repeats starting with the next step\n\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n    # used when outputting for tensorboard\n    ) as p:\n        for iter in range(N):\n            code_iteration_to_profile(iter)\n            # send a signal to the profiler that the next iteration has started\n            p.step()\n"
    },
    {
        "X": "How to use torch.unique, give an example?",
        "Z": "  >>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n>>> output\ntensor([ 2,  3,  1])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([ 0,  2,  1,  2])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([[ 0,  2],\n        [ 1,  2]])\n",
        "Y": ">>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n>>> output\ntensor([ 2,  3,  1])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([ 0,  2,  1,  2])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([[ 0,  2],\n        [ 1,  2]])\n"
    },
    {
        "X": "How to use torch.asin, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])\n"
    },
    {
        "X": "How to use To construct an Optimizer you have to give it an iterable containing the\nparameters (all should be Variable s) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc., give an example?",
        "Z": "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n",
        "Y": "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)\n"
    },
    {
        "X": "How to use Optimizer s also support specifying per-parameter options. To do this, instead\nof passing an iterable of Variable s, pass in an iterable of\ndict s. Each of them will define a separate parameter group, and should contain\na params key, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group.For example, this is very useful when one wants to specify per-layer learning rates:, give an example?",
        "Z": " For example, this is very useful when one wants to specify per-layer learning rates: optim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)\n",
        "Y": "optim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)\n"
    },
    {
        "X": "How to use This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.\nbackward()., give an example?",
        "Z": "  for input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n",
        "Y": "for input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()\n"
    },
    {
        "X": "How to use Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it., give an example?",
        "Z": "  for input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n",
        "Y": "for input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n"
    },
    {
        "X": "How to use Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way:, give an example?",
        "Z": " Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n",
        "Y": "model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()\n"
    },
    {
        "X": "How to use Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it., give an example?",
        "Z": " Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()\n",
        "Y": "model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()\n"
    },
    {
        "X": "How to use In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms., give an example?",
        "Z": " In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. >>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n",
        "Y": ">>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()\n"
    },
    {
        "X": "How to use AveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running:, give an example?",
        "Z": " AveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running: >>> swa_model = AveragedModel(model)\n",
        "Y": ">>> swa_model = AveragedModel(model)\n"
    },
    {
        "X": "How to use AveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running:Here the model model can be an arbitrary torch.nn.Module object. swa_model\nwill keep track of the running averages of the parameters of the model. To update these\naverages, you can use the update_parameters() function:, give an example?",
        "Z": " Here the model model can be an arbitrary torch.nn.Module object. swa_model\nwill keep track of the running averages of the parameters of the model. To update these\naverages, you can use the update_parameters() function: >>> swa_model.update_parameters(model)\n",
        "Y": ">>> swa_model.update_parameters(model)\n"
    },
    {
        "X": "How to use Typically, in SWA the learning rate is set to a high constant value. SWALR is a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:, give an example?",
        "Z": " Typically, in SWA the learning rate is set to a high constant value. SWALR is a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: >>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n",
        "Y": ">>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)\n"
    },
    {
        "X": "How to use update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloader loader at the end of training:, give an example?",
        "Z": " update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloader loader at the end of training: >>> torch.optim.swa_utils.update_bn(loader, swa_model)\n",
        "Y": ">>> torch.optim.swa_utils.update_bn(loader, swa_model)\n"
    },
    {
        "X": "How to use By default, torch.optim.swa_utils.AveragedModel computes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with the\navg_fn parameter. In the following example ema_model computes an exponential moving average., give an example?",
        "Z": "  >>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n",
        "Y": ">>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)\n"
    },
    {
        "X": "How to use In the example below, swa_model is the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:, give an example?",
        "Z": " In the example below, swa_model is the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160: >>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)\n",
        "Y": ">>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)\n"
    },
    {
        "X": "How to use torch.nanquantile, give an example?",
        "Z": "  >>> t = torch.tensor([float('nan'), 1, 2])\n>>> t.quantile(0.5)\ntensor(nan)\n>>> t.nanquantile(0.5)\ntensor(1.5000)\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\n>>> t\ntensor([[nan, nan],\n        [1., 2.]])\n>>> t.nanquantile(0.5, dim=0)\ntensor([1., 2.])\n>>> t.nanquantile(0.5, dim=1)\ntensor([   nan, 1.5000])\n",
        "Y": ">>> t = torch.tensor([float('nan'), 1, 2])\n>>> t.quantile(0.5)\ntensor(nan)\n>>> t.nanquantile(0.5)\ntensor(1.5000)\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\n>>> t\ntensor([[nan, nan],\n        [1., 2.]])\n>>> t.nanquantile(0.5, dim=0)\ntensor([1., 2.])\n>>> t.nanquantile(0.5, dim=1)\ntensor([   nan, 1.5000])\n"
    },
    {
        "X": "How to use torch.symeig, give an example?",
        "Z": " torch.symeig() is deprecated in favor of torch.linalg.eigh()\nand will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion.L, _ = torch.symeig(A, upper=upper) should be replaced with UPLO = \"U\" if upper else \"L\"\nL = torch.linalg.eigvalsh(A, UPLO=UPLO)\n",
        "Y": "UPLO = \"U\" if upper else \"L\"\nL = torch.linalg.eigvalsh(A, UPLO=UPLO)\n"
    },
    {
        "X": "How  L, _ = torch.symeig(A, upper=upper) should be replaced withL, V = torch.symeig(A, eigenvectors=True, upper=upper) should be replaced with, give an example?",
        "Z": " L, _ = torch.symeig(A, upper=upper) should be replaced withL, V = torch.symeig(A, eigenvectors=True, upper=upper) should be replaced with UPLO = \"U\" if upper else \"L\"\nL, V = torch.linalg.eigh(A, UPLO=UPLO)\n",
        "Y": "UPLO = \"U\" if upper else \"L\"\nL, V = torch.linalg.eigh(A, UPLO=UPLO)\n"
    },
    {
        "X": "How  If upper is False, then lower triangular portion is used., give an example?",
        "Z": " If upper is False, then lower triangular portion is used. >>> a = torch.randn(5, 5)\n>>> a = a + a.t()  # To make a symmetric\n>>> a\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\n>>> e, v = torch.symeig(a, eigenvectors=True)\n>>> e\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\n>>> v\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\n>>> a_big = torch.randn(5, 2, 2)\n>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\n>>> e, v = a_big.symeig(eigenvectors=True)\n>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\nTrue\n",
        "Y": ">>> a = torch.randn(5, 5)\n>>> a = a + a.t()  # To make a symmetric\n>>> a\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\n>>> e, v = torch.symeig(a, eigenvectors=True)\n>>> e\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\n>>> v\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\n>>> a_big = torch.randn(5, 2, 2)\n>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\n>>> e, v = a_big.symeig(eigenvectors=True)\n>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\nTrue\n"
    },
    {
        "X": "How to use torch.gcd, give an example?",
        "Z": " Both input and other must have integer types. >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.gcd(a, b)\ntensor([1, 2, 5])\n>>> c = torch.tensor([3])\n>>> torch.gcd(a, c)\ntensor([1, 1, 3])\n",
        "Y": ">>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.gcd(a, b)\ntensor([1, 2, 5])\n>>> c = torch.tensor([3])\n>>> torch.gcd(a, c)\ntensor([1, 1, 3])\n"
    },
    {
        "X": "How to use torch.matrix_exp, give an example?",
        "Z": " Bader, P.; Blanes, S.; Casas, F.\nComputing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.\nMathematics 2019, 7, 1174. >>> a = torch.randn(2, 2, 2)\n>>> a[0, :, :] = torch.eye(2, 2)\n>>> a[1, :, :] = 2 * torch.eye(2, 2)\n>>> a\ntensor([[[1., 0.],\n         [0., 1.]],\n\n        [[2., 0.],\n         [0., 2.]]])\n>>> torch.matrix_exp(a)\ntensor([[[2.7183, 0.0000],\n         [0.0000, 2.7183]],\n\n         [[7.3891, 0.0000],\n          [0.0000, 7.3891]]])\n\n>>> import math\n>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])\n>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\ntensor([[ 0.5000,  0.8660],\n        [-0.8660,  0.5000]])\n",
        "Y": ">>> a = torch.randn(2, 2, 2)\n>>> a[0, :, :] = torch.eye(2, 2)\n>>> a[1, :, :] = 2 * torch.eye(2, 2)\n>>> a\ntensor([[[1., 0.],\n         [0., 1.]],\n\n        [[2., 0.],\n         [0., 2.]]])\n>>> torch.matrix_exp(a)\ntensor([[[2.7183, 0.0000],\n         [0.0000, 2.7183]],\n\n         [[7.3891, 0.0000],\n          [0.0000, 7.3891]]])\n\n>>> import math\n>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])\n>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\ntensor([[ 0.5000,  0.8660],\n        [-0.8660,  0.5000]])\n"
    },
    {
        "X": "How to use Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simple hubconf.py file;hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish)., give an example?",
        "Z": " hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). def entrypoint_name(*args, **kwargs):\n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    ...\n",
        "Y": "def entrypoint_name(*args, **kwargs):\n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    ...\n"
    },
    {
        "X": "How to use Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo, give an example?",
        "Z": " Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo dependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model\n",
        "Y": "dependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model\n"
    },
    {
        "X": "How  Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo, give an example?",
        "Z": " Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo if pretrained:\n    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
        "Y": "if pretrained:\n    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n"
    },
    {
        "X": "How to use torch.hub.list, give an example?",
        "Z": "  >>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n",
        "Y": ">>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n"
    },
    {
        "X": "How to use torch.hub.help, give an example?",
        "Z": "  >>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n",
        "Y": ">>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n"
    },
    {
        "X": "How to use torch.hub.load, give an example?",
        "Z": " If source is 'local', repo_or_dir is expected to be a\npath to a local directory. >>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)\n",
        "Y": ">>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)\n"
    },
    {
        "X": "How to use torch.hub.download_url_to_file, give an example?",
        "Z": "  >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n",
        "Y": ">>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n"
    },
    {
        "X": "How to use torch.hub.load_state_dict_from_url, give an example?",
        "Z": " If the object is already present in model_dir, it\u2019s deserialized and\nreturned.\nThe default value of model_dir is <hub_dir>/checkpoints where\nhub_dir is the directory returned by get_dir(). >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
        "Y": ">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n"
    },
    {
        "X": "How to use torch.log10, give an example?",
        "Z": "  >>> a = torch.rand(5)\n>>> a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n>>> torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\n",
        "Y": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n>>> torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])\n"
    },
    {
        "X": "How to use torch.fmin, give an example?",
        "Z": " Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. >>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])\n",
        "Y": ">>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])\n"
    },
    {
        "X": "How to use torch.outer, give an example?",
        "Z": "  >>> v1 = torch.arange(1., 5.)\n>>> v2 = torch.arange(1., 4.)\n>>> torch.outer(v1, v2)\ntensor([[  1.,   2.,   3.],\n        [  2.,   4.,   6.],\n        [  3.,   6.,   9.],\n        [  4.,   8.,  12.]])\n",
        "Y": ">>> v1 = torch.arange(1., 5.)\n>>> v2 = torch.arange(1., 4.)\n>>> torch.outer(v1, v2)\ntensor([[  1.,   2.,   3.],\n        [  2.,   4.,   6.],\n        [  3.,   6.,   9.],\n        [  4.,   8.,  12.]])\n"
    },
    {
        "X": "How to use torch.tan, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n>>> torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n>>> torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])\n"
    },
    {
        "X": "How to use torch.addr, give an example?",
        "Z": " If vec1 is a vector of size n and vec2 is a vector\nof size m, then input must be\nbroadcastable with a matrix of size\n(n\u00d7m)(n \\times m)(n\u00d7m) and out will be a matrix of size\n(n\u00d7m)(n \\times m)(n\u00d7m). >>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])\n",
        "Y": ">>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])\n"
    },
    {
        "X": "How to use torch.cumprod, give an example?",
        "Z": " For example, if input is a vector of size N, the result will also be\na vector of size N, with elements. >>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])\n",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])\n"
    },
    {
        "X": "How to use torch.inner, give an example?",
        "Z": "  # Dot product\n>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))\ntensor(7)\n\n# Multidimensional input tensors\n>>> a = torch.randn(2, 3)\n>>> a\ntensor([[0.8173, 1.0874, 1.1784],\n        [0.3279, 0.1234, 2.7894]])\n>>> b = torch.randn(2, 4, 3)\n>>> b\ntensor([[[-0.4682, -0.7159,  0.1506],\n        [ 0.4034, -0.3657,  1.0387],\n        [ 0.9892, -0.6684,  0.1774],\n        [ 0.9482,  1.3261,  0.3917]],\n\n        [[ 0.4537,  0.7493,  1.1724],\n        [ 0.2291,  0.5749, -0.2267],\n        [-0.7920,  0.3607, -0.3701],\n        [ 1.3666, -0.5850, -1.7242]]])\n>>> torch.inner(a, b)\ntensor([[[-0.9837,  1.1560,  0.2907,  2.6785],\n        [ 2.5671,  0.5452, -0.6912, -1.5509]],\n\n        [[ 0.1782,  2.9843,  0.7366,  1.5672],\n        [ 3.5115, -0.4864, -1.2476, -4.4337]]])\n\n# Scalar input\n>>> torch.inner(a, torch.tensor(2))\ntensor([[1.6347, 2.1748, 2.3567],\n        [0.6558, 0.2469, 5.5787]])\n",
        "Y": "# Dot product\n>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))\ntensor(7)\n\n# Multidimensional input tensors\n>>> a = torch.randn(2, 3)\n>>> a\ntensor([[0.8173, 1.0874, 1.1784],\n        [0.3279, 0.1234, 2.7894]])\n>>> b = torch.randn(2, 4, 3)\n>>> b\ntensor([[[-0.4682, -0.7159,  0.1506],\n        [ 0.4034, -0.3657,  1.0387],\n        [ 0.9892, -0.6684,  0.1774],\n        [ 0.9482,  1.3261,  0.3917]],\n\n        [[ 0.4537,  0.7493,  1.1724],\n        [ 0.2291,  0.5749, -0.2267],\n        [-0.7920,  0.3607, -0.3701],\n        [ 1.3666, -0.5850, -1.7242]]])\n>>> torch.inner(a, b)\ntensor([[[-0.9837,  1.1560,  0.2907,  2.6785],\n        [ 2.5671,  0.5452, -0.6912, -1.5509]],\n\n        [[ 0.1782,  2.9843,  0.7366,  1.5672],\n        [ 3.5115, -0.4864, -1.2476, -4.4337]]])\n\n# Scalar input\n>>> torch.inner(a, torch.tensor(2))\ntensor([[1.6347, 2.1748, 2.3567],\n        [0.6558, 0.2469, 5.5787]])\n"
    },
    {
        "X": "How to use torch.isclose, give an example?",
        "Z": " where input and other are finite. Where input\nand/or other are nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other when\nequal_nan is True. >>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))\ntensor([ True, False, False])\n>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)\ntensor([True, True])\n",
        "Y": ">>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))\ntensor([ True, False, False])\n>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)\ntensor([True, True])\n"
    },
    {
        "X": "How to use torch.rsqrt, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n>>> torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n>>> torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])\n"
    },
    {
        "X": "How to use torch.searchsorted, give an example?",
        "Z": "  >>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> sorted_sequence\ntensor([[ 1,  3,  5,  7,  9],\n        [ 2,  4,  6,  8, 10]])\n>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> values\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.searchsorted(sorted_sequence, values)\ntensor([[1, 3, 4],\n        [1, 2, 4]])\n>>> torch.searchsorted(sorted_sequence, values, right=True)\ntensor([[2, 3, 5],\n        [1, 3, 4]])\n\n>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])\n>>> sorted_sequence_1d\ntensor([1, 3, 5, 7, 9])\n>>> torch.searchsorted(sorted_sequence_1d, values)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n",
        "Y": ">>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> sorted_sequence\ntensor([[ 1,  3,  5,  7,  9],\n        [ 2,  4,  6,  8, 10]])\n>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> values\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.searchsorted(sorted_sequence, values)\ntensor([[1, 3, 4],\n        [1, 2, 4]])\n>>> torch.searchsorted(sorted_sequence, values, right=True)\ntensor([[2, 3, 5],\n        [1, 3, 4]])\n\n>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])\n>>> sorted_sequence_1d\ntensor([1, 3, 5, 7, 9])\n>>> torch.searchsorted(sorted_sequence_1d, values)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n"
    },
    {
        "X": "How to use torch.block_diag, give an example?",
        "Z": "  >>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n",
        "Y": ">>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])\n"
    },
    {
        "X": "How to use torch.linalg.inv, give an example?",
        "Z": " Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by\nthe inverse, as: torch.linalg.solve(A, B) == A.inv() @ B\n",
        "Y": "torch.linalg.solve(A, B) == A.inv() @ B\n"
    },
    {
        "X": "How  Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices\nthen the output has the same batch dimensions., give an example?",
        "Z": " Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices\nthen the output has the same batch dimensions. >>> x = torch.rand(4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.dist(z, torch.eye(4))\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4).expand_as(x))\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4, dtype=torch.cdouble))\ntensor(7.5107e-16, dtype=torch.float64)\n",
        "Y": ">>> x = torch.rand(4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.dist(z, torch.eye(4))\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4).expand_as(x))\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4, dtype=torch.cdouble))\ntensor(7.5107e-16, dtype=torch.float64)\n"
    },
    {
        "X": "How to use torch.bitwise_or, give an example?",
        "Z": "  >>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])\n",
        "Y": ">>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])\n"
    },
    {
        "X": "How to use torch.igammac, give an example?",
        "Z": " Supports broadcasting to a common shape\nand float inputs. >>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])\n",
        "Y": ">>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])\n"
    },
    {
        "X": "How to use torch.randint, give an example?",
        "Z": " The shape of the tensor is defined by the variable argument size. >>> torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n\n>>> torch.randint(10, (2, 2))\ntensor([[0, 2],\n        [5, 5]])\n\n\n>>> torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n        [6, 7]])\n",
        "Y": ">>> torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n\n>>> torch.randint(10, (2, 2))\ntensor([[0, 2],\n        [5, 5]])\n\n\n>>> torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n        [6, 7]])\n"
    },
    {
        "X": "How to use torch.masked_select, give an example?",
        "Z": " The shapes of the mask tensor and the input tensor don\u2019t need\nto match, but they must be broadcastable. >>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n        [-1.2035,  1.2252,  0.5002,  0.6248],\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\n>>> mask = x.ge(0.5)\n>>> mask\ntensor([[False, False, False, False],\n        [False, True, True, True],\n        [False, False, False, True]])\n>>> torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n",
        "Y": ">>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n        [-1.2035,  1.2252,  0.5002,  0.6248],\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\n>>> mask = x.ge(0.5)\n>>> mask\ntensor([[False, False, False, False],\n        [False, True, True, True],\n        [False, False, False, True]])\n>>> torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])\n"
    },
    {
        "X": "How to use torch.mm, give an example?",
        "Z": " This operator supports TensorFloat32. >>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n        [-0.0760, -3.6705,  2.4784]])\n",
        "Y": ">>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n        [-0.0760, -3.6705,  2.4784]])\n"
    },
    {
        "X": "How to use torch.movedim, give an example?",
        "Z": " Other dimensions of input that are not explicitly moved remain in\ntheir original order and appear at the positions not specified in destination. >>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.movedim(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.movedim(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.movedim(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.movedim(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])\n",
        "Y": ">>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.movedim(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.movedim(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.movedim(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.movedim(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])\n"
    },
    {
        "X": "How to use torch.addmm, give an example?",
        "Z": " This operator supports TensorFloat32. >>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])\n",
        "Y": ">>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])\n"
    },
    {
        "X": "How to use torch.prod, give an example?",
        "Z": "  >>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.8020,  0.5428, -1.5854]])\n>>> torch.prod(a)\ntensor(0.6902)\n",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.8020,  0.5428, -1.5854]])\n>>> torch.prod(a)\ntensor(0.6902)\n"
    },
    {
        "X": "How  If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input., give an example?",
        "Z": " If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. >>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)\n",
        "Y": ">>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)\n"
    },
    {
        "X": "How to use torch.where, give an example?",
        "Z": " The operation is defined as: >>> x = torch.randn(3, 2)\n>>> y = torch.ones(3, 2)\n>>> x\ntensor([[-0.4620,  0.3139],\n        [ 0.3898, -0.7197],\n        [ 0.0478, -0.1657]])\n>>> torch.where(x > 0, x, y)\ntensor([[ 1.0000,  0.3139],\n        [ 0.3898,  1.0000],\n        [ 0.0478,  1.0000]])\n>>> x = torch.randn(2, 2, dtype=torch.double)\n>>> x\ntensor([[ 1.0779,  0.0383],\n        [-0.8785, -1.1089]], dtype=torch.float64)\n>>> torch.where(x > 0, x, 0.)\ntensor([[1.0779, 0.0383],\n        [0.0000, 0.0000]], dtype=torch.float64)\n",
        "Y": ">>> x = torch.randn(3, 2)\n>>> y = torch.ones(3, 2)\n>>> x\ntensor([[-0.4620,  0.3139],\n        [ 0.3898, -0.7197],\n        [ 0.0478, -0.1657]])\n>>> torch.where(x > 0, x, y)\ntensor([[ 1.0000,  0.3139],\n        [ 0.3898,  1.0000],\n        [ 0.0478,  1.0000]])\n>>> x = torch.randn(2, 2, dtype=torch.double)\n>>> x\ntensor([[ 1.0779,  0.0383],\n        [-0.8785, -1.1089]], dtype=torch.float64)\n>>> torch.where(x > 0, x, 0.)\ntensor([[1.0779, 0.0383],\n        [0.0000, 0.0000]], dtype=torch.float64)\n"
    },
    {
        "X": "How to use torch.empty_like, give an example?",
        "Z": "  >>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\n",
        "Y": ">>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\n"
    },
    {
        "X": "How to use torch.bmm, give an example?",
        "Z": " This operator supports TensorFloat32. >>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])\n",
        "Y": ">>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])\n"
    },
    {
        "X": "How to use torch.randn, give an example?",
        "Z": " The shape of the tensor is defined by the variable argument size. >>> torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n>>> torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n        [ 1.1719, -0.4709, -0.1996]])\n",
        "Y": ">>> torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n>>> torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n        [ 1.1719, -0.4709, -0.1996]])\n"
    },
    {
        "X": "How to use torch.poisson, give an example?",
        "Z": "  >>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5\n>>> torch.poisson(rates)\ntensor([[9., 1., 3., 5.],\n        [8., 6., 6., 0.],\n        [0., 4., 5., 3.],\n        [2., 1., 4., 2.]])\n",
        "Y": ">>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5\n>>> torch.poisson(rates)\ntensor([[9., 1., 3., 5.],\n        [8., 6., 6., 0.],\n        [0., 4., 5., 3.],\n        [2., 1., 4., 2.]])\n"
    },
    {
        "X": "How to use torch.vstack, give an example?",
        "Z": " This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by torch.atleast_2d(). >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.vstack((a,b))\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.vstack((a,b))\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.vstack((a,b))\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.vstack((a,b))\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])\n"
    },
    {
        "X": "How to use torch.reshape, give an example?",
        "Z": " A single dimension may be -1, in which case it\u2019s inferred from the remaining\ndimensions and the number of elements in input. >>> a = torch.arange(4.)\n>>> torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n>>> b = torch.tensor([[0, 1], [2, 3]])\n>>> torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])\n",
        "Y": ">>> a = torch.arange(4.)\n>>> torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n>>> b = torch.tensor([[0, 1], [2, 3]])\n>>> torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])\n"
    },
    {
        "X": "How to use torch.bucketize, give an example?",
        "Z": "  >>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])\n",
        "Y": ">>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])\n"
    },
    {
        "X": "How to use torch.copysign, give an example?",
        "Z": " Supports broadcasting to a common shape,\nand integer and float inputs. >>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])\n",
        "Y": ">>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])\n"
    },
    {
        "X": "How to use torch.kthvalue, give an example?",
        "Z": " If keepdim is True, both the values and indices tensors\nare the same size as input, except in the dimension dim where\nthey are of size 1. Otherwise, dim is squeezed\n(see torch.squeeze()), resulting in both the values and\nindices tensors having 1 fewer dimension than the input tensor. >>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.kthvalue(x, 4)\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n\n>>> x=torch.arange(1.,7.).resize_(2,3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.]])\n>>> torch.kthvalue(x, 2, 0, True)\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))\n",
        "Y": ">>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.kthvalue(x, 4)\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n\n>>> x=torch.arange(1.,7.).resize_(2,3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.]])\n>>> torch.kthvalue(x, 2, 0, True)\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))\n"
    },
    {
        "X": "How to use torch.cholesky, give an example?",
        "Z": " torch.cholesky() is deprecated in favor of torch.linalg.cholesky()\nand will be removed in a future PyTorch release.L = torch.cholesky(A) should be replaced with L = torch.linalg.cholesky(A)\n",
        "Y": "L = torch.linalg.cholesky(A)\n"
    },
    {
        "X": "How  L = torch.cholesky(A) should be replaced withU = torch.cholesky(A, upper=True) should be replaced with, give an example?",
        "Z": " L = torch.cholesky(A) should be replaced withU = torch.cholesky(A, upper=True) should be replaced with U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()\n",
        "Y": "U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()\n"
    },
    {
        "X": "How  If upper is True, and AAA is a batch of symmetric positive-definite\nmatrices, then the returned tensor will be composed of upper-triangular Cholesky factors\nof each of the individual matrices. Similarly, when upper is False, the returned\ntensor will be composed of lower-triangular Cholesky factors of each of the individual\nmatrices., give an example?",
        "Z": " If upper is True, and AAA is a batch of symmetric positive-definite\nmatrices, then the returned tensor will be composed of upper-triangular Cholesky factors\nof each of the individual matrices. Similarly, when upper is False, the returned\ntensor will be composed of lower-triangular Cholesky factors of each of the individual\nmatrices. >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2)\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = torch.matmul(l, l.transpose(-1, -2))\n>>> torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2)\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = torch.matmul(l, l.transpose(-1, -2))\n>>> torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)\n"
    },
    {
        "X": "How to use torch.histc, give an example?",
        "Z": " Elements lower than min and higher than max are ignored. >>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])\n",
        "Y": ">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])\n"
    },
    {
        "X": "How to use torch.eq, give an example?",
        "Z": " The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ True, False],\n        [False, True]])\n",
        "Y": ">>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ True, False],\n        [False, True]])\n"
    },
    {
        "X": "How to use torch.unsqueeze, give an example?",
        "Z": " A dim value within the range [-input.dim() - 1, input.dim() + 1)\ncan be used. Negative dim will correspond to unsqueeze()\napplied at dim = dim + input.dim() + 1. >>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])\n"
    },
    {
        "X": "How to use torch.matrix_rank, give an example?",
        "Z": " tol is the threshold below which the singular values (or the eigenvalues\nwhen symmetric is True) are considered to be 0. If tol is not\nspecified, tol is set to S.max() * max(S.size()) * eps where S is the\nsingular values (or the eigenvalues when symmetric is True), and eps\nis the epsilon value for the datatype of input. >>> a = torch.eye(10)\n>>> torch.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.matrix_rank(b)\ntensor(9)\n",
        "Y": ">>> a = torch.eye(10)\n>>> torch.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.matrix_rank(b)\ntensor(9)\n"
    },
    {
        "X": "How to use torch.argmax, give an example?",
        "Z": " This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)\n"
    },
    {
        "X": "How  This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method., give an example?",
        "Z": " This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])\n"
    },
    {
        "X": "How to use torch.imag, give an example?",
        "Z": "  >>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n",
        "Y": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n"
    },
    {
        "X": "How to use torch.exp, give an example?",
        "Z": "  >>> torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])\n",
        "Y": ">>> torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])\n"
    },
    {
        "X": "How to use torch.angle, give an example?",
        "Z": "  >>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])\n",
        "Y": ">>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])\n"
    },
    {
        "X": "How  Similar to SciPy\u2019s scipy.special.xlog1py., give an example?",
        "Z": " Similar to SciPy\u2019s scipy.special.xlog1py. >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])\n",
        "Y": ">>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])\n"
    },
    {
        "X": "How to use torch.Tensor.scatter_, give an example?",
        "Z": " For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
        "Y": "self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n"
    },
    {
        "X": "How  Additionally accepts an optional reduce argument that allows\nspecification of an optional reduction operation, which is applied to all\nvalues in the tensor src into self at the indicies\nspecified in the index. For each value in src, the reduction\noperation is applied to an index in self which is specified by\nits index in src for dimension != dim and by the corresponding\nvalue in index for dimension = dim.Given a 3-D tensor and reduction using the multiplication operation, self\nis updated as:, give an example?",
        "Z": " Additionally accepts an optional reduce argument that allows\nspecification of an optional reduction operation, which is applied to all\nvalues in the tensor src into self at the indicies\nspecified in the index. For each value in src, the reduction\noperation is applied to an index in self which is specified by\nits index in src for dimension != dim and by the corresponding\nvalue in index for dimension = dim.Given a 3-D tensor and reduction using the multiplication operation, self\nis updated as: self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
        "Y": "self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n"
    },
    {
        "X": "How  Reducing with the addition operation is the same as using\nscatter_add_()., give an example?",
        "Z": " Reducing with the addition operation is the same as using\nscatter_add_(). >>> src = torch.arange(1, 11).reshape((2, 5))\n>>> src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n>>> index = torch.tensor([[0, 1, 2, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])\n",
        "Y": ">>> src = torch.arange(1, 11).reshape((2, 5))\n>>> src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n>>> index = torch.tensor([[0, 1, 2, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])\n"
    },
    {
        "X": "How to use torch.range, give an example?",
        "Z": "  >>> torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\n",
        "Y": ">>> torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])\n"
    },
    {
        "X": "How to use torch.broadcast_to, give an example?",
        "Z": "  >>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n"
    },
    {
        "X": "How to use torch.squeeze, give an example?",
        "Z": " When dim is given, a squeeze operation is done only in the given\ndimension. If input is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B)(A\u00d71\u00d7B),\nsqueeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1)\nwill squeeze the tensor to the shape (A\u00d7B)(A \\times B)(A\u00d7B). >>> x = torch.zeros(2, 1, 2, 1, 2)\n>>> x.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x)\n>>> y.size()\ntorch.Size([2, 2, 2])\n>>> y = torch.squeeze(x, 0)\n>>> y.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x, 1)\n>>> y.size()\ntorch.Size([2, 2, 1, 2])\n",
        "Y": ">>> x = torch.zeros(2, 1, 2, 1, 2)\n>>> x.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x)\n>>> y.size()\ntorch.Size([2, 2, 2])\n>>> y = torch.squeeze(x, 0)\n>>> y.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x, 1)\n>>> y.size()\ntorch.Size([2, 2, 1, 2])\n"
    },
    {
        "X": "How to use You can use torch.manual_seed() to seed the RNG for all devices (both\nCPU and CUDA):, give an example?",
        "Z": " You can use torch.manual_seed() to seed the RNG for all devices (both\nCPU and CUDA): import torch\ntorch.manual_seed(0)\n",
        "Y": "import torch\ntorch.manual_seed(0)\n"
    },
    {
        "X": "How to use For custom operators, you might need to set python seed as well:, give an example?",
        "Z": " For custom operators, you might need to set python seed as well: import random\nrandom.seed(0)\n",
        "Y": "import random\nrandom.seed(0)\n"
    },
    {
        "X": "How to use If you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with:, give an example?",
        "Z": " If you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with: import numpy as np\nnp.random.seed(0)\n",
        "Y": "import numpy as np\nnp.random.seed(0)\n"
    },
    {
        "X": "How to use Please check the documentation for torch.use_deterministic_algorithms()\nfor a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:\nhttps://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error:, give an example?",
        "Z": " Please check the documentation for torch.use_deterministic_algorithms()\nfor a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:\nhttps://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error: >>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...\n",
        "Y": ">>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...\n"
    },
    {
        "X": "How to use For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error:When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used:, give an example?",
        "Z": " For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error:When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used: >>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')\n",
        "Y": ">>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')\n"
    },
    {
        "X": "How to use DataLoader will reseed workers following Randomness in multi-process data loading algorithm.\nUse worker_init_fn() and generator to preserve reproducibility:, give an example?",
        "Z": " DataLoader will reseed workers following Randomness in multi-process data loading algorithm.\nUse worker_init_fn() and generator to preserve reproducibility: def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker\n    generator=g,\n)\n",
        "Y": "def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker\n    generator=g,\n)\n"
    },
    {
        "X": "How to use torch.dsplit, give an example?",
        "Z": "  >>> t = torch.arange(16.0).reshape(2, 2, 4)\n>>> t\ntensor([[[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.]],\n        [[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]])\n>>> torch.dsplit(t, 2)\n(tensor([[[ 0.,  1.],\n        [ 4.,  5.]],\n       [[ 8.,  9.],\n        [12., 13.]]]),\n tensor([[[ 2.,  3.],\n          [ 6.,  7.]],\n         [[10., 11.],\n          [14., 15.]]]))\n",
        "Y": ">>> t = torch.arange(16.0).reshape(2, 2, 4)\n>>> t\ntensor([[[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.]],\n        [[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]])\n>>> torch.dsplit(t, 2)\n(tensor([[[ 0.,  1.],\n        [ 4.,  5.]],\n       [[ 8.,  9.],\n        [12., 13.]]]),\n tensor([[[ 2.,  3.],\n          [ 6.,  7.]],\n         [[10., 11.],\n          [14., 15.]]]))\n"
    },
    {
        "X": "How to use torch.can_cast, give an example?",
        "Z": "  >>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse\n",
        "Y": ">>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse\n"
    },
    {
        "X": "How to use torch.linalg.slogdet, give an example?",
        "Z": " Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices then\nthe output has the same batch dimensions. >>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.linalg.det(A)\ntensor(-0.7576)\n>>> torch.linalg.logdet(A)\ntensor(nan)\n>>> torch.linalg.slogdet(A)\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n",
        "Y": ">>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.linalg.det(A)\ntensor(-0.7576)\n>>> torch.linalg.logdet(A)\ntensor(nan)\n>>> torch.linalg.slogdet(A)\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))\n"
    },
    {
        "X": "How to use torch.qr, give an example?",
        "Z": " torch.qr() is deprecated in favor of torch.linalg.qr()\nand will be removed in a future PyTorch release. The boolean parameter some has been\nreplaced with a string parameter mode.Q, R = torch.qr(A) should be replaced with Q, R = torch.linalg.qr(A)\n",
        "Y": "Q, R = torch.linalg.qr(A)\n"
    },
    {
        "X": "How  Q, R = torch.qr(A) should be replaced withQ, R = torch.qr(A, some=False) should be replaced with, give an example?",
        "Z": " Q, R = torch.qr(A) should be replaced withQ, R = torch.qr(A, some=False) should be replaced with Q, R = torch.linalg.qr(A, mode=\"complete\")\n",
        "Y": "Q, R = torch.linalg.qr(A, mode=\"complete\")\n"
    },
    {
        "X": "How  If some is True, then this function returns the thin (reduced) QR factorization.\nOtherwise, if some is False, this function returns the complete QR factorization., give an example?",
        "Z": " If some is True, then this function returns the thin (reduced) QR factorization.\nOtherwise, if some is False, this function returns the complete QR factorization. >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.qr(a, some=False)\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue\n",
        "Y": ">>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.qr(a, some=False)\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue\n"
    },
    {
        "X": "How to use torch.nan_to_num, give an example?",
        "Z": "  >>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])\n>>> torch.nan_to_num(x)\ntensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0)\ntensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)\ntensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])\n",
        "Y": ">>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])\n>>> torch.nan_to_num(x)\ntensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0)\ntensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)\ntensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])\n"
    },
    {
        "X": "How to use torch.div, give an example?",
        "Z": " Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. >>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])\n",
        "Y": ">>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])\n"
    },
    {
        "X": "How to use Mixing Tracing and Scripting, give an example?",
        "Z": "  import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)\n",
        "Y": "import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)\n"
    },
    {
        "X": "How to use Setting the environment variable PYTORCH_JIT=0 will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like pdb to debug the model code.  For example:, give an example?",
        "Z": " Setting the environment variable PYTORCH_JIT=0 will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like pdb to debug the model code.  For example: @torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))\n",
        "Y": "@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))\n"
    },
    {
        "X": "How to use Setting the environment variable PYTORCH_JIT=0 will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like pdb to debug the model code.  For example:Debugging this script with pdb works except for when we invoke the\n@torch.jit.script function. We can globally disable\nJIT, so that we can call the @torch.jit.script\nfunction as a normal Python function and not compile it. If the above script\nis called disable_jit_example.py, we can invoke it like so:, give an example?",
        "Z": " Debugging this script with pdb works except for when we invoke the\n@torch.jit.script function. We can globally disable\nJIT, so that we can call the @torch.jit.script\nfunction as a normal Python function and not compile it. If the above script\nis called disable_jit_example.py, we can invoke it like so: $ PYTORCH_JIT=0 python disable_jit_example.py\n",
        "Y": "$ PYTORCH_JIT=0 python disable_jit_example.py\n"
    },
    {
        "X": "How to use Inspecting Code, give an example?",
        "Z": "  @torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)\n",
        "Y": "@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)\n"
    },
    {
        "X": "How to use TorchScript provides a code pretty-printer for all ScriptModule instances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example:A ScriptModule with a single forward method will have an attribute\ncode, which you can use to inspect the ScriptModule\u2019s code.\nIf the ScriptModule has more than one method, you will need to access\n.code on the method itself and not the module. We can inspect the\ncode of a method named foo on a ScriptModule by accessing .foo.code.\nThe example above produces this output:, give an example?",
        "Z": " A ScriptModule with a single forward method will have an attribute\ncode, which you can use to inspect the ScriptModule\u2019s code.\nIf the ScriptModule has more than one method, you will need to access\n.code on the method itself and not the module. We can inspect the\ncode of a method named foo on a ScriptModule by accessing .foo.code.\nThe example above produces this output: def foo(len: int) -> Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0\n",
        "Y": "def foo(len: int) -> Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0\n"
    },
    {
        "X": "How to use Interpreting Graphs, give an example?",
        "Z": "  @torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)\n",
        "Y": "@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)\n"
    },
    {
        "X": "How to use graph follows the same rules described in the Inspecting Code section\nwith regard to forward method lookup.The example script above produces the graph:, give an example?",
        "Z": " graph follows the same rules described in the Inspecting Code section\nwith regard to forward method lookup.The example script above produces the graph: graph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -> (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -> (%rv.6)\n      -> (%17, %rv.13)\n  return (%rv)\n",
        "Y": "graph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -> (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -> (%rv.6)\n      -> (%17, %rv.13)\n  return (%rv)\n"
    },
    {
        "X": "How to use One way to automatically catch many errors in traces is by using check_inputs\non the torch.jit.trace() API. check_inputs takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:, give an example?",
        "Z": " One way to automatically catch many errors in traces is by using check_inputs\non the torch.jit.trace() API. check_inputs takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: def loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n",
        "Y": "def loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)\n"
    },
    {
        "X": "How to use One way to automatically catch many errors in traces is by using check_inputs\non the torch.jit.trace() API. check_inputs takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:Gives us the following diagnostic information:, give an example?",
        "Z": " Gives us the following diagnostic information: ERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            }\n",
        "Y": "ERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            }\n"
    },
    {
        "X": "How to use  , give an example?",
        "Z": "  def fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\n",
        "Y": "def fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))\n"
    },
    {
        "X": "How to use In this case, data-dependent control flow like this can be captured using\ntorch.jit.script() instead:Which produces:, give an example?",
        "Z": " In this case, data-dependent control flow like this can be captured using\ntorch.jit.script() instead:Which produces: graph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -> (%5, %result.2)\n    }\n    return (%result);\n}\n",
        "Y": "graph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -> (%5, %result.2)\n    }\n    return (%result);\n}\n"
    },
    {
        "X": "How to use The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor:Produces several warnings and a graph which simply returns the input:, give an example?",
        "Z": " Produces several warnings and a graph which simply returns the input: fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n}\n",
        "Y": "fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n}\n"
    },
    {
        "X": "How to use First convert your model from GPU to CPU and then save it, like so:, give an example?",
        "Z": " First convert your model from GPU to CPU and then save it, like so: cpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)\n",
        "Y": "cpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)\n"
    },
    {
        "X": "How to use Frequently Asked Questions, give an example?",
        "Z": "  import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())\n",
        "Y": "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())\n"
    },
    {
        "X": "How to use Migrating to PyTorch 1 2 Recursive Scripting API, give an example?",
        "Z": "  import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)\n",
        "Y": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)\n"
    },
    {
        "X": "How to use Old API:New API:, give an example?",
        "Z": " Old API:New API: try:\n    from typing_extensions import Final\nexcept:\n    # If you don't have `typing_extensions` installed, you can use a\n    # polyfill from `torch.jit`.\n    from torch.jit import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n",
        "Y": "try:\n    from typing_extensions import Final\nexcept:\n    # If you don't have `typing_extensions` installed, you can use a\n    # polyfill from `torch.jit`.\n    from torch.jit import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())\n"
    },
    {
        "X": "How to use torch.column_stack, give an example?",
        "Z": " Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t\nin tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally. >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])\n",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])\n"
    },
    {
        "X": "How to use torch.is_tensor, give an example?",
        "Z": " Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor. >>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue\n",
        "Y": ">>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue\n"
    },
    {
        "X": "How to use torch.set_flush_denormal, give an example?",
        "Z": " Returns True if your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.  set_flush_denormal()\nis only supported on x86 architectures supporting SSE3. >>> torch.set_flush_denormal(True)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n>>> torch.set_flush_denormal(False)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n       [ 1.0000], dtype=torch.float64)\n",
        "Y": ">>> torch.set_flush_denormal(True)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n>>> torch.set_flush_denormal(False)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n       [ 1.0000], dtype=torch.float64)\n"
    },
    {
        "X": "How to use torch.einsum, give an example?",
        "Z": " Equation: # trace\n>>> torch.einsum('ii', torch.randn(4, 4))\ntensor(-1.2104)\n\n# diagonal\n>>> torch.einsum('ii->i', torch.randn(4, 4))\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n# outer product\n>>> x = torch.randn(5)\n>>> y = torch.randn(4)\n>>> torch.einsum('i,j->ij', x, y)\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n        [-0.3744,  0.9381,  1.2685, -1.6070],\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n# batch matrix multiplication\n>>> As = torch.randn(3,2,5)\n>>> Bs = torch.randn(3,5,4)\n>>> torch.einsum('bij,bjk->bik', As, Bs)\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# batch permute\n>>> A = torch.randn(2, 3, 4, 5)\n>>> torch.einsum('...ij->...ji', A).shape\ntorch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\n>>> A = torch.randn(3,5,4)\n>>> l = torch.randn(2,5)\n>>> r = torch.randn(2,4)\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])\n",
        "Y": "# trace\n>>> torch.einsum('ii', torch.randn(4, 4))\ntensor(-1.2104)\n\n# diagonal\n>>> torch.einsum('ii->i', torch.randn(4, 4))\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n# outer product\n>>> x = torch.randn(5)\n>>> y = torch.randn(4)\n>>> torch.einsum('i,j->ij', x, y)\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n        [-0.3744,  0.9381,  1.2685, -1.6070],\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n# batch matrix multiplication\n>>> As = torch.randn(3,2,5)\n>>> Bs = torch.randn(3,5,4)\n>>> torch.einsum('bij,bjk->bik', As, Bs)\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# batch permute\n>>> A = torch.randn(2, 3, 4, 5)\n>>> torch.einsum('...ij->...ji', A).shape\ntorch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\n>>> A = torch.randn(3,5,4)\n>>> l = torch.randn(2,5)\n>>> r = torch.randn(2,4)\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])\n"
    },
    {
        "X": "How to use torch.lerp, give an example?",
        "Z": " The shapes of start and end must be\nbroadcastable. If weight is a tensor, then\nthe shapes of weight, start, and end must be broadcastable. >>> start = torch.arange(1., 5.)\n>>> end = torch.empty(4).fill_(10)\n>>> start\ntensor([ 1.,  2.,  3.,  4.])\n>>> end\ntensor([ 10.,  10.,  10.,  10.])\n>>> torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n",
        "Y": ">>> start = torch.arange(1., 5.)\n>>> end = torch.empty(4).fill_(10)\n>>> start\ntensor([ 1.,  2.,  3.,  4.])\n>>> end\ntensor([ 10.,  10.,  10.,  10.])\n>>> torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n"
    },
    {
        "X": "How to use torch.maximum, give an example?",
        "Z": "  >>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.maximum(a, b)\ntensor([3, 2, 4])\n",
        "Y": ">>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.maximum(a, b)\ntensor([3, 2, 4])\n"
    },
    {
        "X": "How to use torch.norm, give an example?",
        "Z": "  >>> import torch\n>>> a = torch.arange(9, dtype= torch.float) - 4\n>>> b = a.reshape((3, 3))\n>>> torch.norm(a)\ntensor(7.7460)\n>>> torch.norm(b)\ntensor(7.7460)\n>>> torch.norm(a, float('inf'))\ntensor(4.)\n>>> torch.norm(b, float('inf'))\ntensor(4.)\n>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n>>> torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n>>> torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n",
        "Y": ">>> import torch\n>>> a = torch.arange(9, dtype= torch.float) - 4\n>>> b = a.reshape((3, 3))\n>>> torch.norm(a)\ntensor(7.7460)\n>>> torch.norm(b)\ntensor(7.7460)\n>>> torch.norm(a, float('inf'))\ntensor(4.)\n>>> torch.norm(b, float('inf'))\ntensor(4.)\n>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n>>> torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n>>> torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))\n"
    },
    {
        "X": "How to use torch.zeros, give an example?",
        "Z": "  >>> torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n>>> torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])\n",
        "Y": ">>> torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n>>> torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])\n"
    },
    {
        "X": "How to use torch.any, give an example?",
        "Z": "  >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)\n",
        "Y": ">>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)\n"
    },
    {
        "X": "How to use torch.lu_solve, give an example?",
        "Z": " This function supports float, double, cfloat and cdouble dtypes for input. >>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(2, 3, 1)\n>>> A_LU = torch.lu(A)\n>>> x = torch.lu_solve(b, *A_LU)\n>>> torch.norm(torch.bmm(A, x) - b)\ntensor(1.00000e-07 *\n       2.8312)\n",
        "Y": ">>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(2, 3, 1)\n>>> A_LU = torch.lu(A)\n>>> x = torch.lu_solve(b, *A_LU)\n>>> torch.norm(torch.bmm(A, x) - b)\ntensor(1.00000e-07 *\n       2.8312)\n"
    },
    {
        "X": "How to use torch.argsort, give an example?",
        "Z": " This is the second value returned by torch.sort().  See its documentation\nfor the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])\n"
    },
    {
        "X": "How to use torch.cummax, give an example?",
        "Z": "  >>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))\n",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))\n"
    },
    {
        "X": "How to use torch.gather, give an example?",
        "Z": " For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n",
        "Y": "out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2\n"
    },
    {
        "X": "How  input and index must have the same number of dimensions.\nIt is also required that index.size(d) <= input.size(d) for all\ndimensions d != dim.  out will have the same shape as index.\nNote that input and index do not broadcast against each other., give an example?",
        "Z": " input and index must have the same number of dimensions.\nIt is also required that index.size(d) <= input.size(d) for all\ndimensions d != dim.  out will have the same shape as index.\nNote that input and index do not broadcast against each other. >>> t = torch.tensor([[1, 2], [3, 4]])\n>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\ntensor([[ 1,  1],\n        [ 4,  3]])\n",
        "Y": ">>> t = torch.tensor([[1, 2], [3, 4]])\n>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\ntensor([[ 1,  1],\n        [ 4,  3]])\n"
    },
    {
        "X": "How to use torch.isreal, give an example?",
        "Z": "  >>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))\ntensor([True, False, True])\n",
        "Y": ">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))\ntensor([True, False, True])\n"
    },
    {
        "X": "How to use torch.nn.init.calculate_gain, give an example?",
        "Z": "  >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n",
        "Y": ">>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n"
    },
    {
        "X": "How to use torch.nn.init.uniform_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.normal_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.constant_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)\n"
    },
    {
        "X": "How to use torch.nn.init.ones_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.zeros_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.eye_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.dirac_, give an example?",
        "Z": "  >>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)\n",
        "Y": ">>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)\n"
    },
    {
        "X": "How to use torch.nn.init.xavier_uniform_, give an example?",
        "Z": " Also known as Glorot initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))\n"
    },
    {
        "X": "How to use torch.nn.init.xavier_normal_, give an example?",
        "Z": " Also known as Glorot initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.kaiming_uniform_, give an example?",
        "Z": " Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')\n"
    },
    {
        "X": "How to use torch.nn.init.kaiming_normal_, give an example?",
        "Z": " Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')\n"
    },
    {
        "X": "How to use torch.nn.init.orthogonal_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)\n"
    },
    {
        "X": "How to use torch.nn.init.sparse_, give an example?",
        "Z": "  >>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)\n",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)\n"
    },
    {
        "X": "How to use torch.addbmm, give an example?",
        "Z": " This operator supports TensorFloat32. >>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n",
        "Y": ">>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])\n"
    },
    {
        "X": "How to use torch.linalg.pinv, give an example?",
        "Z": " Consider using torch.linalg.lstsq() if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: torch.linalg.lstsq(A, B).solution == A.pinv() @ B\n",
        "Y": "torch.linalg.lstsq(A, B).solution == A.pinv() @ B\n"
    },
    {
        "X": "How  The singular values (or the norm of the eigenvalues when hermitian= True)\nthat are below the specified rcond threshold are treated as zero and discarded in the computation., give an example?",
        "Z": " The singular values (or the norm of the eigenvalues when hermitian= True)\nthat are below the specified rcond threshold are treated as zero and discarded in the computation. >>> A = torch.randn(3, 5)\n>>> A\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.linalg.pinv(A)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n\nBatched linalg.pinv example\n>>> A = torch.randn(2, 6, 3)\n>>> B = torch.linalg.pinv(A)\n>>> torch.matmul(B, A).round()\ntensor([[[1., -0., 0.],\n         [0., 1., -0.],\n         [0., 0., 1.]],\n\n        [[1., -0., 0.],\n         [-0., 1., 0.],\n         [-0., -0., 1.]]])\n\nHermitian input example\n>>> A = torch.randn(3, 3, dtype=torch.complex64)\n>>> A = A + A.t().conj()  # creates a Hermitian matrix\n>>> B = torch.linalg.pinv(A, hermitian=True)\n>>> torch.matmul(B, A)\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\n        5.9605e-08-2.3842e-07j],\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\n        -4.7684e-07+1.1921e-07j],\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\n        1.0000e+00-1.7897e-07j]])\n\nNon-default rcond example\n>>> rcond = 0.5\n>>> A = torch.randn(3, 3)\n>>> torch.linalg.pinv(A)\ntensor([[ 0.2971, -0.4280, -2.0111],\n        [-0.0090,  0.6426, -0.1116],\n        [-0.7832, -0.2465,  1.0994]])\n>>> torch.linalg.pinv(A, rcond)\ntensor([[-0.2672, -0.2351, -0.0539],\n        [-0.0211,  0.6467, -0.0698],\n        [-0.4400, -0.3638, -0.0910]])\n\nMatrix-wise rcond example\n>>> A = torch.randn(5, 6, 2, 3, 3)\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\n>>> torch.linalg.pinv(A, rcond)\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\n>>> torch.linalg.pinv(A, rcond)\n",
        "Y": ">>> A = torch.randn(3, 5)\n>>> A\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.linalg.pinv(A)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n\nBatched linalg.pinv example\n>>> A = torch.randn(2, 6, 3)\n>>> B = torch.linalg.pinv(A)\n>>> torch.matmul(B, A).round()\ntensor([[[1., -0., 0.],\n         [0., 1., -0.],\n         [0., 0., 1.]],\n\n        [[1., -0., 0.],\n         [-0., 1., 0.],\n         [-0., -0., 1.]]])\n\nHermitian input example\n>>> A = torch.randn(3, 3, dtype=torch.complex64)\n>>> A = A + A.t().conj()  # creates a Hermitian matrix\n>>> B = torch.linalg.pinv(A, hermitian=True)\n>>> torch.matmul(B, A)\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\n        5.9605e-08-2.3842e-07j],\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\n        -4.7684e-07+1.1921e-07j],\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\n        1.0000e+00-1.7897e-07j]])\n\nNon-default rcond example\n>>> rcond = 0.5\n>>> A = torch.randn(3, 3)\n>>> torch.linalg.pinv(A)\ntensor([[ 0.2971, -0.4280, -2.0111],\n        [-0.0090,  0.6426, -0.1116],\n        [-0.7832, -0.2465,  1.0994]])\n>>> torch.linalg.pinv(A, rcond)\ntensor([[-0.2672, -0.2351, -0.0539],\n        [-0.0211,  0.6467, -0.0698],\n        [-0.4400, -0.3638, -0.0910]])\n\nMatrix-wise rcond example\n>>> A = torch.randn(5, 6, 2, 3, 3)\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\n>>> torch.linalg.pinv(A, rcond)\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\n>>> torch.linalg.pinv(A, rcond)\n"
    },
    {
        "X": "How to use torch.result_type, give an example?",
        "Z": "  >>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\ntorch.float32\n>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\ntorch.uint8\n",
        "Y": ">>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\ntorch.float32\n>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\ntorch.uint8\n"
    },
    {
        "X": "How to use torch.set_default_dtype, give an example?",
        "Z": " The default floating point dtype is initially torch.float32. >>> # initial default for floating point is torch.float32\n>>> torch.tensor([1.2, 3]).dtype\ntorch.float32\n>>> # initial default for floating point is torch.complex64\n>>> torch.tensor([1.2, 3j]).dtype\ntorch.complex64\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\ntorch.complex128\n",
        "Y": ">>> # initial default for floating point is torch.float32\n>>> torch.tensor([1.2, 3]).dtype\ntorch.float32\n>>> # initial default for floating point is torch.complex64\n>>> torch.tensor([1.2, 3j]).dtype\ntorch.complex64\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\ntorch.complex128\n"
    },
    {
        "X": "How to use torch.cummin, give an example?",
        "Z": "  >>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))\n",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))\n"
    },
    {
        "X": "How to use torch.mode, give an example?",
        "Z": " If keepdim is True, the output tensors are of the same size as\ninput except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting\nin the output tensors having 1 fewer dimension than input. >>> a = torch.randint(10, (5,))\n>>> a\ntensor([6, 5, 1, 0, 2])\n>>> b = a + (torch.randn(50, 1) * 5).long()\n>>> torch.mode(b, 0)\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))\n",
        "Y": ">>> a = torch.randint(10, (5,))\n>>> a\ntensor([6, 5, 1, 0, 2])\n>>> b = a + (torch.randn(50, 1) * 5).long()\n>>> torch.mode(b, 0)\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))\n"
    },
    {
        "X": "How  Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor., give an example?",
        "Z": " Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor. >>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue\n",
        "Y": ">>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue\n"
    },
    {
        "X": "How to use torch.distributed.optim.ZeroRedundancyOptimizer, give an example?",
        "Z": " ZeroRedundancyOptimizer use a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. >>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()\n",
        "Y": ">>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()\n"
    },
    {
        "X": "How to use torch.arange, give an example?",
        "Z": " Note that non-integer step is subject to floating point rounding errors when\ncomparing against end; to avoid inconsistency, we advise adding a small epsilon to end\nin such cases. >>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n",
        "Y": ">>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])\n"
    },
    {
        "X": "How to use torch.hypot, give an example?",
        "Z": " The shapes of input and other must be\nbroadcastable. >>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))\ntensor([5.0000, 5.6569, 6.4031])\n",
        "Y": ">>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))\ntensor([5.0000, 5.6569, 6.4031])\n"
    },
    {
        "X": "How to use torch.as_strided, give an example?",
        "Z": "  >>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])\n",
        "Y": ">>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])\n"
    },
    {
        "X": "How to use torch.transpose, give an example?",
        "Z": " The resulting out tensor shares its underlying storage with the\ninput tensor, so changing the content of one would change the content\nof the other. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])\n",
        "Y": ">>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])\n"
    },
    {
        "X": "How to use torch.cross, give an example?",
        "Z": " If dim is not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. >>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n",
        "Y": ">>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n"
    },
    {
        "X": "How to use torch.rot90, give an example?",
        "Z": "  >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.rot90(x, 1, [0, 1])\ntensor([[1, 3],\n        [0, 2]])\n\n>>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]])\n>>> torch.rot90(x, 1, [1, 2])\ntensor([[[1, 3],\n         [0, 2]],\n\n        [[5, 7],\n         [4, 6]]])\n",
        "Y": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.rot90(x, 1, [0, 1])\ntensor([[1, 3],\n        [0, 2]])\n\n>>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]])\n>>> torch.rot90(x, 1, [1, 2])\ntensor([[[1, 3],\n         [0, 2]],\n\n        [[5, 7],\n         [4, 6]]])\n"
    },
    {
        "X": "How to use torch.Tensor.scatter_add_, give an example?",
        "Z": " For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
        "Y": "self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n"
    },
    {
        "X": "How  self, index and src should have same number of\ndimensions. It is also required that index.size(d) <= src.size(d) for all\ndimensions d, and that index.size(d) <= self.size(d) for all dimensions\nd != dim. Note that index and src do not broadcast., give an example?",
        "Z": " self, index and src should have same number of\ndimensions. It is also required that index.size(d) <= src.size(d) for all\ndimensions d, and that index.size(d) <= self.size(d) for all dimensions\nd != dim. Note that index and src do not broadcast. >>> src = torch.ones((2, 5))\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])\n",
        "Y": ">>> src = torch.ones((2, 5))\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])\n"
    },
    {
        "X": "How to use torch.triu, give an example?",
        "Z": " The argument diagonal controls which diagonal to consider. If\ndiagonal = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121] where\nd1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.3480, -0.5211, -0.4573]])\n>>> torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.0000, -1.0680,  0.6602],\n        [ 0.0000,  0.0000, -0.4573]])\n>>> torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n        [ 0.0000,  0.0000,  0.6602],\n        [ 0.0000,  0.0000,  0.0000]])\n>>> torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.0000, -0.5211, -0.4573]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=1)\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=-1)\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.3480, -0.5211, -0.4573]])\n>>> torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.0000, -1.0680,  0.6602],\n        [ 0.0000,  0.0000, -0.4573]])\n>>> torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n        [ 0.0000,  0.0000,  0.6602],\n        [ 0.0000,  0.0000,  0.0000]])\n>>> torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.0000, -0.5211, -0.4573]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=1)\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=-1)\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])\n"
    },
    {
        "X": "How to use torch.polygamma, give an example?",
        "Z": "  >>> a = torch.tensor([1, 0.5])\n>>> torch.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.polygamma(4, a)\ntensor([ -24.8863, -771.4742])\n",
        "Y": ">>> a = torch.tensor([1, 0.5])\n>>> torch.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.polygamma(4, a)\ntensor([ -24.8863, -771.4742])\n"
    },
    {
        "X": "How to use torch.var, give an example?",
        "Z": " If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var(a, unbiased=False)\ntensor(0.1754)\n",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var(a, unbiased=False)\ntensor(0.1754)\n"
    },
    {
        "X": "How to use torch.allclose, give an example?",
        "Z": " elementwise, for all elements of input and other. The behaviour of this function is analogous to\nnumpy.allclose >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n",
        "Y": ">>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue\n"
    },
    {
        "X": "How to use torch.matmul, give an example?",
        "Z": " This operator supports TensorFloat32. >>> # vector x vector\n>>> tensor1 = torch.randn(3)\n>>> tensor2 = torch.randn(3)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n>>> # matrix x vector\n>>> tensor1 = torch.randn(3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n>>> # batched matrix x broadcasted vector\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n>>> # batched matrix x batched matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(10, 4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n>>> # batched matrix x broadcasted matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n",
        "Y": ">>> # vector x vector\n>>> tensor1 = torch.randn(3)\n>>> tensor2 = torch.randn(3)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n>>> # matrix x vector\n>>> tensor1 = torch.randn(3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n>>> # batched matrix x broadcasted vector\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n>>> # batched matrix x batched matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(10, 4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n>>> # batched matrix x broadcasted matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n"
    },
    {
        "X": "How to use torch.count_nonzero, give an example?",
        "Z": "  >>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])\n",
        "Y": ">>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])\n"
    },
    {
        "X": "How to use torch.meshgrid, give an example?",
        "Z": "  >>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([4, 5, 6])\n>>> grid_x, grid_y = torch.meshgrid(x, y)\n>>> grid_x\ntensor([[1, 1, 1],\n        [2, 2, 2],\n        [3, 3, 3]])\n>>> grid_y\ntensor([[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([4, 5, 6])\n>>> grid_x, grid_y = torch.meshgrid(x, y)\n>>> grid_x\ntensor([[1, 1, 1],\n        [2, 2, 2],\n        [3, 3, 3]])\n>>> grid_y\ntensor([[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]])\n"
    },
    {
        "X": "How to use torch.is_nonzero, give an example?",
        "Z": "  >>> torch.is_nonzero(torch.tensor([0.]))\nFalse\n>>> torch.is_nonzero(torch.tensor([1.5]))\nTrue\n>>> torch.is_nonzero(torch.tensor([False]))\nFalse\n>>> torch.is_nonzero(torch.tensor([3]))\nTrue\n>>> torch.is_nonzero(torch.tensor([1, 3, 5]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with more than one value is ambiguous\n>>> torch.is_nonzero(torch.tensor([]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with no values is ambiguous\n",
        "Y": ">>> torch.is_nonzero(torch.tensor([0.]))\nFalse\n>>> torch.is_nonzero(torch.tensor([1.5]))\nTrue\n>>> torch.is_nonzero(torch.tensor([False]))\nFalse\n>>> torch.is_nonzero(torch.tensor([3]))\nTrue\n>>> torch.is_nonzero(torch.tensor([1, 3, 5]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with more than one value is ambiguous\n>>> torch.is_nonzero(torch.tensor([]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with no values is ambiguous\n"
    },
    {
        "X": "How to use A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:, give an example?",
        "Z": " A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: >>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n",
        "Y": ">>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n"
    },
    {
        "X": "How to use A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:, give an example?",
        "Z": " A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: >>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n",
        "Y": ">>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n"
    },
    {
        "X": "How to use For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:, give an example?",
        "Z": " For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n",
        "Y": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n"
    },
    {
        "X": "How to use The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:, give an example?",
        "Z": " The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: >>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n",
        "Y": ">>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n"
    },
    {
        "X": "How to use For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation., give an example?",
        "Z": " For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n",
        "Y": ">>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n"
    },
    {
        "X": "How to use A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor().Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:, give an example?",
        "Z": " Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n",
        "Y": ">>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n"
    },
    {
        "X": "How to use Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:, give an example?",
        "Z": " Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: >>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n",
        "Y": ">>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])\n"
    },
    {
        "X": "How to use Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\nonly:, give an example?",
        "Z": " Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\nonly: >>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n",
        "Y": ">>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)\n"
    },
    {
        "X": "How to use PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write, give an example?",
        "Z": " PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\n",
        "Y": ">>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)\n"
    },
    {
        "X": "How  PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write, give an example?",
        "Z": " PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write >>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])\n",
        "Y": ">>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])\n"
    },
    {
        "X": "How to use PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:, give an example?",
        "Z": " PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: >>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)\n",
        "Y": ">>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)\n"
    },
    {
        "X": "How to use PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:, give an example?",
        "Z": " while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: >>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)\n",
        "Y": ">>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)\n"
    },
    {
        "X": "How to use However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:, give an example?",
        "Z": " However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: >>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)\n",
        "Y": ">>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)\n"
    },
    {
        "X": "How to use Let\u2019s consider the following example:, give an example?",
        "Z": " Let\u2019s consider the following example: >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n",
        "Y": ">>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n"
    },
    {
        "X": "How to use Let\u2019s consider the following example:As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:, give an example?",
        "Z": " As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: >>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue\n",
        "Y": ">>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue\n"
    },
    {
        "X": "How to use As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:, give an example?",
        "Z": " As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: >>> s.sparse_dim(), s.dense_dim()\n(2, 1)\n",
        "Y": ">>> s.sparse_dim(), s.dense_dim()\n(2, 1)\n"
    },
    {
        "X": "How to use NoteCurrently, one can acquire the COO format data only when the tensor\ninstance is coalesced:, give an example?",
        "Z": " Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: >>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n",
        "Y": ">>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first\n"
    },
    {
        "X": "How to use Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced:For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices():, give an example?",
        "Z": " Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced:For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): >>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])\n",
        "Y": ">>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])\n"
    },
    {
        "X": "How to use If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:, give an example?",
        "Z": " If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: >>> s.is_coalesced()\nFalse\n",
        "Y": ">>> s.is_coalesced()\nFalse\n"
    },
    {
        "X": "How to use Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:, give an example?",
        "Z": " Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: >>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])\n",
        "Y": ">>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])\n"
    },
    {
        "X": "How to use When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general.Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:, give an example?",
        "Z": " When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general.Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: >>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])\n",
        "Y": ">>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])\n"
    },
    {
        "X": "How to use Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present., give an example?",
        "Z": " Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. >>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n      col_indices=tensor([0, 1, 0, 1]),\n      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n      dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)\n",
        "Y": ">>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n      col_indices=tensor([0, 1, 0, 1]),\n      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n      dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)\n"
    },
    {
        "X": "How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:, give an example?",
        "Z": " The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: >>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype = torch.float64)\n>>> sp = a._to_sparse_csr()\n>>> sp\ntensor(crow_indices=tensor([0, 1, 3, 3]),\n      col_indices=tensor([2, 0, 1]),\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)\n",
        "Y": ">>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype = torch.float64)\n>>> sp = a._to_sparse_csr()\n>>> sp\ntensor(crow_indices=tensor([0, 1, 3, 3]),\n      col_indices=tensor([2, 0, 1]),\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)\n"
    },
    {
        "X": "How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors., give an example?",
        "Z": " The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. >>> vec = torch.randn(4, 1, dtype=torch.float64)\n>>> sp.matmul(vec)\ntensor([[0.9078],\n        [1.3180],\n        [0.0000]], dtype=torch.float64)\n",
        "Y": ">>> vec = torch.randn(4, 1, dtype=torch.float64)\n>>> sp.matmul(vec)\ntensor([[0.9078],\n        [1.3180],\n        [0.0000]], dtype=torch.float64)\n"
    },
    {
        "X": "How to use torch.sin, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n>>> torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n>>> torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])\n"
    },
    {
        "X": "How to use torch.gradient, give an example?",
        "Z": "  >>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)\n>>> torch.gradient(t)\ntensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\n>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)\n>>> torch.gradient(t, spacing=(coords,))\ntensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])\n",
        "Y": ">>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)\n>>> torch.gradient(t)\ntensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\n>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)\n>>> torch.gradient(t, spacing=(coords,))\ntensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])\n"
    },
    {
        "X": "How to use torch.bincount, give an example?",
        "Z": " The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n",
        "Y": ">>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n"
    },
    {
        "X": "How to use torch.conj, give an example?",
        "Z": "  >>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\n",
        "Y": ">>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])\n"
    },
    {
        "X": "How to use torch.positive, give an example?",
        "Z": "  >>> t = torch.randn(5)\n>>> t\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.positive(t)\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n",
        "Y": ">>> t = torch.randn(5)\n>>> t\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.positive(t)\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n"
    },
    {
        "X": "How to use torch.atan, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])\n"
    },
    {
        "X": "How to use torch.tensor, give an example?",
        "Z": "  >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n        [ 2.2000,  3.1000],\n        [ 4.9000,  5.2000]])\n\n>>> torch.tensor([0, 1])  # Type inference on data\ntensor([ 0,  1])\n\n>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n...              dtype=torch.float64,\n...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\ntensor(3.1416)\n\n>>> torch.tensor([])  # Create an empty tensor (of size (0,))\ntensor([])\n",
        "Y": ">>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n        [ 2.2000,  3.1000],\n        [ 4.9000,  5.2000]])\n\n>>> torch.tensor([0, 1])  # Type inference on data\ntensor([ 0,  1])\n\n>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n...              dtype=torch.float64,\n...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\ntensor(3.1416)\n\n>>> torch.tensor([])  # Create an empty tensor (of size (0,))\ntensor([])\n"
    },
    {
        "X": "How to use torch.triu_indices, give an example?",
        "Z": " The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.triu_indices(3, 3)\n>>> a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, -1)\n>>> a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1],\n        [1, 2, 2]])\n",
        "Y": ">>> a = torch.triu_indices(3, 3)\n>>> a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, -1)\n>>> a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1],\n        [1, 2, 2]])\n"
    },
    {
        "X": "How to use torch.remainder, give an example?",
        "Z": " Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. >>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n",
        "Y": ">>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])\n"
    },
    {
        "X": "How to use torch.eye, give an example?",
        "Z": "  >>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])\n",
        "Y": ">>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])\n"
    },
    {
        "X": "How to use torch.trunc, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])\n"
    },
    {
        "X": "How to use torch.log2, give an example?",
        "Z": "  >>> a = torch.rand(5)\n>>> a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n\n>>> torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\n",
        "Y": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n\n>>> torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])\n"
    },
    {
        "X": "How to use torch.sparse_coo_tensor, give an example?",
        "Z": "  >>> i = torch.tensor([[0, 1, 1],\n...                   [2, 0, 2]])\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\n...                         dtype=torch.float64,\n...                         device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\n",
        "Y": ">>> i = torch.tensor([[0, 1, 1],\n...                   [2, 0, 2]])\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\n...                         dtype=torch.float64,\n...                         device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)\n"
    },
    {
        "X": "How to use torch.logical_or, give an example?",
        "Z": "  >>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_or(a, b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b.double())\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True,  True, False])\n",
        "Y": ">>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_or(a, b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b.double())\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True,  True, False])\n"
    },
    {
        "X": "How to use torch.solve, give an example?",
        "Z": " torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack().X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B)\n",
        "Y": "X = torch.linalg.solve(A, B)\n"
    },
    {
        "X": "How  Supports real-valued and complex-valued inputs., give an example?",
        "Z": " Supports real-valued and complex-valued inputs. >>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n>>> # Batched solver example\n>>> A = torch.randn(2, 3, 1, 4, 4)\n>>> B = torch.randn(2, 3, 1, 4, 6)\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)\n",
        "Y": ">>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n>>> # Batched solver example\n>>> A = torch.randn(2, 3, 1, 4, 4)\n>>> B = torch.randn(2, 3, 1, 4, 6)\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)\n"
    },
    {
        "X": "How to use torch.vander, give an example?",
        "Z": " The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. >>> x = torch.tensor([1, 2, 3, 5])\n>>> torch.vander(x)\ntensor([[  1,   1,   1,   1],\n        [  8,   4,   2,   1],\n        [ 27,   9,   3,   1],\n        [125,  25,   5,   1]])\n>>> torch.vander(x, N=3)\ntensor([[ 1,  1,  1],\n        [ 4,  2,  1],\n        [ 9,  3,  1],\n        [25,  5,  1]])\n>>> torch.vander(x, N=3, increasing=True)\ntensor([[ 1,  1,  1],\n        [ 1,  2,  4],\n        [ 1,  3,  9],\n        [ 1,  5, 25]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3, 5])\n>>> torch.vander(x)\ntensor([[  1,   1,   1,   1],\n        [  8,   4,   2,   1],\n        [ 27,   9,   3,   1],\n        [125,  25,   5,   1]])\n>>> torch.vander(x, N=3)\ntensor([[ 1,  1,  1],\n        [ 4,  2,  1],\n        [ 9,  3,  1],\n        [25,  5,  1]])\n>>> torch.vander(x, N=3, increasing=True)\ntensor([[ 1,  1,  1],\n        [ 1,  2,  4],\n        [ 1,  3,  9],\n        [ 1,  5, 25]])\n"
    },
    {
        "X": "How to use torch.frexp, give an example?",
        "Z": " Supports float inputs. >>> x = torch.arange(9.)\n>>> mantissa, exponent = torch.frexp(x)\n>>> mantissa\ntensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000])\n>>> exponent\ntensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32)\n>>> torch.ldexp(mantissa, exponent)\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])\n",
        "Y": ">>> x = torch.arange(9.)\n>>> mantissa, exponent = torch.frexp(x)\n>>> mantissa\ntensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000])\n>>> exponent\ntensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32)\n>>> torch.ldexp(mantissa, exponent)\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])\n"
    },
    {
        "X": "How to use torch.addcdiv, give an example?",
        "Z": " For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])\n",
        "Y": ">>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])\n"
    },
    {
        "X": "How to use torch.moveaxis, give an example?",
        "Z": " This function is equivalent to NumPy\u2019s moveaxis function. >>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.moveaxis(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.moveaxis(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.moveaxis(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.moveaxis(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])\n",
        "Y": ">>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.moveaxis(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.moveaxis(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.moveaxis(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.moveaxis(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])\n"
    },
    {
        "X": "How to use torch.lgamma, give an example?",
        "Z": "  >>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.lgamma(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n",
        "Y": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.lgamma(a)\ntensor([ 0.5724,  0.0000, -0.1208])\n"
    },
    {
        "X": "How to use torch.futures.Future.add_done_callback, give an example?",
        "Z": "  >>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"This will run after the future has finished.\")\n>>>     print(fut.wait())\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> This will run after the future has finished.\n>>> 5\n",
        "Y": ">>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"This will run after the future has finished.\")\n>>>     print(fut.wait())\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> This will run after the future has finished.\n>>> 5\n"
    },
    {
        "X": "How to use torch.futures.Future.set_exception, give an example?",
        "Z": "  >>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n",
        "Y": ">>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo\n"
    },
    {
        "X": "How to use torch.futures.Future.set_result, give an example?",
        "Z": "  >>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n",
        "Y": ">>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()\n"
    },
    {
        "X": "How to use torch.futures.Future.then, give an example?",
        "Z": "  >>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n",
        "Y": ">>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None\n"
    },
    {
        "X": "How to use torch.futures.collect_all, give an example?",
        "Z": "  >>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1\n",
        "Y": ">>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1\n"
    },
    {
        "X": "How to use torch.fmod, give an example?",
        "Z": " Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])\n",
        "Y": ">>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])\n"
    },
    {
        "X": "How to use torch.dist, give an example?",
        "Z": " The shapes of input and other must be\nbroadcastable. >>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(inf)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)\n",
        "Y": ">>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(inf)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)\n"
    },
    {
        "X": "How to use torch.empty, give an example?",
        "Z": "  >>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')\n>>> torch.empty_like(a)\ntensor([[0, 0, 0],\n        [0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
        "Y": ">>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')\n>>> torch.empty_like(a)\ntensor([[0, 0, 0],\n        [0, 0, 0]], device='cuda:0', dtype=torch.int32)\n"
    },
    {
        "X": "How to use torch.nansum, give an example?",
        "Z": "  >>> a = torch.tensor([1., 2., float('nan'), 4.])\n>>> torch.nansum(a)\ntensor(7.)\n",
        "Y": ">>> a = torch.tensor([1., 2., float('nan'), 4.])\n>>> torch.nansum(a)\ntensor(7.)\n"
    },
    {
        "X": "How  If keepdim is True, the output tensor is of the same size\nas input except in the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in the\noutput tensor having 1 (or len(dim)) fewer dimension(s)., give an example?",
        "Z": " If keepdim is True, the output tensor is of the same size\nas input except in the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in the\noutput tensor having 1 (or len(dim)) fewer dimension(s). >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n        [-0.2993,  0.9138,  0.9337, -1.6864],\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n>>> torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n>>> torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n        [-0.2993,  0.9138,  0.9337, -1.6864],\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n>>> torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n>>> torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])\n"
    },
    {
        "X": "How to use torch.diagonal, give an example?",
        "Z": " Applying torch.diag_embed() to the output of this function with\nthe same arguments yields a diagonal matrix with the diagonal entries\nof the input. However, torch.diag_embed() has different default\ndimensions, so those need to be explicitly specified. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])\n"
    },
    {
        "X": "How to use torch.le, give an example?",
        "Z": " The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, False], [True, True]])\n",
        "Y": ">>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, False], [True, True]])\n"
    },
    {
        "X": "How to use torch.rand, give an example?",
        "Z": " The shape of the tensor is defined by the variable argument size. >>> torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n>>> torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])\n",
        "Y": ">>> torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n>>> torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])\n"
    },
    {
        "X": "How to use torch.take_along_dim, give an example?",
        "Z": " Functions that return indices along a dimension, like torch.argmax() and torch.argsort(),\nare designed to work with this function. See the examples below. >>> t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n>>> max_idx = torch.argmax(t)\n>>> torch.take_along_dim(t, max_idx)\ntensor([60])\n>>> sorted_idx = torch.argsort(t, dim=1)\n>>> torch.take_along_dim(t, sorted_idx, dim=1)\ntensor([[10, 20, 30],\n        [40, 50, 60]])\n",
        "Y": ">>> t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n>>> max_idx = torch.argmax(t)\n>>> torch.take_along_dim(t, max_idx)\ntensor([60])\n>>> sorted_idx = torch.argsort(t, dim=1)\n>>> torch.take_along_dim(t, sorted_idx, dim=1)\ntensor([[10, 20, 30],\n        [40, 50, 60]])\n"
    },
    {
        "X": "How to use torch.isposinf, give an example?",
        "Z": "  >>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isposinf(a)\ntensor([False,  True, False])\n",
        "Y": ">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isposinf(a)\ntensor([False,  True, False])\n"
    },
    {
        "X": "How to use torch.logical_and, give an example?",
        "Z": "  >>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False, False])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_and(a, b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b.double())\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([False, False,  True, False])\n",
        "Y": ">>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False, False])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_and(a, b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b.double())\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([False, False,  True, False])\n"
    },
    {
        "X": "How to use torch.neg, give an example?",
        "Z": "  >>> a = torch.randn(5)\n>>> a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])\n",
        "Y": ">>> a = torch.randn(5)\n>>> a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])\n"
    },
    {
        "X": "How to use torch.trace, give an example?",
        "Z": "  >>> x = torch.arange(1., 10.).view(3, 3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n>>> torch.trace(x)\ntensor(15.)\n",
        "Y": ">>> x = torch.arange(1., 10.).view(3, 3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n>>> torch.trace(x)\ntensor(15.)\n"
    },
    {
        "X": "How to use There are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desired world_size. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed\npackage. group_name is deprecated as well., give an example?",
        "Z": " Note that multicast address is not supported anymore in the latest distributed\npackage. group_name is deprecated as well. import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n",
        "Y": "import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)\n"
    },
    {
        "X": "How to use Another initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desired world_size. The URL should start\nwith file:// and contain a path to a non-existent file (in an existing\ndirectory) on a shared file system. File-system initialization will automatically\ncreate that file if it doesn\u2019t exist, but will not delete the file. Therefore, it\nis your responsibility to make sure that the file is cleaned up before the next\ninit_process_group() call on the same file path/name.Note that automatic rank assignment is not supported anymore in the latest\ndistributed package and group_name is deprecated as well., give an example?",
        "Z": " Note that automatic rank assignment is not supported anymore in the latest\ndistributed package and group_name is deprecated as well. import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n",
        "Y": "import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)\n"
    },
    {
        "X": "How to use Distributed Key Value Store, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")\n"
    },
    {
        "X": "How to use torch.distributed.Store.set, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n"
    },
    {
        "X": "How to use torch.distributed.Store.get, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")\n"
    },
    {
        "X": "How to use torch.distributed.Store.add, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")\n"
    },
    {
        "X": "How to use torch.distributed.Store.wait, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])\n"
    },
    {
        "X": "How to use torch.distributed.Store.num_keys, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()\n"
    },
    {
        "X": "How to use torch.distributed.Store.delete_key, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")\n"
    },
    {
        "X": "How to use torch.distributed.Store.set_timeout, give an example?",
        "Z": "  >>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])\n"
    },
    {
        "X": "How to use The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:, give an example?",
        "Z": " The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams: # Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n",
        "Y": "# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)\n"
    },
    {
        "X": "How to use torch.distributed.broadcast_object_list, give an example?",
        "Z": "  >>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]\n",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]\n"
    },
    {
        "X": "How to use torch.distributed.all_reduce, give an example?",
        "Z": " Complex tensors are supported. >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n",
        "Y": ">>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1\n"
    },
    {
        "X": "How  Complex tensors are supported., give an example?",
        "Z": " Complex tensors are supported. >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n",
        "Y": ">>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1\n"
    },
    {
        "X": "How to use torch.distributed.all_gather, give an example?",
        "Z": " Complex tensors are supported. >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n",
        "Y": ">>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1\n"
    },
    {
        "X": "How to use torch.distributed.all_gather_object, give an example?",
        "Z": "  >>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]\n"
    },
    {
        "X": "How to use torch.distributed.gather_object, give an example?",
        "Z": "  >>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]\n"
    },
    {
        "X": "How to use torch.distributed.scatter_object_list, give an example?",
        "Z": "  >>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]\n"
    },
    {
        "X": "How to use torch.distributed.all_to_all, give an example?",
        "Z": "  >>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n",
        "Y": ">>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3\n"
    },
    {
        "X": "How to use Note that you can use torch.profiler (recommended, only available after 1.8.1)  or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,\nnccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:, give an example?",
        "Z": " Note that you can use torch.profiler (recommended, only available after 1.8.1)  or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,\nnccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator: import torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n",
        "Y": "import torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)\n"
    },
    {
        "X": "How to use For example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0, give an example?",
        "Z": " For example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n",
        "Y": "import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n"
    },
    {
        "X": "How to use Code running on Node 0Code running on Node 1, give an example?",
        "Z": " Code running on Node 0Code running on Node 1 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n",
        "Y": "import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)\n"
    },
    {
        "X": "How to use In both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(--nproc_per_node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU from GPU 0 to\nGPU (nproc_per_node - 1).How to use this module:, give an example?",
        "Z": " In both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(--nproc_per_node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU from GPU 0 to\nGPU (nproc_per_node - 1).How to use this module: >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n",
        "Y": ">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)\n"
    },
    {
        "X": "How to use How to use this module:Node 1: (IP: 192.168.1.1, and has a free port: 1234), give an example?",
        "Z": " How to use this module:Node 1: (IP: 192.168.1.1, and has a free port: 1234) >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n",
        "Y": ">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n"
    },
    {
        "X": "How to use Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?",
        "Z": " Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2: >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n",
        "Y": ">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)\n"
    },
    {
        "X": "How  Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?",
        "Z": " Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2: >>> python -m torch.distributed.launch --help\n",
        "Y": ">>> python -m torch.distributed.launch --help\n"
    },
    {
        "X": "How to use 2. In your training program, you must parse the command-line argument:\n--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument, give an example?",
        "Z": " 2. In your training program, you must parse the command-line argument:\n--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument >>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local_rank\", type=int)\n>>> args = parser.parse_args()\n",
        "Y": ">>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local_rank\", type=int)\n>>> args = parser.parse_args()\n"
    },
    {
        "X": "How to use Parsing the local_rank argumentSet your device to local rank using either, give an example?",
        "Z": " Parsing the local_rank argumentSet your device to local rank using either >>> torch.cuda.set_device(args.local_rank)  # before your code runs\n",
        "Y": ">>> torch.cuda.set_device(args.local_rank)  # before your code runs\n"
    },
    {
        "X": "How to use Set your device to local rank using eitheror, give an example?",
        "Z": " Set your device to local rank using eitheror >>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n",
        "Y": ">>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run\n"
    },
    {
        "X": "How to use or3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module., give an example?",
        "Z": " or3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module. torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')\n",
        "Y": "torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')\n"
    },
    {
        "X": "How to use 3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module.4. In your training program, you can either use regular distributed functions\nor use torch.nn.parallel.DistributedDataParallel() module. If your\ntraining program uses GPUs for training and you would like to use\ntorch.nn.parallel.DistributedDataParallel() module,\nhere is how to configure it., give an example?",
        "Z": " 3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module.4. In your training program, you can either use regular distributed functions\nor use torch.nn.parallel.DistributedDataParallel() module. If your\ntraining program uses GPUs for training and you would like to use\ntorch.nn.parallel.DistributedDataParallel() module,\nhere is how to configure it. model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[args.local_rank],\n                                                  output_device=args.local_rank)\n",
        "Y": "model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[args.local_rank],\n                                                  output_device=args.local_rank)\n"
    },
    {
        "X": "How to use torch.ceil, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])\n"
    },
    {
        "X": "How to use torch.linalg.det, give an example?",
        "Z": " Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices then\nthe output has the same batch dimensions. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.9478,  0.9158, -1.1295],\n        [ 0.9701,  0.7346, -1.8044],\n        [-0.2337,  0.0557,  0.6929]])\n>>> torch.linalg.det(a)\ntensor(0.0934)\n\n>>> out = torch.empty(0)\n>>> torch.linalg.det(a, out=out)\ntensor(0.0934)\n>>> out\ntensor(0.0934)\n\n>>> a = torch.randn(3, 2, 2)\n>>> a\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> torch.linalg.det(a)\ntensor([1.1990, 0.4099, 0.7386])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.9478,  0.9158, -1.1295],\n        [ 0.9701,  0.7346, -1.8044],\n        [-0.2337,  0.0557,  0.6929]])\n>>> torch.linalg.det(a)\ntensor(0.0934)\n\n>>> out = torch.empty(0)\n>>> torch.linalg.det(a, out=out)\ntensor(0.0934)\n>>> out\ntensor(0.0934)\n\n>>> a = torch.randn(3, 2, 2)\n>>> a\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> torch.linalg.det(a)\ntensor([1.1990, 0.4099, 0.7386])\n"
    },
    {
        "X": "How to use torch.nonzero, give an example?",
        "Z": " As a special case, when input has zero dimensions and a nonzero scalar\nvalue, it is treated as a one-dimensional tensor with one element. >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 4]])\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3]])\n>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n(tensor([0, 1, 2, 4]),)\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n>>> torch.nonzero(torch.tensor(5), as_tuple=True)\n(tensor([0]),)\n",
        "Y": ">>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 4]])\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3]])\n>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n(tensor([0, 1, 2, 4]),)\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n>>> torch.nonzero(torch.tensor(5), as_tuple=True)\n(tensor([0]),)\n"
    },
    {
        "X": "How to use torch.reciprocal, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n>>> torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n>>> torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])\n"
    },
    {
        "X": "How to use torch.sort, give an example?",
        "Z": " A namedtuple of (values, indices) is returned, where the values are the\nsorted values and indices are the indices of the elements in the original\ninput tensor. >>> x = torch.randn(3, 4)\n>>> sorted, indices = torch.sort(x)\n>>> sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n>>> indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n>>> sorted, indices = torch.sort(x, 0)\n>>> sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n>>> indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n>>> x = torch.tensor([0, 1] * 9)\n>>> x.sort()\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\n>>> x.sort(stable=True)\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))\n",
        "Y": ">>> x = torch.randn(3, 4)\n>>> sorted, indices = torch.sort(x)\n>>> sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n>>> indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n>>> sorted, indices = torch.sort(x, 0)\n>>> sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n>>> indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n>>> x = torch.tensor([0, 1] * 9)\n>>> x.sort()\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\n>>> x.sort(stable=True)\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))\n"
    },
    {
        "X": "How to use torch.float_power, give an example?",
        "Z": "  >>> a = torch.randint(10, (4,))\n>>> a\ntensor([6, 4, 7, 1])\n>>> torch.float_power(a, 2)\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\n\n>>> a = torch.arange(1, 5)\n>>> a\ntensor([ 1,  2,  3,  4])\n>>> exp = torch.tensor([2, -3, 4, -5])\n>>> exp\ntensor([ 2, -3,  4, -5])\n>>> torch.float_power(a, exp)\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)\n",
        "Y": ">>> a = torch.randint(10, (4,))\n>>> a\ntensor([6, 4, 7, 1])\n>>> torch.float_power(a, 2)\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\n\n>>> a = torch.arange(1, 5)\n>>> a\ntensor([ 1,  2,  3,  4])\n>>> exp = torch.tensor([2, -3, 4, -5])\n>>> exp\ntensor([ 2, -3,  4, -5])\n>>> torch.float_power(a, exp)\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)\n"
    },
    {
        "X": "How to use torch.diag_embed, give an example?",
        "Z": " Applying torch.diagonal() to the output of this function with\nthe same arguments yields a matrix identical to input. However,\ntorch.diagonal() has different default dimensions, so those\nneed to be explicitly specified. >>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n",
        "Y": ">>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n"
    },
    {
        "X": "How to use torch.frac, give an example?",
        "Z": "  >>> torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])\n",
        "Y": ">>> torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])\n"
    },
    {
        "X": "How to use torch.linalg.householder_product, give an example?",
        "Z": " Supports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions. >>> a = torch.randn(2, 2)\n>>> h, tau = torch.geqrf(a)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> torch.allclose(q, torch.linalg.qr(a)[0])\nTrue\n\n>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\n>>> tau = torch.randn(3, 1, dtype=torch.complex128)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> q\ntensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\n        [-0.6853+0.7953j,  2.0790+0.5620j]],\n\n        [[ 1.4581+1.6989j, -1.5360+0.1193j],\n        [ 1.3877-0.6691j,  1.3512+1.3024j]],\n\n        [[ 1.4766+0.5783j,  0.0361+0.6587j],\n        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)\n",
        "Y": ">>> a = torch.randn(2, 2)\n>>> h, tau = torch.geqrf(a)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> torch.allclose(q, torch.linalg.qr(a)[0])\nTrue\n\n>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\n>>> tau = torch.randn(3, 1, dtype=torch.complex128)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> q\ntensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\n        [-0.6853+0.7953j,  2.0790+0.5620j]],\n\n        [[ 1.4581+1.6989j, -1.5360+0.1193j],\n        [ 1.3877-0.6691j,  1.3512+1.3024j]],\n\n        [[ 1.4766+0.5783j,  0.0361+0.6587j],\n        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)\n"
    },
    {
        "X": "How to use torch.tril, give an example?",
        "Z": " The argument diagonal controls which diagonal to consider. If\ndiagonal = 0, all elements on and below the main diagonal are\nretained. A positive value includes just as many diagonals above the main\ndiagonal, and similarly a negative value excludes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121] where\nd1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0813, -0.8619,  0.7105],\n        [ 0.0935,  0.1380,  2.2112],\n        [-0.3409, -0.9828,  0.0289]])\n>>> torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n        [ 0.0935,  0.1380,  0.0000],\n        [-0.3409, -0.9828,  0.0289]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n>>> torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n>>> torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0813, -0.8619,  0.7105],\n        [ 0.0935,  0.1380,  2.2112],\n        [-0.3409, -0.9828,  0.0289]])\n>>> torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n        [ 0.0935,  0.1380,  0.0000],\n        [-0.3409, -0.9828,  0.0289]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n>>> torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n>>> torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])\n"
    },
    {
        "X": "How to use torch.i0, give an example?",
        "Z": "  >>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\n",
        "Y": ">>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])\n"
    },
    {
        "X": "How to use torch.hstack, give an example?",
        "Z": " This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors. >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.hstack((a,b))\ntensor([1, 2, 3, 4, 5, 6])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.hstack((a,b))\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.hstack((a,b))\ntensor([1, 2, 3, 4, 5, 6])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.hstack((a,b))\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])\n"
    },
    {
        "X": "How to use torch.pow, give an example?",
        "Z": " When exponent is a tensor, the shapes of input\nand exponent must be broadcastable. >>> a = torch.randn(4)\n>>> a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n>>> torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n>>> exp = torch.arange(1., 5.)\n\n>>> a = torch.arange(1., 5.)\n>>> a\ntensor([ 1.,  2.,  3.,  4.])\n>>> exp\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n>>> torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n>>> exp = torch.arange(1., 5.)\n\n>>> a = torch.arange(1., 5.)\n>>> a\ntensor([ 1.,  2.,  3.,  4.])\n>>> exp\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])\n"
    },
    {
        "X": "How  The operation applied is:, give an example?",
        "Z": " The operation applied is: >>> exp = torch.arange(1., 5.)\n>>> base = 2\n>>> torch.pow(base, exp)\ntensor([  2.,   4.,   8.,  16.])\n",
        "Y": ">>> exp = torch.arange(1., 5.)\n>>> base = 2\n>>> torch.pow(base, exp)\ntensor([  2.,   4.,   8.,  16.])\n"
    },
    {
        "X": "How to use torch.floor, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])\n"
    },
    {
        "X": "How to use inference mode, give an example?",
        "Z": "  >>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> with torch.inference_mode():\n...   y = x * x\n>>> y.requires_grad\nFalse\n>>> y._version\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: Inference tensors do not track version counter.\n>>> @torch.inference_mode()\n... def func(x):\n...   return x * x\n>>> out = func(x)\n>>> out.requires_grad\nFalse\n",
        "Y": ">>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> with torch.inference_mode():\n...   y = x * x\n>>> y.requires_grad\nFalse\n>>> y._version\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: Inference tensors do not track version counter.\n>>> @torch.inference_mode()\n... def func(x):\n...   return x * x\n>>> out = func(x)\n>>> out.requires_grad\nFalse\n"
    },
    {
        "X": "How to use torch.as_tensor, give an example?",
        "Z": "  >>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])\n",
        "Y": ">>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])\n"
    },
    {
        "X": "How to use torch.trapz, give an example?",
        "Z": "  >>> y = torch.randn((2, 3))\n>>> y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n>>> torch.trapz(y, x)\ntensor([-1.2220,  0.9683])\n",
        "Y": ">>> y = torch.randn((2, 3))\n>>> y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n>>> torch.trapz(y, x)\ntensor([-1.2220,  0.9683])\n"
    },
    {
        "X": "How to use torch.mul, give an example?",
        "Z": " If input is of type FloatTensor or DoubleTensor, other\nshould be a real number, otherwise it should be an integer >>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])\n",
        "Y": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])\n"
    },
    {
        "X": "How  The shapes of input and other must be\nbroadcastable., give an example?",
        "Z": " The shapes of input and other must be\nbroadcastable. >>> a = torch.randn(4, 1)\n>>> a\ntensor([[ 1.1207],\n        [-0.3137],\n        [ 0.0700],\n        [ 0.8378]])\n>>> b = torch.randn(1, 4)\n>>> b\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n>>> torch.mul(a, b)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n        [-0.1614, -0.0382,  0.1645, -0.7021],\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])\n",
        "Y": ">>> a = torch.randn(4, 1)\n>>> a\ntensor([[ 1.1207],\n        [-0.3137],\n        [ 0.0700],\n        [ 0.8378]])\n>>> b = torch.randn(1, 4)\n>>> b\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n>>> torch.mul(a, b)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n        [-0.1614, -0.0382,  0.1645, -0.7021],\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])\n"
    },
    {
        "X": "How to use torch.logsumexp, give an example?",
        "Z": " If keepdim is True, the output tensor is of the same size\nas input except in the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in the\noutput tensor having 1 (or len(dim)) fewer dimension(s). >>> a = torch.randn(3, 3)\n>>> torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])\n"
    },
    {
        "X": "How to use torch.max, give an example?",
        "Z": "  >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n>>> torch.max(a)\ntensor(0.7445)\n",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n>>> torch.max(a)\ntensor(0.7445)\n"
    },
    {
        "X": "How  If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting\nin the output tensors having 1 fewer dimension than input., give an example?",
        "Z": " If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting\nin the output tensors having 1 fewer dimension than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))\n"
    },
    {
        "X": "How to use torch.utils.model_zoo.load_url, give an example?",
        "Z": " If the object is already present in model_dir, it\u2019s deserialized and\nreturned.\nThe default value of model_dir is <hub_dir>/checkpoints where\nhub_dir is the directory returned by get_dir(). >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
        "Y": ">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n"
    },
    {
        "X": "How to use torch.mean, give an example?",
        "Z": "  >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n>>> torch.mean(a)\ntensor(0.3367)\n",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n>>> torch.mean(a)\ntensor(0.3367)\n"
    },
    {
        "X": "How to use torch.quantile, give an example?",
        "Z": " If q is a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size of q, the remaining dimensions are what remains from the reduction. >>> a = torch.randn(2, 3)\n>>> a\ntensor([[ 0.0795, -1.2117,  0.9765],\n        [ 1.1707,  0.6706,  0.4884]])\n>>> q = torch.tensor([0.25, 0.5, 0.75])\n>>> torch.quantile(a, q, dim=1, keepdim=True)\ntensor([[[-0.5661],\n        [ 0.5795]],\n\n        [[ 0.0795],\n        [ 0.6706]],\n\n        [[ 0.5280],\n        [ 0.9206]]])\n>>> torch.quantile(a, q, dim=1, keepdim=True).shape\ntorch.Size([3, 2, 1])\n>>> a = torch.arange(4.)\n>>> a\ntensor([0., 1., 2., 3.])\n",
        "Y": ">>> a = torch.randn(2, 3)\n>>> a\ntensor([[ 0.0795, -1.2117,  0.9765],\n        [ 1.1707,  0.6706,  0.4884]])\n>>> q = torch.tensor([0.25, 0.5, 0.75])\n>>> torch.quantile(a, q, dim=1, keepdim=True)\ntensor([[[-0.5661],\n        [ 0.5795]],\n\n        [[ 0.0795],\n        [ 0.6706]],\n\n        [[ 0.5280],\n        [ 0.9206]]])\n>>> torch.quantile(a, q, dim=1, keepdim=True).shape\ntorch.Size([3, 2, 1])\n>>> a = torch.arange(4.)\n>>> a\ntensor([0., 1., 2., 3.])\n"
    },
    {
        "X": "How to use torch.vdot, give an example?",
        "Z": "  >>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])\n",
        "Y": ">>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])\n"
    },
    {
        "X": "How to use torch.asinh, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])\n"
    },
    {
        "X": "How to use torch.lstsq, give an example?",
        "Z": " torch.lstsq() is deprecated in favor of torch.linalg.lstsq()\nand will be removed in a future PyTorch release. torch.linalg.lstsq()\nhas reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returned solution in torch.lstsq() stores the residuals of the solution in the\nlast m - n columns in the case m > n. In torch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple.Unpacking the solution as``X = torch.lstsq(B, A).solution[:A.size(1)]`` should be replaced with X = torch.linalg.lstsq(A, B).solution\n",
        "Y": "X = torch.linalg.lstsq(A, B).solution\n"
    },
    {
        "X": "How  Returned tensor XXX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k). The first nnn\nrows of XXX contains the solution. If m\u2265nm \\geq nm\u2265n, the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in the\nremaining m\u2212nm - nm\u2212n rows of that column., give an example?",
        "Z": " Returned tensor XXX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k). The first nnn\nrows of XXX contains the solution. If m\u2265nm \\geq nm\u2265n, the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in the\nremaining m\u2212nm - nm\u2212n rows of that column. >>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])\n",
        "Y": ">>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])\n"
    },
    {
        "X": "How to use torch.view_as_real, give an example?",
        "Z": "  >>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])\n>>> torch.view_as_real(x)\ntensor([[ 0.4737, -0.3839],\n        [-0.2098, -0.6699],\n        [ 0.3470, -0.9451],\n        [-0.5174, -1.3136]])\n",
        "Y": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])\n>>> torch.view_as_real(x)\ntensor([[ 0.4737, -0.3839],\n        [-0.2098, -0.6699],\n        [ 0.3470, -0.9451],\n        [-0.5174, -1.3136]])\n"
    },
    {
        "X": "How to use torch.isfinite, give an example?",
        "Z": " Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([True,  False,  True,  False,  False])\n",
        "Y": ">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([True,  False,  True,  False,  False])\n"
    },
    {
        "X": "How to use torch.flipud, give an example?",
        "Z": " Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])\n",
        "Y": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])\n"
    },
    {
        "X": "How to use torch.full, give an example?",
        "Z": "  >>> torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])\n",
        "Y": ">>> torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])\n"
    },
    {
        "X": "How to use torch.Generator, give an example?",
        "Z": "  >>> g_cpu = torch.Generator()\n>>> g_cuda = torch.Generator(device='cuda')\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cuda = torch.Generator(device='cuda')\n"
    },
    {
        "X": "How  Gets the current device of the generator., give an example?",
        "Z": " Gets the current device of the generator. >>> g_cpu = torch.Generator()\n>>> g_cpu.device\ndevice(type='cpu')\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.device\ndevice(type='cpu')\n"
    },
    {
        "X": "How to use torch.Generator.get_state, give an example?",
        "Z": "  >>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()\n"
    },
    {
        "X": "How to use torch.Generator.initial_seed, give an example?",
        "Z": "  >>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647\n"
    },
    {
        "X": "How to use torch.Generator.manual_seed, give an example?",
        "Z": "  >>> g_cpu = torch.Generator()\n>>> g_cpu.manual_seed(2147483647)\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.manual_seed(2147483647)\n"
    },
    {
        "X": "How to use torch.Generator.seed, give an example?",
        "Z": "  >>> g_cpu = torch.Generator()\n>>> g_cpu.seed()\n1516516984916\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.seed()\n1516516984916\n"
    },
    {
        "X": "How to use torch.Generator.set_state, give an example?",
        "Z": "  >>> g_cpu = torch.Generator()\n>>> g_cpu_other = torch.Generator()\n>>> g_cpu.set_state(g_cpu_other.get_state())\n",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu_other = torch.Generator()\n>>> g_cpu.set_state(g_cpu_other.get_state())\n"
    },
    {
        "X": "How to use torch.deg2rad, give an example?",
        "Z": "  >>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])\n",
        "Y": ">>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])\n"
    },
    {
        "X": "How to use torch.var_mean, give an example?",
        "Z": " If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var_mean(a, unbiased=False)\n(tensor(0.1754), tensor(-0.8509))\n",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var_mean(a, unbiased=False)\n(tensor(0.1754), tensor(-0.8509))\n"
    },
    {
        "X": "How to use torch.complex, give an example?",
        "Z": "  >>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64\n",
        "Y": ">>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64\n"
    },
    {
        "X": "How to use torch.triangular_solve, give an example?",
        "Z": " Supports input of float, double, cfloat and cdouble data types. >>> A = torch.randn(2, 2).triu()\n>>> A\ntensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]])\n>>> b = torch.randn(2, 3)\n>>> b\ntensor([[-0.0210,  2.3513, -1.5492],\n        [ 1.5429,  0.7403, -1.0243]])\n>>> torch.triangular_solve(b, A)\ntorch.return_types.triangular_solve(\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\n        [ 1.9320,  0.9270, -1.2826]]),\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]]))\n",
        "Y": ">>> A = torch.randn(2, 2).triu()\n>>> A\ntensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]])\n>>> b = torch.randn(2, 3)\n>>> b\ntensor([[-0.0210,  2.3513, -1.5492],\n        [ 1.5429,  0.7403, -1.0243]])\n>>> torch.triangular_solve(b, A)\ntorch.return_types.triangular_solve(\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\n        [ 1.9320,  0.9270, -1.2826]]),\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]]))\n"
    },
    {
        "X": "How to use torch.ones, give an example?",
        "Z": "  >>> torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n>>> torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])\n",
        "Y": ">>> torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n>>> torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])\n"
    },
    {
        "X": "How to use torch.t, give an example?",
        "Z": " 0-D and 1-D tensors are returned as is. When input is a 2-D tensor this\nis equivalent to transpose(input, 0, 1). >>> x = torch.randn(())\n>>> x\ntensor(0.1995)\n>>> torch.t(x)\ntensor(0.1995)\n>>> x = torch.randn(3)\n>>> x\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> torch.t(x)\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.4875,  0.9158, -0.5872],\n        [ 0.3938, -0.6929,  0.6932]])\n>>> torch.t(x)\ntensor([[ 0.4875,  0.3938],\n        [ 0.9158, -0.6929],\n        [-0.5872,  0.6932]])\n",
        "Y": ">>> x = torch.randn(())\n>>> x\ntensor(0.1995)\n>>> torch.t(x)\ntensor(0.1995)\n>>> x = torch.randn(3)\n>>> x\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> torch.t(x)\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.4875,  0.9158, -0.5872],\n        [ 0.3938, -0.6929,  0.6932]])\n>>> torch.t(x)\ntensor([[ 0.4875,  0.3938],\n        [ 0.9158, -0.6929],\n        [-0.5872,  0.6932]])\n"
    },
    {
        "X": "How to use torch.gt, give an example?",
        "Z": " The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [False, False]])\n",
        "Y": ">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [False, False]])\n"
    },
    {
        "X": "How to use torch.round, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n>>> torch.round(a)\ntensor([ 1.,  1.,  1., -1.])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n>>> torch.round(a)\ntensor([ 1.,  1.,  1., -1.])\n"
    },
    {
        "X": "How to use torch.pca_lowrank, give an example?",
        "Z": " This function returns a namedtuple (U, S, V) which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrix AAA such that A=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT.References: - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n  structure with randomness: probabilistic algorithms for\n  constructing approximate matrix decompositions,\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n  `arXiv <http://arxiv.org/abs/0909.4061>`_).\n",
        "Y": "- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n  structure with randomness: probabilistic algorithms for\n  constructing approximate matrix decompositions,\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n  `arXiv <http://arxiv.org/abs/0909.4061>`_).\n"
    },
    {
        "X": "How to use torch.narrow, give an example?",
        "Z": "  >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n",
        "Y": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])\n"
    },
    {
        "X": "How to use torch.min, give an example?",
        "Z": "  >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n>>> torch.min(a)\ntensor(0.6750)\n",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n>>> torch.min(a)\ntensor(0.6750)\n"
    },
    {
        "X": "How  If keepdim is True, the output tensors are of the same size as\ninput except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension than input., give an example?",
        "Z": " If keepdim is True, the output tensors are of the same size as\ninput except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n        [-1.4644, -0.2635, -0.3651,  0.6134],\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\n>>> torch.min(a, 1)\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))\n",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n        [-1.4644, -0.2635, -0.3651,  0.6134],\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\n>>> torch.min(a, 1)\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))\n"
    },
    {
        "X": "How to use torch.multinomial, give an example?",
        "Z": " If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. >>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n>>> torch.multinomial(weights, 2)\ntensor([1, 2])\n>>> torch.multinomial(weights, 4) # ERROR!\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n>>> torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])\n",
        "Y": ">>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n>>> torch.multinomial(weights, 2)\ntensor([1, 2])\n>>> torch.multinomial(weights, 4) # ERROR!\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n>>> torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])\n"
    },
    {
        "X": "How to use To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019s Linear module.\nThis module applies an affine transformation to its input., give an example?",
        "Z": " To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019s Linear module.\nThis module applies an affine transformation to its input. import torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias\n",
        "Y": "import torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias\n"
    },
    {
        "X": "How to use This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:, give an example?",
        "Z": " This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: m = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n",
        "Y": "m = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)\n"
    },
    {
        "X": "How to use Note that the module itself is callable, and that calling it invokes its forward() function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement a backward() function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail in\nNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call to\nparameters() or named_parameters(),\nwhere the latter includes each parameter\u2019s name:, give an example?",
        "Z": " Note that the module itself is callable, and that calling it invokes its forward() function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement a backward() function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail in\nNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call to\nparameters() or named_parameters(),\nwhere the latter includes each parameter\u2019s name: for parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n",
        "Y": "for parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))\n"
    },
    {
        "X": "How to use Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using the Sequential module. It allows us to chain together\nmultiple modules:, give an example?",
        "Z": " Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using the Sequential module. It allows us to chain together\nmultiple modules: net = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)\n",
        "Y": "net = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)\n"
    },
    {
        "X": "How to use In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation.For example, here\u2019s a simple neural network implemented as a custom module:, give an example?",
        "Z": " In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation.For example, here\u2019s a simple neural network implemented as a custom module: import torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x\n",
        "Y": "import torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x\n"
    },
    {
        "X": "How to use For example, here\u2019s a simple neural network implemented as a custom module:This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children():, give an example?",
        "Z": " For example, here\u2019s a simple neural network implemented as a custom module:This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children(): net = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())\n",
        "Y": "net = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())\n"
    },
    {
        "X": "How to use This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children():To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:, give an example?",
        "Z": " This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children():To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules: class BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())\n",
        "Y": "class BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())\n"
    },
    {
        "X": "How to use To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:, give an example?",
        "Z": " To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict: class DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')\n",
        "Y": "class DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')\n"
    },
    {
        "X": "How to use Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:, give an example?",
        "Z": " Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: for parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))\n",
        "Y": "for parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))\n"
    },
    {
        "X": "How to use For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It\u2019s also easy to move all parameters to a different device or change their precision using\nto():, give an example?",
        "Z": " For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It\u2019s also easy to move all parameters to a different device or change their precision using\nto(): # Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n",
        "Y": "# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)\n"
    },
    {
        "X": "How to use Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers from torch.optim:, give an example?",
        "Z": " Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers from torch.optim: # Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n",
        "Y": "# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()\n"
    },
    {
        "X": "How to use In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the\nkey parts of training are present:After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue of l1\u2019s weight parameter shows that its values are now much closer to 0 (as may be expected):, give an example?",
        "Z": " In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the\nkey parts of training are present:After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue of l1\u2019s weight parameter shows that its values are now much closer to 0 (as may be expected): print(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)\n",
        "Y": "print(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)\n"
    },
    {
        "X": "How to use In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. \u201cstate dictionary\u201d):, give an example?",
        "Z": " In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. \u201cstate dictionary\u201d): # Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>\n",
        "Y": "# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>\n"
    },
    {
        "X": "How to use A module\u2019s state_dict contains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this:, give an example?",
        "Z": " A module\u2019s state_dict contains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this: class RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean\n",
        "Y": "class RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean\n"
    },
    {
        "X": "How to use As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this:Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk:, give an example?",
        "Z": " As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this:Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk: m = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))\n",
        "Y": "m = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))\n"
    },
    {
        "X": "How to use Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent:, give an example?",
        "Z": " Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent: self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\n",
        "Y": "self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)\n"
    },
    {
        "X": "How to use As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with\nto():, give an example?",
        "Z": " As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with\nto(): # Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)\n",
        "Y": "# Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)\n"
    },
    {
        "X": "How  A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:, give an example?",
        "Z": " A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: >>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n",
        "Y": ">>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n"
    },
    {
        "X": "How  A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:, give an example?",
        "Z": " A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: >>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n",
        "Y": ">>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')\n"
    },
    {
        "X": "How  For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:, give an example?",
        "Z": " For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n",
        "Y": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])\n"
    },
    {
        "X": "How  The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:, give an example?",
        "Z": " The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: >>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n",
        "Y": ">>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5\n"
    },
    {
        "X": "How  For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation., give an example?",
        "Z": " For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n",
        "Y": ">>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])\n"
    },
    {
        "X": "How to use torch.flip, give an example?",
        "Z": "  >>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])\n",
        "Y": ">>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])\n"
    },
    {
        "X": "How to use torch.svd, give an example?",
        "Z": " torch.svd() is deprecated in favor of torch.linalg.svd()\nand will be removed in a future PyTorch release.U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with U, S, Vh = torch.linalg.svd(A, full_matrices=not some)\nV = Vh.transpose(-2, -1).conj()\n",
        "Y": "U, S, Vh = torch.linalg.svd(A, full_matrices=not some)\nV = Vh.transpose(-2, -1).conj()\n"
    },
    {
        "X": "How  U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with_, S, _ = torch.svd(A, some=some, compute_uv=False) should be replaced with, give an example?",
        "Z": " U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with_, S, _ = torch.svd(A, some=some, compute_uv=False) should be replaced with S = torch.svdvals(A)\n",
        "Y": "S = torch.svdvals(A)\n"
    },
    {
        "X": "How  Supports input of float, double, cfloat and cdouble data types.\nThe dtypes of U and V are the same as input\u2019s. S will\nalways be real-valued, even if input is complex., give an example?",
        "Z": " Supports input of float, double, cfloat and cdouble data types.\nThe dtypes of U and V are the same as input\u2019s. S will\nalways be real-valued, even if input is complex. >>> a = torch.randn(5, 3)\n>>> a\ntensor([[ 0.2364, -0.7752,  0.6372],\n        [ 1.7201,  0.7394, -0.0504],\n        [-0.3371, -1.0584,  0.5296],\n        [ 0.3550, -0.4022,  1.5569],\n        [ 0.2445, -0.0158,  1.1414]])\n>>> u, s, v = torch.svd(a)\n>>> u\ntensor([[ 0.4027,  0.0287,  0.5434],\n        [-0.1946,  0.8833,  0.3679],\n        [ 0.4296, -0.2890,  0.5261],\n        [ 0.6604,  0.2717, -0.2618],\n        [ 0.4234,  0.2481, -0.4733]])\n>>> s\ntensor([2.3289, 2.0315, 0.7806])\n>>> v\ntensor([[-0.0199,  0.8766,  0.4809],\n        [-0.5080,  0.4054, -0.7600],\n        [ 0.8611,  0.2594, -0.4373]])\n>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(8.6531e-07)\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, v = torch.svd(a_big)\n>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\ntensor(2.6503e-06)\n",
        "Y": ">>> a = torch.randn(5, 3)\n>>> a\ntensor([[ 0.2364, -0.7752,  0.6372],\n        [ 1.7201,  0.7394, -0.0504],\n        [-0.3371, -1.0584,  0.5296],\n        [ 0.3550, -0.4022,  1.5569],\n        [ 0.2445, -0.0158,  1.1414]])\n>>> u, s, v = torch.svd(a)\n>>> u\ntensor([[ 0.4027,  0.0287,  0.5434],\n        [-0.1946,  0.8833,  0.3679],\n        [ 0.4296, -0.2890,  0.5261],\n        [ 0.6604,  0.2717, -0.2618],\n        [ 0.4234,  0.2481, -0.4733]])\n>>> s\ntensor([2.3289, 2.0315, 0.7806])\n>>> v\ntensor([[-0.0199,  0.8766,  0.4809],\n        [-0.5080,  0.4054, -0.7600],\n        [ 0.8611,  0.2594, -0.4373]])\n>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(8.6531e-07)\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, v = torch.svd(a_big)\n>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\ntensor(2.6503e-06)\n"
    },
    {
        "X": "How to use torch.real, give an example?",
        "Z": "  >>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n",
        "Y": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n"
    },
    {
        "X": "How to use torch.xlogy, give an example?",
        "Z": " Similar to SciPy\u2019s scipy.special.xlogy. >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])\n",
        "Y": ">>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])\n"
    },
    {
        "X": "How to use torch.quantize_per_channel, give an example?",
        "Z": "  >>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)\n",
        "Y": ">>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)\n"
    },
    {
        "X": "How to use torch.enable_grad, give an example?",
        "Z": " Also functions as a decorator. (Make sure to instantiate with parenthesis.) >>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n",
        "Y": ">>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue\n"
    },
    {
        "X": "How to use torch.eig, give an example?",
        "Z": " torch.eig() is deprecated in favor of torch.linalg.eig()\nand will be removed in a future PyTorch release.\ntorch.linalg.eig() returns complex tensors of dtype cfloat or cdouble\nrather than real tensors mimicking complex tensors.L, _ = torch.eig(A) should be replaced with L_complex = torch.linalg.eigvals(A)\n",
        "Y": "L_complex = torch.linalg.eigvals(A)\n"
    },
    {
        "X": "How  L, _ = torch.eig(A) should be replaced withL, V = torch.eig(A, eigenvectors=True) should be replaced with, give an example?",
        "Z": " L, _ = torch.eig(A) should be replaced withL, V = torch.eig(A, eigenvectors=True) should be replaced with L_complex, V_complex = torch.linalg.eig(A)\n",
        "Y": "L_complex, V_complex = torch.linalg.eig(A)\n"
    },
    {
        "X": "How to use At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader\nclass.  It represents a Python iterable over a dataset, with support forThese options are configured by the constructor arguments of a\nDataLoader, which has signature:, give an example?",
        "Z": " These options are configured by the constructor arguments of a\nDataLoader, which has signature: DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n",
        "Y": "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)\n"
    },
    {
        "X": "How to use After fetching a list of samples using the indices from sampler, the function\npassed as the collate_fn argument is used to collate lists of samples\ninto batches.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?",
        "Z": " After fetching a list of samples using the indices from sampler, the function\npassed as the collate_fn argument is used to collate lists of samples\ninto batches.In this case, loading from a map-style dataset is roughly equivalent with: for indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n",
        "Y": "for indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])\n"
    },
    {
        "X": "How to use In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?",
        "Z": " In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with: dataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n",
        "Y": "dataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])\n"
    },
    {
        "X": "How to use When automatic batching is disabled, the default collate_fn simply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?",
        "Z": " When automatic batching is disabled, the default collate_fn simply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with: for index in sampler:\n    yield collate_fn(dataset[index])\n",
        "Y": "for index in sampler:\n    yield collate_fn(dataset[index])\n"
    },
    {
        "X": "How  In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?",
        "Z": " In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with: for data in iter(dataset):\n    yield collate_fn(data)\n",
        "Y": "for data in iter(dataset):\n    yield collate_fn(data)\n"
    },
    {
        "X": "How to use See the example below., give an example?",
        "Z": " See the example below. class SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n",
        "Y": "class SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())\n"
    },
    {
        "X": "How to use torch.utils.data.IterableDataset, give an example?",
        "Z": " When a subclass is used with DataLoader, each\nitem in the dataset will be yielded from the DataLoader\niterator. When num_workers > 0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers. get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s __iter__() method or the DataLoader \u2018s\nworker_init_fn option to modify each copy\u2019s behavior. >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]\n",
        "Y": ">>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]\n"
    },
    {
        "X": "How to use torch.utils.data.random_split, give an example?",
        "Z": " Randomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.: >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n",
        "Y": ">>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n"
    },
    {
        "X": "How to use torch.utils.data.WeightedRandomSampler, give an example?",
        "Z": "  >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[4, 4, 1, 4, 5]\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]\n",
        "Y": ">>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[4, 4, 1, 4, 5]\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]\n"
    },
    {
        "X": "How to use torch.utils.data.BatchSampler, give an example?",
        "Z": "  >>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
        "Y": ">>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n"
    },
    {
        "X": "How to use torch.utils.data.distributed.DistributedSampler, give an example?",
        "Z": " It is especially useful in conjunction with\ntorch.nn.parallel.DistributedDataParallel. In such a case, each\nprocess can pass a DistributedSampler instance as a\nDataLoader sampler, and load a subset of the\noriginal dataset that is exclusive to it. >>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)\n",
        "Y": ">>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)\n"
    },
    {
        "X": "How to use torch.sum, give an example?",
        "Z": "  >>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n>>> torch.sum(a)\ntensor(-0.5475)\n",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n>>> torch.sum(a)\ntensor(-0.5475)\n"
    },
    {
        "X": "How to use torch.bitwise_not, give an example?",
        "Z": "  >>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)\n",
        "Y": ">>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)\n"
    },
    {
        "X": "How to use torch.vsplit, give an example?",
        "Z": "  >>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.vsplit(t, 2)\n(tensor([[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]),\n tensor([[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]))\n>>> torch.vsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[12., 13., 14., 15.]]),\n tensor([], size=(0, 4)))\n",
        "Y": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.vsplit(t, 2)\n(tensor([[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]),\n tensor([[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]))\n>>> torch.vsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[12., 13., 14., 15.]]),\n tensor([], size=(0, 4)))\n"
    },
    {
        "X": "How to use torch.hsplit, give an example?",
        "Z": "  >>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))\n",
        "Y": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))\n"
    },
    {
        "X": "How to use torch.fliplr, give an example?",
        "Z": " Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.fliplr(x)\ntensor([[1, 0],\n        [3, 2]])\n",
        "Y": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.fliplr(x)\ntensor([[1, 0],\n        [3, 2]])\n"
    },
    {
        "X": "How to use torch.cosh, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n"
    },
    {
        "X": "How to use torch.rad2deg, give an example?",
        "Z": "  >>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\n>>> torch.rad2deg(a)\ntensor([[ 180.0233, -180.0233],\n        [ 359.9894, -359.9894],\n        [  89.9544,  -89.9544]])\n",
        "Y": ">>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\n>>> torch.rad2deg(a)\ntensor([[ 180.0233, -180.0233],\n        [ 359.9894, -359.9894],\n        [  89.9544,  -89.9544]])\n"
    },
    {
        "X": "How to use torch.broadcast_shapes, give an example?",
        "Z": " This is equivalent to\ntorch.broadcast_tensors(*map(torch.empty, shapes))[0].shape\nbut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])\n",
        "Y": ">>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])\n"
    },
    {
        "X": "How to use torch.get_default_dtype, give an example?",
        "Z": "  >>> torch.get_default_dtype()  # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.get_default_dtype()  # default is now changed to torch.float64\ntorch.float64\n>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\n>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\ntorch.float32\n",
        "Y": ">>> torch.get_default_dtype()  # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.get_default_dtype()  # default is now changed to torch.float64\ntorch.float64\n>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\n>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\ntorch.float32\n"
    },
    {
        "X": "How to use torch.cartesian_prod, give an example?",
        "Z": "  >>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])\n",
        "Y": ">>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])\n"
    },
    {
        "X": "How to use torch.zeros_like, give an example?",
        "Z": "  >>> input = torch.empty(2, 3)\n>>> torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n",
        "Y": ">>> input = torch.empty(2, 3)\n>>> torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n"
    },
    {
        "X": "How to use torch.flatten, give an example?",
        "Z": " Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view,\nor copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can\nbe viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the\nflattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned. >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n>>> torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n",
        "Y": ">>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n>>> torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])\n"
    },
    {
        "X": "How to use torch.bitwise_and, give an example?",
        "Z": "  >>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])\n",
        "Y": ">>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])\n"
    },
    {
        "X": "How to use torch.digamma, give an example?",
        "Z": "  >>> a = torch.tensor([1, 0.5])\n>>> torch.digamma(a)\ntensor([-0.5772, -1.9635])\n",
        "Y": ">>> a = torch.tensor([1, 0.5])\n>>> torch.digamma(a)\ntensor([-0.5772, -1.9635])\n"
    },
    {
        "X": "How to use This feature is under a Beta release and its API may change.FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action:, give an example?",
        "Z": " FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: import torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph(x):\n    %param : [#users=1] = self.param\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\n    return clamp_1\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n\"\"\"\n",
        "Y": "import torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph(x):\n    %param : [#users=1] = self.param\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\n    return clamp_1\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n\"\"\"\n"
    },
    {
        "X": "How to use What is an FX transform? Essentially, it\u2019s a function that looks like this., give an example?",
        "Z": " What is an FX transform? Essentially, it\u2019s a function that looks like this. import torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n",
        "Y": "import torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)\n"
    },
    {
        "X": "How to use NoteIt is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:, give an example?",
        "Z": " It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: import torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n",
        "Y": "import torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm\n"
    },
    {
        "X": "How to use Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is:All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:, give an example?",
        "Z": " All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n",
        "Y": "import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()\n"
    },
    {
        "X": "How to use One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls., give an example?",
        "Z": " One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. import torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n",
        "Y": "import torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)\n"
    },
    {
        "X": "How to use One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls.We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below., give an example?",
        "Z": " We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. # Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n",
        "Y": "# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)\n"
    },
    {
        "X": "How to use Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph.To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph., give an example?",
        "Z": " To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. # Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n",
        "Y": "# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)\n"
    },
    {
        "X": "How to use A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like:, give an example?",
        "Z": " A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: import torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n",
        "Y": "import torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)\n"
    },
    {
        "X": "How to use Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:, give an example?",
        "Z": " Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n",
        "Y": "import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"\n"
    },
    {
        "X": "How to use Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:, give an example?",
        "Z": " Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n",
        "Y": "assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))\n"
    },
    {
        "X": "How to use Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked., give an example?",
        "Z": " Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n",
        "Y": "import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)\n"
    },
    {
        "X": "How to use If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there., give an example?",
        "Z": " If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. # Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n",
        "Y": "# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()\n"
    },
    {
        "X": "How to use GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder., give an example?",
        "Z": " GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. m = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n",
        "Y": "m = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()\n"
    },
    {
        "X": "How to use Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:, give an example?",
        "Z": " Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: # Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph(x, y):\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\n    return add_1\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args      kwargs\n-------------  ------  -----------------------  --------  --------\nplaceholder    x       x                        ()        {}\nplaceholder    y       y                        ()        {}\ncall_function  add_1   <built-in function add>  (x, y)    {}\n\"\"\"\n",
        "Y": "# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph(x, y):\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\n    return add_1\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args      kwargs\n-------------  ------  -----------------------  --------  --------\nplaceholder    x       x                        ()        {}\nplaceholder    y       y                        ()        {}\ncall_function  add_1   <built-in function add>  (x, y)    {}\n\"\"\"\n"
    },
    {
        "X": "How to use Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.Going off of the example above, consider the following code:, give an example?",
        "Z": " Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.Going off of the example above, consider the following code: # Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n",
        "Y": "# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)\n"
    },
    {
        "X": "How to use The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program.For example, let\u2019s examine the following program:, give an example?",
        "Z": " For example, let\u2019s examine the following program: def func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n",
        "Y": "def func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"\n"
    },
    {
        "X": "How to use On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:, give an example?",
        "Z": " On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n",
        "Y": "import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"\n"
    },
    {
        "X": "How to use The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:, give an example?",
        "Z": " The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: def f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})\n",
        "Y": "def f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})\n"
    },
    {
        "X": "How to use FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:, give an example?",
        "Z": " FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: import torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n",
        "Y": "import torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"\n"
    },
    {
        "X": "How to use FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:, give an example?",
        "Z": " The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: torch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n",
        "Y": "torch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"\n"
    },
    {
        "X": "How to use The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:, give an example?",
        "Z": " The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: class MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n",
        "Y": "class MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)\n"
    },
    {
        "X": "How to use Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example:, give an example?",
        "Z": " Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: class MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n",
        "Y": "class MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"\n"
    },
    {
        "X": "How to use Miscellanea, give an example?",
        "Z": "  @torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)\n",
        "Y": "@torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)\n"
    },
    {
        "X": "How to use torch.fx.symbolic_trace, give an example?",
        "Z": " concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures.For example: def f(a, b):\n    if b == True:\n        return a\n    else:\n        return a*2\n",
        "Y": "def f(a, b):\n    if b == True:\n        return a\n    else:\n        return a*2\n"
    },
    {
        "X": "How  Note that although you can still pass in different values of b, they will be ignored.We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example:, give an example?",
        "Z": " Note that although you can still pass in different values of b, they will be ignored.We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: def f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7\n",
        "Y": "def f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7\n"
    },
    {
        "X": "How to use torch.fx.wrap, give an example?",
        "Z": " This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: # foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n",
        "Y": "# foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)\n"
    },
    {
        "X": "How  This function can also equivalently be used as a decorator:, give an example?",
        "Z": " This function can also equivalently be used as a decorator: # foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n",
        "Y": "# foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y\n"
    },
    {
        "X": "How to use For example, the following code, give an example?",
        "Z": " For example, the following code import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n",
        "Y": "import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n"
    },
    {
        "X": "How to use Will produce the following Graph:, give an example?",
        "Z": " For example, the following codeWill produce the following Graph: print(gm.graph)\n",
        "Y": "print(gm.graph)\n"
    },
    {
        "X": "How to use API Reference, give an example?",
        "Z": "  graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n",
        "Y": "graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1\n"
    },
    {
        "X": "How to use torch.fx.Graph.eliminate_dead_code, give an example?",
        "Z": " Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect. def forward(self, x):\n    a = x + 1\n    return x + self.attr_1\n",
        "Y": "def forward(self, x):\n    a = x + 1\n    return x + self.attr_1\n"
    },
    {
        "X": "How  Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect.After dead code is eliminated, a = x + 1 has been removed, and the rest\nof forward remains., give an example?",
        "Z": " Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect.After dead code is eliminated, a = x + 1 has been removed, and the rest\nof forward remains. def forward(self, x):\n    return x + self.attr_1\n",
        "Y": "def forward(self, x):\n    return x + self.attr_1\n"
    },
    {
        "X": "How to use torch.fx.Graph.inserting_after, give an example?",
        "Z": " Set the point at which create_node and companion methods will insert into the graph.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n",
        "Y": "with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently\n"
    },
    {
        "X": "How to use torch.fx.Graph.inserting_before, give an example?",
        "Z": " Set the point at which create_node and companion methods will insert into the graph.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n",
        "Y": "with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently\n"
    },
    {
        "X": "How to use torch.fx.Graph.node_copy, give an example?",
        "Z": "  # Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n",
        "Y": "# Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])\n"
    },
    {
        "X": "How to use torch.fx.Node.prepend, give an example?",
        "Z": "  Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n",
        "Y": "Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax\n"
    },
    {
        "X": "How to use Methods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overrideable methods\nin terms of call hierarchy:, give an example?",
        "Z": " Methods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overrideable methods\nin terms of call hierarchy: run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n",
        "Y": "run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()\n"
    },
    {
        "X": "How to use Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Interpreter like so:, give an example?",
        "Z": " Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Interpreter like so: class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\n",
        "Y": "class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())\n"
    },
    {
        "X": "How to use Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Transformer like so:, give an example?",
        "Z": " Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Transformer like so: class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\n",
        "Y": "class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())\n"
    },
    {
        "X": "How to use torch.fx.replace_pattern, give an example?",
        "Z": " A list of Match objects representing the places\nin the original graph that pattern was matched to. The list\nis empty if there are no matches. Match is defined as: class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n",
        "Y": "class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]\n"
    },
    {
        "X": "How  When the pattern is matched, it will be removed from the larger\nfunction and replaced by replacement. If there are multiple\nmatches for pattern in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node\nis the parameter that appears directly after self, while the\nlast Node is whatever the function returns.)One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacing, give an example?",
        "Z": " When the pattern is matched, it will be removed from the larger\nfunction and replaced by replacement. If there are multiple\nmatches for pattern in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node\nis the parameter that appears directly after self, while the\nlast Node is whatever the function returns.)One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacing def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n",
        "Y": "def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)\n"
    },
    {
        "X": "How  One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacingwith, give an example?",
        "Z": " One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacingwith def replacement(x, y):\n    return torch.relu(x)\n",
        "Y": "def replacement(x, y):\n    return torch.relu(x)\n"
    },
    {
        "X": "How  In this case, replacement needs the same number of parameters\nas pattern (both x and y), even though the parameter\ny isn\u2019t used in replacement.After calling subgraph_rewriter.replace_pattern, the generated\nPython code looks like this:, give an example?",
        "Z": " In this case, replacement needs the same number of parameters\nas pattern (both x and y), even though the parameter\ny isn\u2019t used in replacement.After calling subgraph_rewriter.replace_pattern, the generated\nPython code looks like this: def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n",
        "Y": "def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2\n"
    },
    {
        "X": "How to use torch.split, give an example?",
        "Z": " If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. >>> a = torch.arange(10).reshape(5,2)\n>>> a\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n>>> torch.split(a, 2)\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n>>> torch.split(a, [1,4])\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))\n",
        "Y": ">>> a = torch.arange(10).reshape(5,2)\n>>> a\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n>>> torch.split(a, 2)\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n>>> torch.split(a, [1,4])\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))\n"
    },
    {
        "X": "How to use torch.sign, give an example?",
        "Z": "  >>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n>>> torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])\n",
        "Y": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n>>> torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])\n"
    },
    {
        "X": "How to use torch.utils.cpp_extension.CppExtension, give an example?",
        "Z": " All arguments are forwarded to the setuptools.Extension\nconstructor. >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n",
        "Y": ">>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n"
    },
    {
        "X": "How to use torch.utils.cpp_extension.CUDAExtension, give an example?",
        "Z": " All arguments are forwarded to the setuptools.Extension\nconstructor. >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n",
        "Y": ">>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })\n"
    },
    {
        "X": "How to use torch.utils.cpp_extension.load, give an example?",
        "Z": " CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option. >>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)\n",
        "Y": ">>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)\n"
    },
    {
        "X": "How to use torch.utils.cpp_extension.load_inline, give an example?",
        "Z": " See load() for a description of arguments omitted below. >>> from torch.utils.cpp_extension import load_inline\n>>> source = \\'\\'\\'\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\\'\\'\\'\n>>> module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])\n",
        "Y": ">>> from torch.utils.cpp_extension import load_inline\n>>> source = \\'\\'\\'\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\\'\\'\\'\n>>> module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])\n"
    },
    {
        "X": "How to use torch.repeat_interleave, give an example?",
        "Z": "  >>> x = torch.tensor([1, 2, 3])\n>>> x.repeat_interleave(2)\ntensor([1, 1, 2, 2, 3, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\n>>> torch.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]])\n>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]])\n",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> x.repeat_interleave(2)\ntensor([1, 1, 2, 2, 3, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\n>>> torch.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]])\n>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]])\n"
    },
    {
        "X": "How to use torch.add, give an example?",
        "Z": " If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. >>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])\n"
    },
    {
        "X": "How  If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer., give an example?",
        "Z": " If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. >>> a = torch.randn(4)\n>>> a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(a, b, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(a, b, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])\n"
    },
    {
        "X": "How to use torch.kron, give an example?",
        "Z": " Supports real-valued and complex-valued inputs. >>> mat1 = torch.eye(2)\n>>> mat2 = torch.ones(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 1., 0., 0.],\n        [1., 1., 0., 0.],\n        [0., 0., 1., 1.],\n        [0., 0., 1., 1.]])\n\n>>> mat1 = torch.eye(2)\n>>> mat2 = torch.arange(1, 5).reshape(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]])\n",
        "Y": ">>> mat1 = torch.eye(2)\n>>> mat2 = torch.ones(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 1., 0., 0.],\n        [1., 1., 0., 0.],\n        [0., 0., 1., 1.],\n        [0., 0., 1., 1.]])\n\n>>> mat1 = torch.eye(2)\n>>> mat2 = torch.arange(1, 5).reshape(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]])\n"
    },
    {
        "X": "How to use torch.polar, give an example?",
        "Z": "  >>> import numpy as np\n>>> abs = torch.tensor([1, 2], dtype=torch.float64)\n>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n>>> z = torch.polar(abs, angle)\n>>> z\ntensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)\n",
        "Y": ">>> import numpy as np\n>>> abs = torch.tensor([1, 2], dtype=torch.float64)\n>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n>>> z = torch.polar(abs, angle)\n>>> z\ntensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)\n"
    },
    {
        "X": "How to use torch.use_deterministic_algorithms, give an example?",
        "Z": " Note that deterministic operations tend to have worse performance than\nnondeterministic operations. >>> torch.use_deterministic_algorithms(True)\n\n# Forward mode nondeterministic error\n>>> torch.randn(10).index_copy(0, torch.tensor([0]), torch.randn(1))\n...\nRuntimeError: index_copy does not have a deterministic implementation...\n\n# Backward mode nondeterministic error\n>>> torch.randn(10, requires_grad=True, device='cuda').index_select(0, torch.tensor([0], device='cuda')).backward()\n...\nRuntimeError: index_add_cuda_ does not have a deterministic implementation...\n",
        "Y": ">>> torch.use_deterministic_algorithms(True)\n\n# Forward mode nondeterministic error\n>>> torch.randn(10).index_copy(0, torch.tensor([0]), torch.randn(1))\n...\nRuntimeError: index_copy does not have a deterministic implementation...\n\n# Backward mode nondeterministic error\n>>> torch.randn(10, requires_grad=True, device='cuda').index_select(0, torch.tensor([0], device='cuda')).backward()\n...\nRuntimeError: index_add_cuda_ does not have a deterministic implementation...\n"
    },
    {
        "X": "How to use torch.addcmul, give an example?",
        "Z": " For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])\n",
        "Y": ">>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])\n"
    },
    {
        "X": "How to use torch.fmax, give an example?",
        "Z": " Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])\n",
        "Y": ">>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])\n"
    },
    {
        "X": "How to use torch.tensor_split, give an example?",
        "Z": "  >>> x = torch.arange(8)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))\n\n>>> x = torch.arange(7)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))\n>>> torch.tensor_split(x, (1, 6))\n(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))\n\n>>> x = torch.arange(14).reshape(2, 7)\n>>> x\ntensor([[ 0,  1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12, 13]])\n>>> torch.tensor_split(x, 3, dim=1)\n(tensor([[0, 1, 2],\n        [7, 8, 9]]),\n tensor([[ 3,  4],\n        [10, 11]]),\n tensor([[ 5,  6],\n        [12, 13]]))\n>>> torch.tensor_split(x, (1, 6), dim=1)\n(tensor([[0],\n        [7]]),\n tensor([[ 1,  2,  3,  4,  5],\n        [ 8,  9, 10, 11, 12]]),\n tensor([[ 6],\n        [13]]))\n",
        "Y": ">>> x = torch.arange(8)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))\n\n>>> x = torch.arange(7)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))\n>>> torch.tensor_split(x, (1, 6))\n(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))\n\n>>> x = torch.arange(14).reshape(2, 7)\n>>> x\ntensor([[ 0,  1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12, 13]])\n>>> torch.tensor_split(x, 3, dim=1)\n(tensor([[0, 1, 2],\n        [7, 8, 9]]),\n tensor([[ 3,  4],\n        [10, 11]]),\n tensor([[ 5,  6],\n        [12, 13]]))\n>>> torch.tensor_split(x, (1, 6), dim=1)\n(tensor([[0],\n        [7]]),\n tensor([[ 1,  2,  3,  4,  5],\n        [ 8,  9, 10, 11, 12]]),\n tensor([[ 6],\n        [13]]))\n"
    },
    {
        "X": "How to use torch.std, give an example?",
        "Z": " If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std(a, unbiased=False)\ntensor(0.4188)\n",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std(a, unbiased=False)\ntensor(0.4188)\n"
    },
    {
        "X": "How to use torch.cos, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n"
    },
    {
        "X": "How to use This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size.Diagram:, give an example?",
        "Z": " Diagram: # original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                 /\nlinear_weight_fp32\n\n# dynamically quantized model\n# linear and LSTM weights are in int8\nprevious_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n                     /\n   linear_weight_int8\n",
        "Y": "# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                 /\nlinear_weight_fp32\n\n# dynamically quantized model\n# linear and LSTM weights are in int8\nprevious_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n                     /\n   linear_weight_int8\n"
    },
    {
        "X": "How to use Diagram:API example:, give an example?",
        "Z": " Diagram:API example: import torch\n\n# define a floating point model\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.fc = torch.nn.Linear(4, 4)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n# create a quantized model instance\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_fp32,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model\ninput_fp32 = torch.randn(4, 4, 4, 4)\nres = model_int8(input_fp32)\n",
        "Y": "import torch\n\n# define a floating point model\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.fc = torch.nn.Linear(4, 4)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n# create a quantized model instance\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_fp32,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model\ninput_fp32 = torch.randn(4, 4, 4, 4)\nres = model_int8(input_fp32)\n"
    },
    {
        "X": "How to use Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ.Diagram:, give an example?",
        "Z": " Diagram: # original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                    /\n    linear_weight_fp32\n\n# statically quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                    /\n  linear_weight_int8\n",
        "Y": "# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                    /\n    linear_weight_fp32\n\n# statically quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                    /\n  linear_weight_int8\n"
    },
    {
        "X": "How to use Diagram:, give an example?",
        "Z": " Diagram: import torch\n\n# define a floating point model where some layers could be statically quantized\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval mode for static quantization logic to work\nmodel_fp32.eval()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# Fuse the activations to preceding layers, where applicable.\n# This needs to be done manually depending on the model architecture.\n# Common fusions include `conv + relu` and `conv + batchnorm + relu`\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n\n# Prepare the model for static quantization. This inserts observers in\n# the model that will observe activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n\n# calibrate the prepared model to determine quantization parameters for activations\n# in a real world setting, the calibration would be done with a representative dataset\ninput_fp32 = torch.randn(4, 1, 4, 4)\nmodel_fp32_prepared(input_fp32)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, and replaces key operators with quantized\n# implementations.\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)\n",
        "Y": "import torch\n\n# define a floating point model where some layers could be statically quantized\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval mode for static quantization logic to work\nmodel_fp32.eval()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# Fuse the activations to preceding layers, where applicable.\n# This needs to be done manually depending on the model architecture.\n# Common fusions include `conv + relu` and `conv + batchnorm + relu`\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n\n# Prepare the model for static quantization. This inserts observers in\n# the model that will observe activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n\n# calibrate the prepared model to determine quantization parameters for activations\n# in a real world setting, the calibration would be done with a representative dataset\ninput_fp32 = torch.randn(4, 1, 4, 4)\nmodel_fp32_prepared(input_fp32)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, and replaces key operators with quantized\n# implementations.\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)\n"
    },
    {
        "X": "How to use Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT.Diagram:, give an example?",
        "Z": " Diagram: # original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                      /\n    linear_weight_fp32\n\n# model with fake_quants for modeling quantization numerics during training\nprevious_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n                           /\n   linear_weight_fp32 -- fq\n\n# quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                     /\n   linear_weight_int8\n",
        "Y": "# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                      /\n    linear_weight_fp32\n\n# model with fake_quants for modeling quantization numerics during training\nprevious_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n                           /\n   linear_weight_fp32 -- fq\n\n# quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                     /\n   linear_weight_int8\n"
    },
    {
        "X": "How  Diagram:, give an example?",
        "Z": " Diagram: import torch\n\n# define a floating point model where some layers could benefit from QAT\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to train mode for QAT logic to work\nmodel_fp32.train()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n\n# fuse the activations to preceding layers, where applicable\n# this needs to be done manually depending on the model architecture\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n    [['conv', 'bn', 'relu']])\n\n# Prepare the model for QAT. This inserts observers and fake_quants in\n# the model that will observe weight and activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n\n# run the training loop (not shown)\ntraining_loop(model_fp32_prepared)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, fuses modules where appropriate,\n# and replaces key operators with quantized implementations.\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)\n",
        "Y": "import torch\n\n# define a floating point model where some layers could benefit from QAT\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to train mode for QAT logic to work\nmodel_fp32.train()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n\n# fuse the activations to preceding layers, where applicable\n# this needs to be done manually depending on the model architecture\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n    [['conv', 'bn', 'relu']])\n\n# Prepare the model for QAT. This inserts observers and fake_quants in\n# the model that will observe weight and activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n\n# run the training loop (not shown)\ntraining_loop(model_fp32_prepared)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, fuses modules where appropriate,\n# and replaces key operators with quantized implementations.\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)\n"
    },
    {
        "X": "How to use There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function)., give an example?",
        "Z": " There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). import torch.quantization.quantize_fx as quantize_fx\nimport copy\n\nmodel_fp = UserModel(...)\n\n#\n# post training dynamic/weight_only quantization\n#\n\n# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_to_quantize.eval()\nqconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# no calibration needed when we only have dynamici/weight_only quantization\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# post training static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\nmodel_to_quantize.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# calibrate (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# quantization aware training for static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig('qnnpack')}\nmodel_to_quantize.train()\n# prepare\nmodel_prepared = quantize_fx.prepare_qat_fx(model_to_qunatize, qconfig_dict)\n# training loop (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# fusion\n#\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_fused = quantize_fx.fuse_fx(model_to_quantize)\n",
        "Y": "import torch.quantization.quantize_fx as quantize_fx\nimport copy\n\nmodel_fp = UserModel(...)\n\n#\n# post training dynamic/weight_only quantization\n#\n\n# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_to_quantize.eval()\nqconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# no calibration needed when we only have dynamici/weight_only quantization\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# post training static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\nmodel_to_quantize.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# calibrate (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# quantization aware training for static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig('qnnpack')}\nmodel_to_quantize.train()\n# prepare\nmodel_prepared = quantize_fx.prepare_qat_fx(model_to_qunatize, qconfig_dict)\n# training loop (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# fusion\n#\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_fused = quantize_fx.fuse_fx(model_to_quantize)\n"
    },
    {
        "X": "How to use If you see an error similar to:, give an example?",
        "Z": " If you see an error similar to: RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...\n",
        "Y": "RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...\n"
    },
    {
        "X": "How to use If you see an error similar to:This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:, give an example?",
        "Z": " This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv(x)\n        return x\n",
        "Y": "class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv(x)\n        return x\n"
    },
    {
        "X": "How  If you see an error similar to:, give an example?",
        "Z": " If you see an error similar to: RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.\n",
        "Y": "RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.\n"
    },
    {
        "X": "How to use If you see an error similar to:This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:, give an example?",
        "Z": " This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n        # this module will not be quantized (see `qconfig = None` logic below)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv1(x)\n        # during the convert step, this will be replaced with a\n        # `dequantize` call\n        x = self.dequant(x)\n        x = self.conv2(x)\n        return x\n\nm = M()\nm.qconfig = some_qconfig\n# turn off quantization for conv2\nm.conv2.qconfig = None\n",
        "Y": "class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n        # this module will not be quantized (see `qconfig = None` logic below)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv1(x)\n        # during the convert step, this will be replaced with a\n        # `dequantize` call\n        x = self.dequant(x)\n        x = self.conv2(x)\n        return x\n\nm = M()\nm.qconfig = some_qconfig\n# turn off quantization for conv2\nm.conv2.qconfig = None\n"
    },
    {
        "X": "How to use torch.fake_quantize_per_tensor_affine, give an example?",
        "Z": "  >>> x = torch.randn(4)\n>>> x\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\ntensor([0.1000, 1.0000, 0.4000, 0.0000])\n",
        "Y": ">>> x = torch.randn(4)\n>>> x\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\ntensor([0.1000, 1.0000, 0.4000, 0.0000])\n"
    },
    {
        "X": "How to use torch.baddbmm, give an example?",
        "Z": " This operator supports TensorFloat32. >>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n",
        "Y": ">>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])\n"
    },
    {
        "X": "How to use torch.ge, give an example?",
        "Z": " The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, True], [False, True]])\n",
        "Y": ">>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, True], [False, True]])\n"
    },
    {
        "X": "How to use torch.all, give an example?",
        "Z": "  >>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)\n",
        "Y": ">>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)\n"
    },
    {
        "X": "How to use torch.dot, give an example?",
        "Z": "  >>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n",
        "Y": ">>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n"
    },
    {
        "X": "How to use Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:, give an example?",
        "Z": " Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: import torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()\n",
        "Y": "import torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()\n"
    },
    {
        "X": "How to use The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\nand runnable with:, give an example?",
        "Z": " The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\nand runnable with: pip install tensorboard\ntensorboard --logdir=runs\n",
        "Y": "pip install tensorboard\ntensorboard --logdir=runs\n"
    },
    {
        "X": "How to use This can then be visualized with TensorBoard, which should be installable\nand runnable with:Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface., give an example?",
        "Z": " This can then be visualized with TensorBoard, which should be installable\nand runnable with:Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.__init__, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalar, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalars, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_histogram, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_image, give an example?",
        "Z": " Note that this requires the pillow package. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_images, give an example?",
        "Z": " Note that this requires the pillow package. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_text, give an example?",
        "Z": "  writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n",
        "Y": "writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_embedding, give an example?",
        "Z": "  import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n",
        "Y": "import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars, give an example?",
        "Z": "  layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n",
        "Y": "layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_mesh, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()\n"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_hparams, give an example?",
        "Z": "  from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})\n"
    },
    {
        "X": "How to use torch.linspace, give an example?",
        "Z": "  >>> torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n>>> torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=1)\ntensor([-10.])\n",
        "Y": ">>> torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n>>> torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=1)\ntensor([-10.])\n"
    },
    {
        "X": "How to use torch.lcm, give an example?",
        "Z": " Both input and other must have integer types. >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.lcm(a, b)\ntensor([15, 20, 15])\n>>> c = torch.tensor([3])\n>>> torch.lcm(a, c)\ntensor([15, 30, 15])\n",
        "Y": ">>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.lcm(a, b)\ntensor([15, 20, 15])\n>>> c = torch.tensor([3])\n>>> torch.lcm(a, c)\ntensor([15, 30, 15])\n"
    },
    {
        "X": "How to use The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.In fact, resetting all .grads to None before each\naccumulation phase, e.g.:, give an example?",
        "Z": " The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.In fact, resetting all .grads to None before each\naccumulation phase, e.g.: for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n",
        "Y": "for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()\n"
    },
    {
        "X": "How to use torch.autograd.Function, give an example?",
        "Z": " Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. >>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)\n",
        "Y": ">>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)\n"
    },
    {
        "X": "How to use torch.autograd.profiler.profile, give an example?",
        "Z": "  >>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n",
        "Y": ">>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------\n"
    },
    {
        "X": "How to use It is useful when running the program under nvprof:, give an example?",
        "Z": " It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n",
        "Y": "nvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n"
    },
    {
        "X": "How to use torch.autograd.profiler.emit_nvtx, give an example?",
        "Z": " Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. >>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n",
        "Y": ">>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)\n"
    },
    {
        "X": "How to use torch.autograd.detect_anomaly, give an example?",
        "Z": " This does two things: >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n",
        "Y": ">>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n"
    },
    {
        "X": "How to use torch.chain_matmul, give an example?",
        "Z": "  >>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n",
        "Y": ">>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])\n"
    },
    {
        "X": "How to use torch.tanh, give an example?",
        "Z": "  >>> a = torch.randn(4)\n>>> a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n>>> torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n>>> torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])\n"
    },
    {
        "X": "How to use torch.atan2, give an example?",
        "Z": " The shapes of input and other must be\nbroadcastable. >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])\n"
    },
    {
        "X": "How to use torch.ne, give an example?",
        "Z": " The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [True, False]])\n",
        "Y": ">>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [True, False]])\n"
    },
    {
        "X": "How to use torch.msort, give an example?",
        "Z": "  >>> t = torch.randn(3, 4)\n>>> t\ntensor([[-0.1321,  0.4370, -1.2631, -1.1289],\n        [-2.0527, -1.1250,  0.2275,  0.3077],\n        [-0.0881, -0.1259, -0.5495,  1.0284]])\n>>> torch.msort(t)\ntensor([[-2.0527, -1.1250, -1.2631, -1.1289],\n        [-0.1321, -0.1259, -0.5495,  0.3077],\n        [-0.0881,  0.4370,  0.2275,  1.0284]])\n",
        "Y": ">>> t = torch.randn(3, 4)\n>>> t\ntensor([[-0.1321,  0.4370, -1.2631, -1.1289],\n        [-2.0527, -1.1250,  0.2275,  0.3077],\n        [-0.0881, -0.1259, -0.5495,  1.0284]])\n>>> torch.msort(t)\ntensor([[-2.0527, -1.1250, -1.2631, -1.1289],\n        [-0.1321, -0.1259, -0.5495,  0.3077],\n        [-0.0881,  0.4370,  0.2275,  1.0284]])\n"
    },
    {
        "X": "How to use torch.utils.bottleneck is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler.Run it on the command line with, give an example?",
        "Z": " Run it on the command line with python -m torch.utils.bottleneck /path/to/source/script.py [args]\n",
        "Y": "python -m torch.utils.bottleneck /path/to/source/script.py [args]\n"
    },
    {
        "X": "How to use torch.utils.checkpoint.checkpoint_sequential, give an example?",
        "Z": " See checkpoint() on how checkpointing works. >>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n",
        "Y": ">>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n"
    },
    {
        "X": "How to use torch.unbind, give an example?",
        "Z": " Returns a tuple of all slices along a given dimension, already without it. >>> torch.unbind(torch.tensor([[1, 2, 3],\n>>>                            [4, 5, 6],\n>>>                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n",
        "Y": ">>> torch.unbind(torch.tensor([[1, 2, 3],\n>>>                            [4, 5, 6],\n>>>                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n"
    },
    {
        "X": "How to use torch.cholesky_inverse, give an example?",
        "Z": " If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n"
    },
    {
        "X": "How to use torch.lu, give an example?",
        "Z": "  >>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = torch.lu(A)\n>>> A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n         [ 0.1684,  1.1551,  0.1940],\n         [ 0.1193,  0.6189, -0.5497]],\n\n        [[ 0.4526,  1.2526, -0.3285],\n         [-0.7988,  0.7175, -0.9701],\n         [ 0.2634, -0.9255, -0.3459]]])\n>>> pivots\ntensor([[ 3,  3,  3],\n        [ 3,  3,  3]], dtype=torch.int32)\n>>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n>>> if info.nonzero().size(0) == 0:\n...   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!\n",
        "Y": ">>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = torch.lu(A)\n>>> A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n         [ 0.1684,  1.1551,  0.1940],\n         [ 0.1193,  0.6189, -0.5497]],\n\n        [[ 0.4526,  1.2526, -0.3285],\n         [-0.7988,  0.7175, -0.9701],\n         [ 0.2634, -0.9255, -0.3459]]])\n>>> pivots\ntensor([[ 3,  3,  3],\n        [ 3,  3,  3]], dtype=torch.int32)\n>>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n>>> if info.nonzero().size(0) == 0:\n...   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!\n"
    },
    {
        "X": "How to use torch.igamma, give an example?",
        "Z": " Supports broadcasting to a common shape\nand float inputs. >>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])\n",
        "Y": ">>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])\n"
    },
    {
        "X": "How to use torch.load, give an example?",
        "Z": " User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). >>> torch.load('tensors.pt')\n# Load all tensors onto the CPU\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n>>> with open('tensor.pt', 'rb') as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n>>> torch.load('module.pt', encoding='ascii')\n",
        "Y": ">>> torch.load('tensors.pt')\n# Load all tensors onto the CPU\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n>>> with open('tensor.pt', 'rb') as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n>>> torch.load('module.pt', encoding='ascii')\n"
    }
]