[
    {
        "X": "InferenceMode is an analogous to what?",
        "Y": "no_grad",
        "Z": "InferenceMode is a new context manager analogous to no_grad to be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "Code run under InferenceMode gets better performance by disabling what?",
        "Y": "by disabling view tracking and version counter bumps",
        "Z": "InferenceMode is a new context manager analogous  to no_grad to be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "Whataracteristic the  context manager  InferenceMode have?",
        "Y": "thread local",
        "Z": "Context-manager that enables or disables inference mode InferenceMode is a new context manager analogous  to no_grad to be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "InferenceMode functions as what?",
        "Y": "decorator",
        "Z": "InferenceMode is a new context manager analogous  to no_grad to be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "What do you make sure to make  InferenceMode work as decorator?",
        "Y": "instantiate with parenthesis",
        "Z": "InferenceMode is a new context manager analogous  to no_grad to be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "What does InferenceMode do?",
        "Y": "enable or disable gradients locally",
        "Z": "InferenceMode is a new context manager analogous  to no_grad to be used when you are certain your operations will have no interactions\nwith autograd (e.g., model training). Code run under this mode gets better\nperformance by disabling view tracking and version counter bumps. This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "What does the context manager function as?",
        "Y": "decorator",
        "Z": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "What does mode(bool) do?",
        "Y": "Flag whether to enable or disable inference mode",
        "Z": "This context manager is thread local; it will not affect computation\nin other threads. Also functions as a decorator. (Make sure to instantiate with parenthesis.) Note Inference mode is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. mode(bool) \u2013 Flag whether to enable or disable inference mode",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "What does torch.trapz do?",
        "Y": "Estimate ∫y dx along dim, using trapezoid rule",
        "Z": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim.",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    {
        "X": "what torch.trapz return?",
        "Y": "A Tensor with the same shape as the input, each element being integral along dim",
        "Z": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim.",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    {
        "X": "for the dim  parameter to torch.trapz- By default, what is the dimension along which to integrate?",
        "Y": "the last dimension",
        "Z": "Estimate\u222bydx\\int y\\,dx\u222bydxalongdim, using the trapezoid rule. y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim. Example: As above, but the sample points are spaced uniformly at a distance ofdx. y(Tensor) \u2013 The values of the function to integrate dx(float) \u2013 The distance between points at whichyis sampled. dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension. A Tensor with the same shape as the input, except withdimremoved.\nEach element of the returned tensor represents the estimated integral\u222bydx\\int y\\,dx\u222bydxalongdim.",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    {
        "X": "for the dx parameter to torch.trapz What is the point at which the functiony is sampled?",
        "Y": "x(Tensor)",
        "Z": "y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    {
        "X": "for the x  parameter to torch.trapz -If the functionyis not in ascending order, what contributes negatively to the estimated integral?",
        "Y": "Ifxis not in ascending order",
        "Z": "y(Tensor) \u2013 The values of the function to integrate x(Tensor) \u2013 The points at which the functionyis sampled.\nIfxis not in ascending order, intervals on which it is decreasing\ncontribute negatively to the estimated integral (i.e., the convention\u222babf=\u2212\u222bbaf\\int_a^b f = -\\int_b^a f\u222bab\u200bf=\u2212\u222bba\u200bfis followed). dim(int) \u2013 The dimension along which to integrate.\nBy default, use the last dimension.",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    {
        "X": "If inputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should",
        "Y": "integer",
        "Z": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    {
        "X": "for torch.mul What ore the parameters and out?",
        "Y": "input , other, out where out i = other X inout i",
        "Z": "If input is of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    {
        "X": "for torch.mul Each element of the tensorinputis multiplied by the corresponding element of what?",
        "Y": "Tensor other",
        "Z": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    {
        "X": "for torch.mul The shapes of input and other must be what?",
        "Y": "be broadcastable",
        "Z": "Ifinputis of typeFloatTensororDoubleTensor,othershould be a real number, otherwise it should be an integer input(Tensor) \u2013 the input tensor. other(Number) \u2013 the number to be multiplied to each element ofinput out(Tensor,optional) \u2013 the output tensor. Example: Each element of the tensorinputis multiplied by the corresponding\nelement of the Tensorother. The resulting tensor is returned. The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the first multiplicand tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    {
        "X": "in torch.logsumexp How is the computation handled?",
        "Y": "numerically stabilized",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    {
        "X": "in torch.logsumexp If keepdim is True, the output tensor has how many fewer dimension(s)?",
        "Y": "1",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. The computation is numerically\nstabilized. For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    {
        "X": "in torch.logsumexp, If the output tensor is of the same size as input, how many dimension(s) does it have?",
        "Y": "1",
        "Z": "For summation indexjjjgiven bydimand other indicesiii, the result is IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    {
        "X": "torch.max Returns the maximum value of all elements in what?",
        "Y": "the input tensor",
        "Z": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax).",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "for torch.max What is the maximum value of each row of the inputtensor in the given dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax).",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "for  torch.max If keepdimisTrue, the output tensors have what?",
        "Y": "1 fewer dimension thaninput",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "By  torch.max, What is returned If there are multiple maximal values in a reduced row ?",
        "Y": "the indices of the first maximal value",
        "Z": "Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "for torch.min If the output tensors are of the same size asinput, what is the default?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "for torch.max , output tensors are of the same size as input, in what  default condition?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "By torch.max What is returned for  in the given dimensiondim?",
        "Y": "the maximum value of each rpw of the input  tensor ",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "For torch.max, If keepdimisTrue, the output tensors are of what size?",
        "Y": "size 1",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "for torch.min, What happens if the output tensors are squeezed?",
        "Y": "1 fewer dimension thaninput",
        "Z": "Returns the minimum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemin(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "For torch.max What happens if there are multiple maximal values in a reduced row?",
        "Y": "the indices of the first maximal value are returned",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "for torch.max What default value indicates that the output tensor hasdimretained?",
        "Y": "False",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "in torch.max  What are returned if there are multiple maximal values in a reduced row?",
        "Y": "indices of the first maximal value",
        "Z": "Returns the maximum value of all elements in theinputtensor. Warning This function produces deterministic (sub)gradients unlikemax(dim=0) input(Tensor) \u2013 the input tensor. Example: Returns a namedtuple(values,indices)wherevaluesis the maximum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each maximum value found\n(argmax). IfkeepdimisTrue, the output tensors are of the same size\nasinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting\nin the output tensors having 1 fewer dimension thaninput. Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "for torch.max, What is the default value of the output tensor hasdimretained?",
        "Y": "False",
        "Z": "Note If there are multiple maximal values in a reduced row then\nthe indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. Default:False. out(tuple,optional) \u2013 the result tuple of two output tensors (max, max_indices) Example: Seetorch.maximum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "What does move totorch.hub do?",
        "Y": "Loads the Torch serialized object at the given URL",
        "Z": "Moved totorch.hub. Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    {
        "X": "in torch.utils.model_zoo.load_url where the object should already be  present ,when it's deserialized and returned?",
        "Y": "model_dir",
        "Z": "Loads the Torch serialized object at the given URL. If downloaded file is a zip file, it will be automatically\ndecompressed. If the object is already present inmodel_dir, it\u2019s deserialized and\nreturned.\nThe default value ofmodel_diris<hub_dir>/checkpointswherehub_diris the directory returned byget_dir(). url(string) \u2013 URL of the object to download model_dir(string,optional) \u2013 directory in which to save the object map_location(optional) \u2013 a function or a dict specifying how to remap storage locations (see torch.load) progress(bool,optional) \u2013 whether or not to display a progress bar to stderr.\nDefault: True check_hash(bool,optional) \u2013 If True, the filename part of the URL should follow the naming conventionfilename-<sha256>.extwhere<sha256>is the first eight or more\ndigits of the SHA256 hash of the contents of the file. The hash is used to\nensure unique names and to verify the contents of the file.\nDefault: False",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    {
        "X": "for torch.mean How many dimension(s) smaller is the output tensor?",
        "Y": "1",
        "Z": "Returns the mean value of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    {
        "X": "in torch.mean What do you do ifdimis a list of dimensions?",
        "Y": "reduce",
        "Z": "Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    {
        "X": "for torch.mean How many dimension(s) fewer dimension(s) does the output tensor have?",
        "Y": "1",
        "Z": "input(Tensor) \u2013 the input tensor. Example: Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    {
        "X": "in torch.mean What do you do ifdimis a list of dimensions over all of them?",
        "Y": "reduce",
        "Z": "Returns the mean value of each row of theinputtensor in the given\ndimensiondim. Ifdimis a list of dimensions,\nreduce over all of them. IfkeepdimisTrue, the output tensor is of the same size\nasinputexcept in the dimension(s)dimwhere it is of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in the\noutput tensor having 1 (orlen(dim)) fewer dimension(s). input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    {
        "X": "in torch.quantile How is the result computed if the quantile lies between two data points?",
        "Y": "linear interpolation",
        "Z": "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index.",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "in torch.quantile What do we map to compute the quantile?",
        "Y": "q in [0, 1] to the range of indices [0, n]",
        "Z": "To compute the quantile, we map q in [0, 1] to the range of indices [0, n] to find the location\nof the quantile in the sorted input. If the quantile lies between two data pointsa<bwith\nindicesiandjin the sorted order, result is computed using linear interpolation as follows: a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index.",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "in torch.quantile What is fraction?",
        "Y": "fractional part of the computed quantile index",
        "Z": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "in torch.quantile If the first dimension of the output represents quantiles and has size equal to the size ofq, what is the size of the remaining dimensions?",
        "Y": "Ifqis a 1D tensor",
        "Z": "a+(b-a)*fraction, wherefractionis the fractional part of the computed quantile index. Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce.",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "in torch.quantile What resulted in the inputtensor being flattened before computation?",
        "Y": "defaultdimisNone",
        "Z": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "in torch.quantile What does the first dimension of the output represent?",
        "Y": "the first dimension of the output represents the quantiles",
        "Z": "Ifqis a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size ofq, the remaining dimensions are what remains from the reduction. Note By defaultdimisNoneresulting in theinputtensor being flattened before computation. input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of values in the range [0, 1]. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "What does torch.vdot do?",
        "Y": "Computes the dot product of two 1D tensors",
        "Z": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    {
        "X": "What function handles complex numbers differently than dot(a, b)?",
        "Y": "vdot(a, b)",
        "Z": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    {
        "X": "What intentionally only supports computing the dot product of two 1D tensors with the same number of elements?",
        "Y": "torch.vdot",
        "Z": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    {
        "X": "for torch.vdot When is the conjugate of input(Tensor) used?",
        "Y": "if it\u2019s complex",
        "Z": "Computes the dot product of two 1D tensors. The vdot(a, b) function handles complex numbers\ndifferently than dot(a, b). If the first argument is complex, the complex conjugate of the\nfirst argument is used for the calculation of the dot product. Note Unlike NumPy\u2019s vdot, torch.vdot intentionally only supports computing the dot product\nof two 1D tensors with the same number of elements. input(Tensor) \u2013 first tensor in the dot product, must be 1D. Its conjugate is used if it\u2019s complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    {
        "X": "What is the same as totorch.hann_window(L+1,periodic=False)?",
        "Y": "havetorch.hann_window",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    {
        "X": "for torch.hann_window ,what is the use of periodic parameter?",
        "Y": "If True, returns a window to be used as periodic function. If False, return a symmetric window.",
        "Z": "The inputwindow_lengthis a positive integer controlling the\nreturned window size.periodicflag determines whether the returned\nwindow trims off the last duplicate value from the symmetric window and is\nready to be used as a periodic window with functions liketorch.stft(). Therefore, ifperiodicis true, theNNNin\nabove formula is in factwindow_length+1\\text{window\\_length} + 1window_length+1. Also, we always havetorch.hann_window(L,periodic=True)equal totorch.hann_window(L+1,periodic=False)[:-1]). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.hann_window.html#torch.hann_window"
    },
    {
        "X": "What does the returned solutionin torch.lstsq()store the residuals of the solution in?",
        "Y": "ncolumns",
        "Z": "Warning torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    {
        "X": "The returnedsolutionintorch.lstsq()stores the residuals of the solution in the lastm - what in the",
        "Y": "ncolumns",
        "Z": "torch.lstsq()is deprecated in favor oftorch.linalg.lstsq()and will be removed in a future PyTorch release.torch.linalg.lstsq()has reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returnedsolutionintorch.lstsq()stores the residuals of the solution in the\nlastm - ncolumns in the casem > n. Intorch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    {
        "X": "What documentation provides more details about the global deterministic flag?",
        "Y": "totorch.use_deterministic_algorithms()",
        "Z": "Returns True if the global deterministic flag is turned on. Refer totorch.use_deterministic_algorithms()documentation for more details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.are_deterministic_algorithms_enabled.html#torch.are_deterministic_algorithms_enabled"
    },
    {
        "X": "What kind of distribution is the fillsselftensor drawn from?",
        "Y": "geometric distribution",
        "Z": "Fillsselftensor with elements drawn from the geometric distribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    {
        "X": "What is filled with elements drawn from the geometric distribution?",
        "Y": "Fillsselftensor",
        "Z": "Fillsselftensor with elements drawn from the geometric distribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"
    },
    {
        "X": "What kind of distribution is the fillsselftensor?",
        "Y": "uniform",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    {
        "X": "What does torch.flipudis do to return a new tensor?",
        "Y": "Flip the entries in each column in the up/down direction",
        "Z": "Flip tensor in the up/down direction, returning a new tensor. Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. Note Requires the tensor to be at least 1-D. Note torch.flipudmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flipud,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipudis expected to be slower thannp.flipud.",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "What makes a copy of input's data?",
        "Y": " torch.flip",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "What program returns a view in constant time?",
        "Y": "NumPy",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "Where is the tensor's dtype inferred?",
        "Y": "fromfill_value",
        "Z": "Creates a tensor of sizesizefilled withfill_value. The\ntensor\u2019s dtype is inferred fromfill_value. size(int...) \u2013 a list, tuple, ortorch.Sizeof integers defining the\nshape of the output tensor. fill_value(Scalar) \u2013 the value to fill the output tensor with. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    {
        "X": "What is the name of the function that uses numbers from the continuous uniform distribution?",
        "Y": "Fillsselftensor",
        "Z": "Fillsselftensor with numbers sampled from the continuous uniform\ndistribution:",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"
    },
    {
        "X": "What is the name of the generator state?",
        "Y": "atorch.ByteTensor",
        "Z": "Gets the current device of the generator. Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does the torch.Generator do?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is the name of the object that returns the initial seed for generating random numbers?",
        "Y": "atorch.Generatorobject",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is a number that has a good balance of 0 and 1 bits?",
        "Y": "a large seed",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "a good balance of what in a seed needs to be considered?",
        "Y": "0 and 1  bits",
        "Z": "Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "As What is the  Generator state is retured by generator?",
        "Y": "atorch.ByteTensor",
        "Z": "Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does Atorch.ByteTensor contain?",
        "Y": "Tensor",
        "Z": "Example: Returns the Generator state as atorch.ByteTensor. Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does Atorch.ByteTensor contain.",
        "Y": "Tensor all the necessary bits to restore a Generator to a specific point in time",
        "Z": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does Atorch.ByteTensor return?",
        "Y": "atorch.Generatorobject",
        "Z": "Atorch.ByteTensorwhich contains all the necessary bits\nto restore a Generator to a specific point in time. Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What does the Tensor return?",
        "Y": "atorch.Generatorobject",
        "Z": "Tensor Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "What is the name of the function that returns the initial seed for generating random numbers?",
        "Y": "torch.Generator Returns the initial seed for generating random numbers",
        "Z": "Example: Returns the initial seed for generating random numbers. Example: Sets the seed for generating random numbers. Returns atorch.Generatorobject.\nIt is recommended to set a large seed, i.e. a number that has a good balance of 0\nand 1 bits. Avoid having many 0 bits in the seed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "for  torch.is_inference_mode_enabled Returns what if inference mode is currently enabled?",
        "Y": "True",
        "Z": "Returns True if inference mode is currently enabled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_inference_mode_enabled.html#torch.is_inference_mode_enabled"
    },
    {
        "X": "for torch.var_mean What contains the variance and mean?",
        "Y": "tuple",
        "Z": "IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate\nthe variance. Otherwise, the sample variance is calculated, without any\ncorrection. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What is the name of the function that determines whether the output tensor is hasdimretained or not?",
        "Y": "keepdim",
        "Z": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What is the function that calculates the variance and mean of all elements in the inputtensor?",
        "Y": "Calculates the variance and mean of all elements in theinputtensor",
        "Z": "input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints) \u2013 the dimension or dimensions to reduce. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "in torch.var_mean A tuple (var, mean) contains what?",
        "Y": "variance and mean",
        "Z": "keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor. unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1).",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What indicates that Bessel's correction will be used?",
        "Y": "IfunbiasedisTrue",
        "Z": "unbiased(bool) \u2013 whether to use Bessel\u2019s correction (\u03b4N=1\\delta N = 1\u03b4N=1). keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. A tuple (var, mean) containing the variance and mean. Calculates the variance and mean of all elements in theinputtensor. IfunbiasedisTrue, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "What type of storage does Everytorch.Tensorcast to?",
        "Y": "bfloat16",
        "Z": "Atorch.Storageis a contiguous, one-dimensional array of a single\ndata type. Everytorch.Tensorhas a corresponding storage of the same data type. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Casts this storage to Casts this storage to bool type Casts this storage to byte type Casts this storage",
        "Y": "bfloat16",
        "Z": "Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Casts this storage to Casts this storage to byte type Casts this storage to char type Casts this storage to",
        "Y": "bool",
        "Z": "Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Casts this storage to what type of type?",
        "Y": "bfloat16",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does device(int) contain?",
        "Y": "destination GPU id",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Casts this storage to Returns a copy of?",
        "Y": "char",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does device(int) \u2013 refer to ?",
        "Y": "destination GPU id",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does Casts this storage to complex double type Casts this storage to complex float type Returns if it's not already on the CPU",
        "Y": "a CPU copy",
        "Z": "Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": " storage to complex is casted  of what type?",
        "Y": "float",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does cast this storage to complex float type return if it's not already on the CPU?",
        "Y": "a CPU copy",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the storage is already on the CPU, where is the CPU copy of the storage returned?",
        "Y": "if it\u2019s not already on the CPU",
        "Z": "Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory, the copy will be what with respect to the host?",
        "Y": "asynchronous",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What has no effect if the source is in pinned memory?",
        "Y": "argument",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Where is the original object stored?",
        "Y": "CUDA memory",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory, the argument has what effect?",
        "Y": "no effect",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does kwargs contain for compatibility?",
        "Y": "keyasyncin place of thenon_blockingargument",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the object is already in what, then no copy is performed and the original object is returned?",
        "Y": "CUDA memory",
        "Z": "If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what happens?",
        "Y": "the argument has no effect",
        "Z": "Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the keyasyncin cast to?",
        "Y": "double type",
        "Z": "device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory, the copy will be asynchronous with respect to the host, what effect does the argument have?",
        "Y": "no effect",
        "Z": "non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is used to share memory between all processes?",
        "Y": "IfsharedisTrue",
        "Z": "Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if the storage is cast to double type?",
        "Y": "All changes are written to the file",
        "Z": "Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What may kwargs contain for compatibility?",
        "Y": "keyasyncin place of thenon_blockingargument",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the size of the storage?",
        "Y": "sizeis the number of elements in the storage",
        "Z": "Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What will be created if needed?",
        "Y": "IfsharedisTruethe file",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if the storage is cast to float type IfsharedisTrue?",
        "Y": "All changes are written to the file",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "IfsharedisTrue, the file must contain what?",
        "Y": "at leastsize * sizeof(Type)bytes",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the file name to map shared?",
        "Y": "filename(str)",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What means that memory is shared between all processes?",
        "Y": "IfsharedisTrue",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if memory is shared between all processes?",
        "Y": "All changes are written to the file",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What means that the changes on the storage do not affect the file?",
        "Y": "IfsharedisFalse",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the size of a file?",
        "Y": "sizeis the number of elements in the storage",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does size mean?",
        "Y": "sizeis the number of elements in the storage",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the file must contain at leastsize * sizeof(Type)bytes, what is it called?",
        "Y": "IfsharedisFalse",
        "Z": "sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does Casts this storage to if it's not already pinned?",
        "Y": "Copies the storage to pinned memory",
        "Z": "Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Which storage does not need to be moved for sharing across processes?",
        "Y": "CUDA storages",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What returns the storage to shared memory?",
        "Y": "self",
        "Z": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "for storage What does self return?",
        "Y": "self Casts this storage to short type Returns a list containing the elements of this storage",
        "Z": "IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Is shared memory able to be resized?",
        "Y": "Storages in shared memory cannot be resized",
        "Z": "Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does self do to a storage?",
        "Y": "self Casts this storage to short type",
        "Z": "size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is cast to pinned memory if it's not already pinned?",
        "Y": "Copies",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does not need to be moved for sharing across processes?",
        "Y": "CUDA storages",
        "Z": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the name of the storage that casts it to short type?",
        "Y": "self",
        "Z": "Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage is cast to pinned memory?",
        "Y": "Copies",
        "Z": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type does self cast the storage to?",
        "Y": "ifdtypeis not provided",
        "Z": "Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What does the storage do if it's not already pinned?",
        "Y": "Copies",
        "Z": "Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is performed if the storage is already of the correct type?",
        "Y": "no copy",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the storage is already of the correct type, what is performed and the original object is returned?",
        "Y": "no copy",
        "Z": "Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the desired type of a storage?",
        "Y": "dtype(typeorstring)",
        "Z": "This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What is the desired type of the storage?",
        "Y": "dtype(typeorstring)",
        "Z": "filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if the type of the storage is not provided?",
        "Y": "ifdtypeis not provided",
        "Z": "Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What returns the type of the storage?",
        "Y": "ifdtypeis not provided",
        "Z": "Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What has no effect if the source is in pinned memory and destination is on the GPU?",
        "Y": "argument",
        "Z": "Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type does return the type of the object?",
        "Y": "ifdtypeis not provided",
        "Z": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens if the object is already of the correct type?",
        "Y": "no copy is performed and the original object is returned",
        "Z": "Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file. sizeis the number of elements in the storage. IfsharedisFalse,\nthen the file must contain at leastsize * sizeof(Type)bytes\n(Typeis the type of storage). IfsharedisTruethe file will be\ncreated if needed. filename(str) \u2013 file name to map shared(bool) \u2013 whether to share memory size(int) \u2013 number of elements in the storage Casts this storage to half type Casts this storage to int type Casts this storage to long type Copies the storage to pinned memory, if it\u2019s not already pinned. Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What happens to the argument if the source is in pinned memory and destination is on the GPU?",
        "Y": "no effect",
        "Z": "Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory and destination is on the GPU, the copy is performed asynchronously with respect to the host, what happens",
        "Y": "argument has no effect",
        "Z": "If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does Theasyncarg cast to?",
        "Y": "bfloat16",
        "Z": "Moves the storage to shared memory. This is a no-op for storages already in shared memory and for CUDA\nstorages, which do not need to be moved for sharing across processes.\nStorages in shared memory cannot be resized. Returns: self Casts this storage to short type Returns a list containing the elements of this storage Returns the type ifdtypeis not provided, else casts this object to\nthe specified type. If this is already of the correct type, no copy is performed and the\noriginal object is returned. dtype(typeorstring) \u2013 The desired type non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device. non_blocking(bool) \u2013 IfTrueand the source is in pinned memory,\nthe copy will be asynchronous with respect to the host. Otherwise,\nthe argument has no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Casts this storage to double type Casts this storage to float type IfsharedisTrue, then memory is shared between all processes.\nAll changes are written to the file. IfsharedisFalse, then the changes on\nthe storage do not affect the file.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the argument is true, the copy is performed asynchronously with respect to the host, and the source is in pinned memory and destination is",
        "Y": "no effect",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "What type of storage does the bool type cast to?",
        "Y": "byte type",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "If the source is in pinned memory and destination is on the GPU, how is the copy performed with respect to the host?",
        "Y": "asynchronously",
        "Z": "non_blocking(bool) \u2013 IfTrue, and the source is in pinned memory\nand destination is on the GPU or vice versa, the copy is performed\nasynchronously with respect to the host. Otherwise, the argument\nhas no effect. **kwargs\u2013 For compatibility, may contain the keyasyncin place of\nthenon_blockingargument. Theasyncarg is deprecated. Casts this storage to bfloat16 type Casts this storage to bool type Casts this storage to byte type Casts this storage to char type Returns a copy of this storage Casts this storage to complex double type Casts this storage to complex float type Returns a CPU copy of this storage if it\u2019s not already on the CPU Returns a copy of this object in CUDA memory. If this object is already in CUDA memory and on the correct device, then\nno copy is performed and the original object is returned. device(int) \u2013 The destination GPU id. Defaults to the current device.",
        "source": "https://pytorch.org/docs/stable/storage.html"
    },
    {
        "X": "Solves a system of equations with what?",
        "Y": "triangular coefficient",
        "Z": "Solves a system of equations with a triangular coefficient matrixAAAand multiple right-hand sidesbbb. In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What does solvesAAA assume is?",
        "Y": "upper-triangular",
        "Z": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the value of * of more batch dimensions?",
        "Y": "zero",
        "Z": "In particular, solvesAX=bAX = bAX=band assumesAAAis upper-triangular\nwith the default keyword arguments. torch.triangular_solve(b, A)can take in 2D inputsb, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputsX Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "Supports input of what data types?",
        "Y": "float, double, cfloat and cdouble",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What supports multiple right-hand sides of size(,m,k)(*, m, k)(,m,k",
        "Y": "b(Tensor)",
        "Z": "Supports input of float, double, cfloat and cdouble data types. b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value of the upper-triangular system of equations?",
        "Y": "True",
        "Z": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the name of the multiple right-hand sides of size(,m,k)?",
        "Y": "b(Tensor)",
        "Z": "b(Tensor) \u2013 multiple right-hand sides of size(\u2217,m,k)(*, m, k)(\u2217,m,k)where\u2217*\u2217is zero of more batch dimensions A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value of the transpose(bool,optional)?",
        "Y": "False",
        "Z": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value for whether the AAA should be transposed before being sent into the solver?",
        "Y": "transpose",
        "Z": "A(Tensor) \u2013 the input triangular coefficient matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m)where\u2217*\u2217is zero or more batch dimensions upper(bool,optional) \u2013 whether to solve the upper-triangular system\nof equations (default) or the lower-triangular system of equations. Default:True. transpose(bool,optional) \u2013 whetherAAAshould be transposed before\nbeing sent into the solver. Default:False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "What is the default value of seetorch.set_default_tensor_type()?",
        "Y": "ifNone",
        "Z": "Returns a tensor filled with the scalar value1, with the shape defined\nby the variable argumentsize. size(int...) \u2013 a sequence of integers defining the shape of the output tensor.\nCan be a variable number of arguments or a collection like a list or tuple. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    {
        "X": "What is a boolean tensor that is True whereinputis greater thanother?",
        "Y": "Computesinput",
        "Z": "Computesinput>other\\text{input} > \\text{other}input>otherelement-wise. The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis greater thanotherand False elsewhere Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    {
        "X": "What represents the principal directions of AAA?",
        "Y": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "Z": "Performs linear Principal Component Analysis (PCA) on a low-rank\nmatrix, batches of such matrices, or sparse matrix. This function returns a namedtuple(U,S,V)which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrixAAAsuch thatA=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT. Note The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions?",
        "Y": "AAAis",
        "Z": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the relation of(U,S,V)to PCA?",
        "Y": "AAAis a data matrix withmsamples andnfeatures theVVVcolumns",
        "Z": "The relation of(U,S,V)to PCA is as follows: AAAis a data matrix withmsamples andnfeatures theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "The size of returned matrices depend on the specified rank and what other value?",
        "Y": "q",
        "Z": "theVVVcolumns represent the principal directions S\u2217\u22172/(m\u22121)S ** 2 / (m - 1)S\u2217\u22172/(m\u22121)contains the eigenvalues ofATA/(m\u22121)A^T A / (m - 1)ATA/(m\u22121)which is the covariance ofAwhencenter=Trueis provided. matmul(A,V[:,:k])projects data to the first k\nprincipal components Note Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the default q value?",
        "Y": "By default,q=min(6,m,n)",
        "Z": "Different from the standard SVD, the size of returned\nmatrices depend on the specified rank and q\nvalues as follows: UUUis m x q matrix SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the default setting to center the input tensor?",
        "Y": "if True",
        "Z": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References:",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the default value of the q(int,optional)?",
        "Y": "By default,q=min(6,m,n)",
        "Z": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References:",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is a reference to the number of subspace iterations to conduct?",
        "Y": "References",
        "Z": "SSSis q-vector VVVis n x q matrix Note To obtain repeatable results, reset the seed for the\npseudorandom number generator A(Tensor) \u2013 the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q(int,optional) \u2013 a slightly overestimated rank ofAAA. By default,q=min(6,m,n). center(bool,optional) \u2013 if True, center the input tensor,\notherwise, assume that the input is\ncentered. niter(int,optional) \u2013 the number of subspace iterations to\nconduct; niter must be a nonnegative\ninteger, and defaults to 2. References:",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "What is the starting dimension length?",
        "Y": "the distance to the ending dimension",
        "Z": "Returns a new tensor that is a narrowed version ofinputtensor. The\ndimensiondimis input fromstarttostart+length. The\nreturned tensor andinputtensor share the same underlying storage. input(Tensor) \u2013 the tensor to narrow dim(int) \u2013 the dimension along which to narrow start(int) \u2013 the starting dimension length(int) \u2013 the distance to the ending dimension Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    {
        "X": "Andindicesis what of each minimum value found (argmin)?",
        "Y": "index location",
        "Z": "Example: Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What is the value of the minimum value of each row of the inputtensor in the given dimensiondim?",
        "Y": "Note",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What returns the minimum value of each row of theinputtensor in the given dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis the minimum\nvalue of each row of theinputtensor in the given dimensiondim. Andindicesis the index location of each minimum value found\n(argmin). IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "When are the output tensors of the same size as input?",
        "Y": "IfkeepdimisTrue",
        "Z": "IfkeepdimisTrue, the output tensors are of the same size asinputexcept in the dimensiondimwhere they are of size 1.\nOtherwise,dimis squeezed (seetorch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension thaninput. Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not.",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What is the tuple of two output tensors?",
        "Y": "out",
        "Z": "Note If there are multiple minimal values in a reduced row then\nthe indices of the first minimal value are returned. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the tuple of two output tensors (min, min_indices) Example: Seetorch.minimum().",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "What kind of function is the call to geqrf?",
        "Y": "low-level",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What is the return value of the low-level function for calling LAPACK's geqrf?",
        "Y": "namedtuple",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What does the function compute?",
        "Y": "QR decomposition ofinput",
        "Z": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "Where are the elements ofRare stored?",
        "Y": "on and above the diagonal",
        "Z": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "Where is the matrix stored?",
        "Y": "below the diagonal",
        "Z": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What is the result of withtorch.ormqr()?",
        "Y": "matrix-matrix multiplication",
        "Z": "Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What is the name of the LAPACK function that computes Q and R matrices?",
        "Y": "geqrf",
        "Z": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What can solve matrix equations using?",
        "Y": "a QR decomposition",
        "Z": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What is the input matrix out(tuple,optional)?",
        "Y": "input(Tensor)",
        "Z": "This is a low-level function for calling LAPACK\u2019s geqrf directly. This function\nreturns a namedtuple (a, tau) as defined inLAPACK documentation for geqrf. Computes a QR decomposition ofinput.\nBothQandRmatrices are stored in the same output tensora.\nThe elements ofRare stored on and above the diagonal.\nElementary reflectors (or Householder vectors) implicitly defining matrixQare stored below the diagonal.\nThe results of this function can be used together withtorch.linalg.householder_product()to obtain theQmatrix or\nwithtorch.ormqr(), which uses an implicit representation of theQmatrix,\nfor an efficient matrix-matrix multiplication. SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What is the default value for the input matrix out(tuple,optional)?",
        "Y": "Default:None",
        "Z": "SeeLAPACK documentation for geqrffor further details. Note See alsotorch.linalg.qr(), which computes Q and R matrices, andtorch.linalg.lstsq()with thedriver=\"gels\"option for a function that can solve matrix equations using a QR decomposition. input(Tensor) \u2013 the input matrix out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor). Ignored ifNone. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"
    },
    {
        "X": "What returns a number of samples from the multinomial probability distribution located in the corresponding row of tensorinput?",
        "Y": "tensor",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What must the rows ofinputdo be?",
        "Y": "non-negative, finite and have a non-zero sum",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is ordered from left to right according to when each was sampled?",
        "Y": "Indices",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinput is a vector, what is it?",
        "Y": "Ifinputis a vector",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled\nfrom the multinomial probability distribution located in the corresponding row\nof tensorinput. Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What must the rows of inputdo be?",
        "Y": "non-negative, finite and have a non-zero sum",
        "Z": "The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinputis a matrix, outis a matrix of shape(m times textnum_samples",
        "Y": "matrix withmrows",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement.",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinputis a matrix,outis a matrix of shape(mnum_samples)?",
        "Y": "matrix withmrows",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "If what isTrue, samples are drawn with replacement. If not, samples are drawn without replacement.",
        "Y": "replacement",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "If replacement isTrue, samples are drawn with replacement. If not, what happens to them?",
        "Y": "they are drawn without replacement",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is the name of the variable that is used when a sample index is drawn for a row?",
        "Y": "Note",
        "Z": "Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "Ifinputis a matrix withmrows, outis a matrix of shape(mnum_samples)?",
        "Y": "matrix withmrows",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is the name of a vector that is a matrix of sizenum_samples?",
        "Y": "Note",
        "Z": "Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "If a sample index is drawn for a row, it cannot be drawn again for that row?",
        "Y": "If replacement isTrue",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What must num_samples be lower than when drawn without replacement?",
        "Y": "number of non-zero elements ininput",
        "Z": "Note The rows ofinputdo not need to sum to one (in which case we use\nthe values as weights), but must be non-negative, finite and have\na non-zero sum. Indices are ordered from left to right according to when each was sampled\n(first samples are placed in first column). Ifinputis a vector,outis a vector of sizenum_samples. Ifinputis a matrix withmrows,outis an matrix of shape(m\u00d7num_samples)(m \\times \\text{num\\_samples})(m\u00d7num_samples). If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What does input(Tensor) contain probabilities?",
        "Y": "input tensor",
        "Z": "If replacement isTrue, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What is drawn for a row, it cannot be drawn again for that row?",
        "Y": "a sample index",
        "Z": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. Note When drawn without replacement,num_samplesmust be lower than\nnumber of non-zero elements ininput(or the min number of non-zero\nelements in each row ofinputif it is a matrix). input(Tensor) \u2013 the input tensor containing probabilities num_samples(int) \u2013 number of samples to draw replacement(bool,optional) \u2013 whether to draw with replacement or not",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "What do modules make it easy to specify for PyTorch's Optimizers to update?",
        "Y": "learnable parameters",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How do modules work with PyTorch's autogradsystem?",
        "Y": "Easy to work with and transform",
        "Z": "Tightly integrated with PyTorch\u2019sautogradsystem.Modules make it simple to specify learnable parameters for PyTorch\u2019s Optimizers to update. Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is one of the advantages of using modules?",
        "Y": "Easy to work with and transform",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Why are many topics in this note elaborated on in other notes or tutorials?",
        "Y": "modules are so fundamental to PyTorch",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is a Simple Custom Module Modules a part of?",
        "Y": "Building Blocks Neural Network Training",
        "Z": "Easy to work with and transform.Modules are straightforward to save and restore, transfer between\nCPU / GPU / TPU devices, prune, quantize, and more. This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is a simple custom module?",
        "Y": "Module State Module Hooks Advanced Features",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is a module that can be used for Neural Network Training?",
        "Y": "Module State Module Hooks Advanced Features",
        "Z": "Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does this module apply to its input?",
        "Y": "an affine transformation",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the basic feature of the Linearmodule module?",
        "Y": "module has the following fundamental characteristics of modules",
        "Z": "Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an example of a module that can be used to train a neural network?",
        "Y": "Neural Network Training with Modules Module State Module Hooks Advanced Features",
        "Z": "Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does the Linearmodule module apply to its input?",
        "Y": "an affine transformation",
        "Z": "This note describes modules, and is intended for all PyTorch users. Since modules are so fundamental to PyTorch,\nmany topics in this note are elaborated on in other notes or tutorials, and links to many of those documents\nare provided here as well. A Simple Custom Module Modules as Building Blocks Neural Network Training with Modules Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the basic feature of the Linearmodule?",
        "Y": "module has the following fundamental characteristics of modules",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the module that lets us look at a custom version of PyTorch'sLinearmodule?",
        "Y": "Module State Module Hooks Advanced Features",
        "Z": "Module State Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the feature that lets us look at a custom version of PyTorch'sLinearmodule?",
        "Y": "Module Hooks Advanced Features",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does the simple module inherit from the base Module class?",
        "Y": "module has the following fundamental characteristics of modules",
        "Z": "Module Hooks Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is a custom version of PyTorch'sLinearmodule called?",
        "Y": "Advanced Features",
        "Z": "Advanced Features To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the module that applies an affine transformation to its input?",
        "Y": "a simpler, custom version of PyTorch\u2019sLinearmodule",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What defines some that is used in computation?",
        "Y": "state",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is each of the random-initializedweightandbiastensors defined as?",
        "Y": "aParameter",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019sLinearmodule.\nThis module applies an affine transformation to its input. This simple module has the following fundamental characteristics of modules: It inherits from the base Module class.All modules should subclassModulefor composability with other modules. It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be considered the \"learnable\" aspects of the module's computation?",
        "Y": "Parameters",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Modules are not required to have what?",
        "Y": "state",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What performs arbitrary computation involving any number of inputs and outputs?",
        "Y": "theforward()implementation",
        "Z": "It defines some \u201cstate\u201d that is used in computation.Here, the state consists of randomly-initializedweightandbiastensors that define the affine\ntransformation. Because each of these is defined as aParameter, they areregisteredfor the module and will automatically be tracked and returned from calls\ntoparameters(). Parameters can be\nconsidered the \u201clearnable\u201d aspects of the module\u2019s computation (more on this later). Note that modules\nare not required to have state, and can also be stateless. It defines a forward() function that performs the computation.For this affine transformation module, the input\nis matrix-multiplied with theweightparameter (using the@short-hand notation) and added to thebiasparameter to produce the output. More generally, theforward()implementation for a module can perform arbitrary\ncomputation involving any number of inputs and outputs. This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: Note that the module itself is callable, and that calling it invokes itsforward()function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement abackward()function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail inNeural Network Training with Modules. The full set of parameters registered by the module can be iterated through via a call toparameters()ornamed_parameters(),\nwhere the latter includes each parameter\u2019s name:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What can be constructed and called in this simple module?",
        "Y": "Instances",
        "Z": "This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are aspects of a module's computation that should be \"learned\"?",
        "Y": "the parameters registered by a module",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What program can be used to update the parameters of a module?",
        "Y": "PyTorch",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Modules can contain other what?",
        "Y": "modules",
        "Z": "In general, the parameters registered by a module are aspects of the module\u2019s computation that should be\n\u201clearned\u201d. A later section of this note shows how to update these parameters using one of PyTorch\u2019s Optimizers.\nBefore we get to that, however, let\u2019s first examine how modules can be composed with one another. Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the simplest way to do this?",
        "Y": "using theSequentialmodule",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does Sequential feed the output of the first MyLinearmodule into?",
        "Y": "theReLU",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using theSequentialmodule. It allows us to chain together\nmultiple modules: Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does Sequential automatically feed the output of the first MyLinearmodule as?",
        "Y": "input into theReLU",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Sequentialautomatically feeds the output of the firstMyLinearmodule as input into the ReLU, and the output of that as input",
        "Y": "in-order chaining of modules",
        "Z": "Note thatSequentialautomatically feeds the output of the firstMyLinearmodule as input\ninto theReLU, and the output of that as input into the secondMyLinearmodule. As\nshown, it is limited to in-order chaining of modules. In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is recommended to define for anything beyond the simplest use cases?",
        "Y": "a custom module",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation. For example, here\u2019s a simple neural network implemented as a custom module:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does s() andnamed_modules()recursivelyiterate through a module and its child modules?",
        "Y": "module",
        "Z": "To go deeper than just the immediate children,modules()andnamed_modules()recursivelyiterate through a module and its child modules: Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What register submodules from a list or dict?",
        "Y": "TheModuleListandModuleDictmodules",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is it sometimes necessary for a module to do?",
        "Y": "dynamically define submodules",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nTheModuleListandModuleDictmodules are useful here; they\nregister submodules from a list or dict: For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do calls toparameters() andnamed_parameters() recursively include?",
        "Y": "child parameters",
        "Z": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls toparameters()andnamed_parameters()will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: It\u2019s also easy to move all parameters to a different device or change their precision usingto(): These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Where can PyTorch provide a large library of performant modules?",
        "Y": "thetorch.nnnamespace",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is a full example of?",
        "Y": "training a neural network",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do you want to know about PyTorch?",
        "Y": "more information",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an example of in the next section?",
        "Y": "training a neural network",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an example of how to train a neural network?",
        "Y": "Recursivelyapply()a function",
        "Z": "These examples show how elaborate neural networks can be formed through module composition. To allow for\nquick and easy construction of neural networks with minimal boilerplate, PyTorch provides a large library of\nperformant modules within thetorch.nnnamespace that perform computation commonly found within neural\nnetworks, including pooling, convolutions, loss functions, etc. In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an example of a function that can be added to a module and its submodules?",
        "Y": "Recursivelyapply()a function",
        "Z": "For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is a function that can be added to a module and its submodules?",
        "Y": "Recursivelyapply()a function",
        "Z": "Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is one of PyTorch's Optimizers?",
        "Y": "Defining neural net modules",
        "Z": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are present in this simplified example?",
        "Y": "key parts of training",
        "Z": "Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present:",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What happens when a network is built?",
        "Y": "it has to be trained",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does the network learn to output in this simplified example?",
        "Y": "zero",
        "Z": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss,",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the optimizer in this example?",
        "Y": "stochastic gradient descent optimizer",
        "Z": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss,",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does the optimizer do to the network's gradients?",
        "Y": "zeros",
        "Z": "A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an optimizer in this case?",
        "Y": "stochastic gradient descent optimizer",
        "Z": "An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What does loss.backward() do to update the parameters' gradients?",
        "Y": "zeros",
        "Z": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What happens after the above snippet is run?",
        "Y": "the network\u2019s parameters have changed",
        "Z": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "The value ofl1'sweightparameter is closer to what value?",
        "Y": "0",
        "Z": "acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is an example of?",
        "Y": "training a neural network",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients,",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What happens after the above snippet has been run?",
        "Y": "the network\u2019s parameters have changed",
        "Z": "In the next section, we give a full example of training a neural network. For more information, check out: Recursivelyapply()a function to a module and its submodules Library of PyTorch-provided modules:torch.nn Defining neural net modules:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_module.html Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving itsstate_dict(i.e. \u201cstate dictionary\u201d):",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What shows that the network's values are closer to 0?",
        "Y": "examining the value ofl1\u2019sweightparameter",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the article that explains how to train neural networks?",
        "Y": "Using Optimizers",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers fromtorch.optim: In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employingtorch.abs()as a loss function. While this is not a very interesting task, the\nkey parts of training are present: A network is created. An optimizer (in this case, a stochastic gradient descent optimizer) is created, and the network\u2019s\nparameters are associated with it. acquires an input, runs the network, computes a loss, zeros the network\u2019s parameters\u2019 gradients, calls loss.backward() to update the parameters\u2019 gradients, calls optimizer.step() to apply the gradients to the parameters. After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html Introduction to autograd:https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the term for training neural networks?",
        "Y": "Neural network training",
        "Z": "After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue ofl1\u2019sweightparameter shows that its values are now much closer to 0 (as may be expected): Training neural networks can often be tricky. For more information, check out: Using Optimizers:https://pytorch.org/tutorials/beginner/examples_nn/two_layer_net_optim.html. Neural network training:https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of a module's dict that affects its computation?",
        "Y": "state",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is included in a module'sstate_dict?",
        "Y": "module\u2019s parameters",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "For some modules, it may be useful to have state that affects module computation but is not learnable?",
        "Y": "beyond parameters",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What provides the concept of buffers?",
        "Y": "PyTorch",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Parameters are what type of state a module can have?",
        "Y": "learnable aspects of computation",
        "Z": "A module\u2019sstate_dictcontains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have: Parameters: learnable aspects of computation; contained within thestate_dict",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What do all hooks allow the user to return that will be used throughout the remaining computation?",
        "Y": "updated value",
        "Z": "Backward hooksare called during the backward pass. They can be installed withregister_full_backward_hook(). These hooks will be called when the backward for this\nModule has been computed and will allow the user to access the gradients for both the inputs and outputs.\nAlternatively, they can be installed globally for all modules withregister_module_full_backward_hook(). All hooks allow the user to return an updated value that will be used throughout the remaining computation.\nThus, these hooks can be used to either execute arbitrary code along the regular module forward/backward or\nmodify some inputs/outputs without having to change the module\u2019sforward()function.",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What provides advanced features that are designed to work with modules?",
        "Y": "PyTorch",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What are the functionalities of PyTorch's advanced features called when writing a new module?",
        "Y": "inherited",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Where can an in-depth discussion of PyTorch's advanced features be found?",
        "Y": "links below",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the name of the feature that is inherited when writing a new module?",
        "Y": "Profiling",
        "Z": "PyTorch also provides several more advanced features that are designed to work with modules. All these functionalities\nare \u201cinherited\u201d when writing a new module. In-depth discussion of these features can be found in the links below. For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "Exporting modules to what is a good way to use C++?",
        "Y": "TorchScript",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What language is a good example of a language that can be exported to TorchScript?",
        "Y": "C++",
        "Z": "For more information, check out: Profiling:https://pytorch.org/tutorials/beginner/profiler.html Pruning:https://pytorch.org/tutorials/intermediate/pruning_tutorial.html Quantization:https://pytorch.org/tutorials/recipes/quantization.html Exporting modules to TorchScript (e.g. for usage from C++):https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "What is the result oftorch.FloatTensor.abs()?",
        "Y": "a new tensor",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What are some of the attributes of atorch.Tensor?",
        "Y": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "Z": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What kind of memory usage could be caused by the current implementation oftorch.Tensor?",
        "Y": "unexpectedly high",
        "Z": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What does the new Tensor return withdataas?",
        "Y": "tensor data",
        "Z": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "Returns a new Tensor withdataas what?",
        "Y": "tensor data",
        "Z": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "What is the order of a n-D tensor along a given axis in?",
        "Y": "dims",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "What is torch.flip expected to be?",
        "Y": "slower thannp.flip",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "What does dims(a listortuple) flip on?",
        "Y": "axis",
        "Z": "Reverse the order of a n-D tensor along given axis in dims. Note torch.flipmakes a copy ofinput\u2019s data. This is different from NumPy\u2019snp.flip,\nwhich returns a view in constant time. Since copying a tensor\u2019s data is more work than viewing that data,torch.flipis expected to be slower thannp.flip. input(Tensor) \u2013 the input tensor. dims(a listortuple) \u2013 axis to flip on Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "What returns the reduced singular value decomposition?",
        "Y": "IfsomeisTrue",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "If the last two dimensions ofinputaremandn, the returnedUandVmatrices will contain what?",
        "Y": "onlymin(n, m)orthonormal columns",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What types of data types does torch support?",
        "Y": "float, double, cfloat and cdouble data types",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd():",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The dtypes ofUandVare the same asinput's.Swill always be what?",
        "Y": "real-valued",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd():",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What should be replaced with torch.linalg.svd()?",
        "Y": "Note Differences",
        "Z": "Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd():",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What does torch.svd() return ifcompute_uvisFalse?",
        "Y": "zero-filled tensors forUandVh",
        "Z": "IfsomeisTrue(default), the method returns the reduced singular\nvalue decomposition. In this case, if the last two dimensions ofinputaremandn, then the returnedUandVmatrices will contain onlymin(n, m)orthonormal columns. Ifcompute_uvisFalse, the returnedUandVwill be\nzero-filled matrices of shape(m, m)and(n, n)respectively, and the same device asinput. The argumentsomehas no effect whencompute_uvisFalse. Supportsinputof float, double, cfloat and cdouble data types.\nThe dtypes ofUandVare the same asinput\u2019s.Swill\nalways be real-valued, even ifinputis complex. Warning torch.svd()is deprecated in favor oftorch.linalg.svd()and will be removed in a future PyTorch release. U,S,V=torch.svd(A,some=some,compute_uv=True)(default) should be replaced with _,S,_=torch.svd(A,some=some,compute_uv=False)should be replaced with Note Differences withtorch.linalg.svd(): someis the opposite oftorch.linalg.svd()\u2019sfull_matrices. Note that\ndefault value for both isTrue, so the default behavior is\neffectively the opposite. torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the name of the batch of matrices that returns the singular values of each matrix in the batch in descending order?",
        "Y": "Ifinputis",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The Tensor can only be used to do what?",
        "Y": "compute gradients ifcompute_uvisTrue",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What does theStensor can only be used to compute gradients ifcompute_uvisTrue?",
        "Y": "Note",
        "Z": "torch.svd()returnsV, whereastorch.linalg.svd()returnsVh, that is,V\u1d34. Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "Ifcompute_uvisFalse, what does torch.svd() return?",
        "Y": "zero-filled tensors",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What can TheStensor be used to compute gradients ifcompute_uvisTrue?",
        "Y": "Note",
        "Z": "Ifcompute_uvisFalse,torch.svd()returns zero-filled\ntensors forUandVh, whereastorch.linalg.svd()returns\nempty tensors. Note The singular values are returned in descending order. Ifinputis a batch of matrices,\nthen the singular values of each matrix in the batch are returned in descending order. Note TheStensor can only be used to compute gradients ifcompute_uvisTrue. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When someisFalse, the gradients onU[..., :, min(m, n):]andV[...",
        "Y": "the backward pass",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the name of the algorithm used in the implementation oftorch.linalg.svd() on CPU?",
        "Y": "LAPACK",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What program uses routinegesddon earlier versions of CUDA?",
        "Y": "MAGMA",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What does the implementation oftorch.linalg.svd()on GPU use?",
        "Y": "Note",
        "Z": "WhensomeisFalse, the gradients onU[\u2026, :, min(m, n):]andV[\u2026, :, min(m, n):]will be ignored in the backward pass, as those vectors\ncan be arbitrary bases of the corresponding subspaces. Note The implementation oftorch.linalg.svd()on CPU uses LAPACK\u2019s routine?gesdd(a divide-and-conquer algorithm) instead of?gesvdfor speed. Analogously,\non GPU, it uses cuSOLVER\u2019s routinesgesvdjandgesvdjBatchedon CUDA 10.1.243\nand later, and MAGMA\u2019s routinegesddon earlier versions of CUDA. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "The gradients with respect toUandVwill only be what when the input does not have zero or repeated singular values?",
        "Y": "finite",
        "Z": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When the matrix has what, the gradients also depend onS1?",
        "Y": "small singular values",
        "Z": "The returnedUwill not be contiguous. The matrix (or batch of matrices) will\nbe represented as a column-major matrix (i.e. Fortran-contiguous). Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values is close to",
        "Y": "Warning",
        "Z": "Warning The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the warning that the gradients with respect toUandVwill be numerically unstable when the distance between any two singular values is close to zero",
        "Y": "Warning",
        "Z": "The gradients with respect toUandVwill only be finite when the input does not\nhave zero nor repeated singular values. Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What will be numerically unstable if the distance between any two singular values is close to zero?",
        "Y": "the gradients with respect toUandV",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors. input(Tensor) \u2013 the input tensor of size(*, m, n)where*is zero or more\nbatch dimensions consisting of(m, n)matrices.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is a warning when the distance between any two singular values is close to zero?",
        "Y": "Warning",
        "Z": "Warning If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the name of the warning that the gradients with respect toUandVwill be numerically unstable if the distance between any two singular values",
        "Y": "Warning",
        "Z": "If the distance between any two singular values is close to zero, the gradients with respect toUandVwill be numerically unstable, as they depends on1min\u2061i\u2260j\u03c3i2\u2212\u03c3j2\\frac{1}{\\min_{i \\neq j} \\sigma_i^2 - \\sigma_j^2}mini\ue020=j\u200b\u03c3i2\u200b\u2212\u03c3j2\u200b1\u200b. The same happens when the matrix\nhas small singular values, as these gradients also depend onS\u207b\u00b9. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "For what type of input is the singular value decomposition not unique?",
        "Y": "complex-valuedinput",
        "Z": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "When does the same happen for complex-valuedinput?",
        "Y": "wheninputhas repeated singular values",
        "Z": "For complex-valuedinputthe singular value decomposition is not unique,\nasUandVmay be multiplied by an arbitrary phase factorei\u03d5e^{i \\phi}ei\u03d5on every column.\nThe same happens wheninputhas repeated singular values, where one may multiply\nthe columns of the spanning subspace inUandVby a rotation matrix\nandthe resulting vectors will span the same subspace.\nDifferent platforms, like NumPy, or inputs on different device types,\nmay produce differentUandVtensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "What is the function that converts a DLPack to a tensor?",
        "Y": "Decodes a DLPack to a tensor",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "What is the dlpack?",
        "Y": "PyCapsule object",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "How many times can a dlpack be consumed?",
        "Y": "once",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "What is the name of the tensor to be exported?",
        "Y": "tensor",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "How many times can each dlpack be consumed?",
        "Y": "once",
        "Z": "Decodes a DLPack to a tensor. dlpack\u2013 a PyCapsule object with the dltensor The tensor will share the memory with the object represented\nin the dlpack.\nNote that each dlpack can only be consumed once. Returns a DLPack representing the tensor. tensor\u2013 a tensor to be exported The dlpack shares the tensors memory.\nNote that each dlpack can only be consumed once.",
        "source": "https://pytorch.org/docs/stable/dlpack.html"
    },
    {
        "X": "What type of tensor is input(Tensor)?",
        "Y": "float 1D tensor",
        "Z": "Converts a float tensor to a per-channel quantized tensor with given scales and zero points. input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What does input(Tensor) use to quantize scales?",
        "Y": "float tensor",
        "Z": "input(Tensor) \u2013 float tensor to quantize scales(Tensor) \u2013 float 1D tensor of scales to use, size should matchinput.size(axis) zero_points(int) \u2013 integer 1D tensor of offset to use, size should matchinput.size(axis) axis(int) \u2013 dimension on which apply per-channel quantization dtype(torch.dtype) \u2013 the desired data type of returned tensor.\nHas to be one of the quantized dtypes:torch.quint8,torch.qint8,torch.qint32 A newly quantized tensor Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "What is the function that computes the eigenvalues and eigenvectors of a real square matrix?",
        "Y": "Computes the eigenvalues and eigenvectors of a real square matrix",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Wheninputis on CUDA,torch.eig()causes what?",
        "Y": "host-device synchronization",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1].",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Backward pass is supported only if eigenvalues and eigenvectors are all real valued?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What should be replaced with something else?",
        "Y": "L,_=torch.eig(A)",
        "Z": "Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What does torch.eig() cause wheninputis on CUDA?",
        "Y": "host-device synchronization",
        "Z": "Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What should be replaced with L,_=torch.eig(A)?",
        "Y": "L,V=torch.eig(A,eigenvectors=True)",
        "Z": "Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What should be replaced with L,V=torch.eig(A,eigenvectors=True)?",
        "Y": "L,_=torch.eig(A)",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1].",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is the square matrix of shape?",
        "Y": "input(Tensor)",
        "Z": "input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is a namedtuple containing eigenvalues, eigenvectors?",
        "Y": "output tensors",
        "Z": "eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is the first element of an eigenvalue of input?",
        "Y": "the first element is the real part and the second element is the imaginary part",
        "Z": "eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Are eigenvalues ordered or ordered?",
        "Y": "not necessarily ordered",
        "Z": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Out(tuple,optional) - the output what?",
        "Y": "tensors",
        "Z": "out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is the real part of an eigenvalue of input?",
        "Y": "the first element is the real part and the second element is the imaginary part",
        "Z": "out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "Are eigenvalues ordered?",
        "Y": "not necessarily",
        "Z": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is a namedtuple containing eigenvalues(Tensor)?",
        "Y": "Shape",
        "Z": "A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is the second element of an eigenvalue of input?",
        "Y": "imaginary part",
        "Z": "eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is the eigenvalue of input?",
        "Y": "the first element is the real part and the second element is the imaginary part",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix. Note Since eigenvalues and eigenvectors might be complex, backward pass is supported only\nif eigenvalues and eigenvectors are all real valued. Wheninputis on CUDA,torch.eig()causes\nhost-device synchronization. Warning torch.eig()is deprecated in favor oftorch.linalg.eig()and will be removed in a future PyTorch release.torch.linalg.eig()returns complex tensors of dtypecfloatorcdoublerather than real tensors mimicking complex tensors. L,_=torch.eig(A)should be replaced with L,V=torch.eig(A,eigenvectors=True)should be replaced with input(Tensor) \u2013 the square matrix of shape(n\u00d7n)(n \\times n)(n\u00d7n)for which the eigenvalues and eigenvectors\nwill be computed eigenvectors(bool) \u2013Trueto compute both eigenvalues and eigenvectors;\notherwise, only eigenvalues will be computed out(tuple,optional) \u2013 the output tensors  A namedtuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(n\u00d72)(n \\times 2)(n\u00d72). Each row is an eigenvalue ofinput,\nwhere the first element is the real part and the second element is the imaginary part.\nThe eigenvalues are not necessarily ordered. eigenvectors(Tensor): Ifeigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor of shape(n\u00d7n)(n \\times n)(n\u00d7n)can be used to compute normalized (unit length)\neigenvectors of corresponding eigenvalues as follows.\nIf the correspondingeigenvalues[j]is a real number, columneigenvectors[:, j]is the eigenvector\ncorresponding toeigenvalues[j].\nIf the correspondingeigenvalues[j]andeigenvalues[j + 1]form a complex conjugate pair, then the\ntrue eigenvectors can be computed astrue\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j] = eigenvectors[:, j] + i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j]=eigenvectors[:,j]+i\u00d7eigenvectors[:,j+1],true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1]\\text{true eigenvector}[j + 1] = eigenvectors[:, j] - i \\times eigenvectors[:, j + 1]true\u00a0eigenvector[j+1]=eigenvectors[:,j]\u2212i\u00d7eigenvectors[:,j+1].",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What is another name for Tensor?",
        "Y": "Tensor",
        "Z": "(Tensor,Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "What happens to the dimension(s) of input?",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "What happens to other dimensions of input that are not explicitly moved?",
        "Y": "remain in their original order",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "What are the original positions of the dims to move?",
        "Y": "source",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.is_complex.html#torch.is_complex"
    },
    {
        "X": "What must be the original positions of the dims to move?",
        "Y": "unique",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "What is the name of the destination position for each of the original dims?",
        "Y": "destination",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "Destination positions for each of the original dims must also be what?",
        "Y": "unique",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "What must be unique for each of the original dims to move?",
        "Y": "Examples",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination. Other dimensions ofinputthat are not explicitly moved remain in",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "What are scaling factors on matrix-vector product betweenmat1 andmat2?",
        "Y": "alphaandbetaare scaling factors",
        "Z": "alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "source": "https//pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "What are the arguments for inputs of typeFloatTensororDoubleTensor?",
        "Y": "argumentsbetaandalphamust be real numbers",
        "Z": "alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "source": "https//pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm"
    },
    {
        "X": "What is the second matrix to be matrix multiplied?",
        "Y": "mat2(Tensor)",
        "Z": "alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What do alphaandbetaare scaling factors on?",
        "Y": "matrix-vector product",
        "Z": "alphaandbetaare scaling factors on matrix-vector product betweenmat1andmat2and the added matrixinputrespectively. Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "source": "https//pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm"
    },
    {
        "X": "What do argumentsbeta andalphamust be?",
        "Y": "argumentsbetaandalphamust be real numbers",
        "Z": "Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "What is the first matrix to be matrix multiplied?",
        "Y": "mat1(Tensor)",
        "Z": "Ifbetais 0, theninputwill be ignored, andnanandinfin",
        "source": "https//pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm"
    },
    {
        "X": "Returns what tensor of sizeendstartstepleftlceil fractextend -",
        "Y": "1-D tensor",
        "Z": "Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Convert the data into atorch.Tensor.   Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns the product of all elements in the inputtensor?",
        "Y": "Returns the product of all elements in theinputtensor",
        "Z": "Returns the product of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns the product of each row of the inputtensor in the given dimensiondim.",
        "Y": "None",
        "Z": "Returns the product of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What is returned by the inputtensor?",
        "Y": "the product of each row of theinputtensor in the given dimensiondim",
        "Z": "Returns the product of all elements in theinputtensor. input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "If specified, the input tensor is casted before the operation is performed.",
        "Y": "todtype",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "Why is the input tensor casted todtype before the operation is performed?",
        "Y": "data type overflows",
        "Z": "input(Tensor) \u2013 the input tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What is returned in the given dimensiondim?",
        "Y": "the product of each row of theinputtensor",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "This is useful for preventing what?",
        "Y": "data type overflows",
        "Z": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What is the function that returns the product of each row of the inputtensor in the given dimensiondim?",
        "Y": "Returns the product of each row of theinputtensor in the given dimensiondim",
        "Z": "Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What indicates that the output tensor is of the same size asinput?",
        "Y": "IfkeepdimisTrue",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What does the output tensor have if keepdimisTrue?",
        "Y": "1 fewer dimension thaninput",
        "Z": "Returns the product of each row of theinputtensor in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What returns whether the output tensor hasdimretained or not?",
        "Y": "keepdim(bool)",
        "Z": "input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What is returned by the output tensor if keepdimisTrue?",
        "Y": "Returns the product of each row of theinputtensor in the given dimensiondim",
        "Z": "Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "If the output tensor is of the same size as input, what does it do?",
        "Y": "IfkeepdimisTrue",
        "Z": "Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "When is the output tensor of the same size asinput?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns the product of each row of theinputtensor in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What returns the product of each row of theinputtensor in the given dimensiondim?",
        "Y": "Returns the product of each row of theinputtensor in the given dimensiondim",
        "Z": "Returns the product of each row of theinputtensor in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What returns the output tensor of the same size asinput except in the dimensiondim where it is of size 1?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns the product of each row of theinputtensor in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "If keepdimisTrue, the output tensor has how many dimension fewer than input?",
        "Y": "1",
        "Z": "Returns the product of each row of theinputtensor in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What makes the output tensor of the same size as input?",
        "Y": "IfkeepdimisTrue",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "IfkeepdimisTrue, the output tensor has how much less dimension than input?",
        "Y": "1",
        "Z": "IfkeepdimisTrue, the output tensor is of the same size",
        "source": "https//pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "What does Alias fortorch.special.exp2() do?",
        "Y": "Alias fortorch.special.exp2()",
        "Z": "Alias fortorch.special.exp2().",
        "source": "https//pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2"
    },
    {
        "X": "What type of element does the function return?",
        "Y": "tensor",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.exp2.html#torch.exp2"
    },
    {
        "X": "What is the operation defined as?",
        "Y": "tensorscondition,x,ymust bebroadcastable",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is the current valid scalar and tensor combination?",
        "Y": "1",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is the Scalar of presently valid scalar and tensor combination?",
        "Y": "integral dtype and torch",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is the scalar of dtype and torch?",
        "Y": "x",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "Return what of elements selected from eitherxory, depending oncondition?",
        "Y": "a tensor",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What are two examples of scalar and tensor combination?",
        "Y": "floating dtype and torch",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is a valid scalar and tensor combination?",
        "Y": "floating dtype and torch",
        "Z": "Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What condition returns a tensor of elements selected from eitherxory or bothxory?",
        "Y": "BoolTensor",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What are two examples of valid scalar and tensor combination?",
        "Y": "floating dtype and torch",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "Where are values selected?",
        "Y": "indices",
        "Z": "The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What are two examples of a valid scalar and tensor combination?",
        "Y": "integral dtype and torch",
        "Z": "The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is the Scalar of floating dtype and torch?",
        "Y": "integral dtype and torch",
        "Z": "Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What are bebroadcastable?",
        "Y": "tensorscondition,x,ymust",
        "Z": "The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What must bebroadcastable?",
        "Y": "tensorscondition,x,y",
        "Z": "The tensorscondition,x,ymust bebroadcastable. Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What are two examples of current valid scalar and tensor combination?",
        "Y": "Currently valid scalar and tensor combination",
        "Z": "Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What condition(BoolTensor) yields x?",
        "Y": "True",
        "Z": "Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What type of dtype and torch?",
        "Y": "complex",
        "Z": "Note Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What condition(BoolTensor) is nonzero?",
        "Y": "True",
        "Z": "Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What type of dtype and torch are presently valid scalar and tensor combination?",
        "Y": "complex",
        "Z": "Currently valid scalar and tensor combination are",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What ield x if condition(BoolTensor) is True?",
        "Y": "y",
        "Z": "condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What does torch.where(condition,as_tuple=True) refer to?",
        "Y": "alsotorch.nonzero",
        "Z": "condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What does condition(BoolTensor) ield x?",
        "Y": "y",
        "Z": "condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is another name for torch.where(condition)is identical to totorch.nonzero(condition,as_tup",
        "Y": "alsotorch.nonzero()",
        "Z": "condition(BoolTensor) \u2013 When True (nonzero), yield x, otherwise yield y x(TensororScalar) \u2013 value (if",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is the equivalent of input.torch.empty_like(input)?",
        "Y": "totorch.empty",
        "Z": "Returns an uninitialized tensor with the same size asinput.torch.empty_like(input)is equivalent totorch.empty(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is the desired layout of the returned tensor?",
        "Y": "layout",
        "Z": "Returns an uninitialized tensor with the same size asinput.torch.empty_like(input)is equivalent totorch.empty(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like"
    },
    {
        "X": "What type of tensor is returned with the same size as input?",
        "Y": "uninitialized tensor",
        "Z": "Returns an uninitialized tensor with the same size asinput.torch.empty_like(input)is equivalent totorch.empty(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like"
    },
    {
        "X": "dtype(torch.dtype, optional) \u2013 what type of returned Tensor?",
        "Y": "the desired data type",
        "Z": "Returns an uninitialized tensor with the same size asinput.torch.empty_like(input)is equivalent totorch.empty(input.size(),dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like"
    },
    {
        "X": "What must inputandmat2 be?",
        "Y": "3-D tensors",
        "Z": "inputandmat2must be 3-D tensors each containing",
        "source": "https//pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like"
    },
    {
        "X": "What is seetorch.matmul() used for?",
        "Y": "broadcasting matrix products",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2. inputandmat2must be 3-D tensors each containing",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What does seetorch.matmul() do?",
        "Y": "notbroadcast",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2. inputandmat2must be 3-D tensors each containing",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What is used for broadcasting matrix products?",
        "Y": "seetorch.matmul()",
        "Z": "This operator supportsTensorFloat32. Note This function does notbroadcast.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What function is used for broadcasting matrix products?",
        "Y": "seetorch.matmul()",
        "Z": "inputandmat2must be 3-D tensors each containing",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What is the flag to choose between?",
        "Y": "a faster non-deterministic calculation, or a slower deterministic calculation",
        "Z": "This operator supportsTensorFloat32. Note This function does notbroadcast.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "This argument is only available for what?",
        "Y": "sparse-dense CUDA bmm",
        "Z": "This operator supportsTensorFloat32. Note This function does notbroadcast.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What is false out(Tensor,optional)?",
        "Y": "the output tensor",
        "Z": "Warning torch.matrix_rank()is deprecated in favor oftorch.linalg.matrix_rank()and will be removed in a future PyTorch release. The parametersymmetricwas",
        "source": "https//pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "What is returned from a normal distribution with mean0and variance1?",
        "Y": "a tensor filled with random numbers",
        "Z": "Returns a tensor filled with random numbers from a normal distribution",
        "source": "https//pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"
    },
    {
        "X": "Can be a collection like a list or tuple?",
        "Y": "variable number of arguments",
        "Z": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"
    },
    {
        "X": "ifNone uses a global default what?",
        "Y": "Default",
        "Z": "size(int...) \u2013 a sequence of integers defining the shape of the output tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"
    },
    {
        "X": "What is the input tensor containing the rates of the Poisson distribution generator?",
        "Y": "a pseudorandom number generator",
        "Z": "Returns a tensor of the same size asinputwith each element",
        "source": "https//pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson"
    },
    {
        "X": "What is returned for each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininput?",
        "Y": "a tensor of the same size",
        "Z": "Returns a tensor of the same size asinputwith each element",
        "source": "https//pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson"
    },
    {
        "X": "What is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped?",
        "Y": "Stack tensors",
        "Z": "Stack tensors in sequence vertically (row wise). This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped bytorch.atleast_2d(). tensors(sequence of Tensors) \u2013 sequence of tensors to concatenate out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson"
    },
    {
        "X": "How are all 1-D tensors reshaped?",
        "Y": "bytorch.atleast_2d()",
        "Z": "Stack tensors in sequence vertically (row wise). This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped bytorch.atleast_2d(). tensors(sequence of Tensors) \u2013 sequence of tensors to concatenate out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack"
    },
    {
        "X": "What is an example of a sequence of tensors to concatenate out(Tensor,optional) \u2013",
        "Y": "Example",
        "Z": "Stack tensors in sequence vertically (row wise). This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped bytorch.atleast_2d(). tensors(sequence of Tensors) \u2013 sequence of tensors to concatenate out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack"
    },
    {
        "X": "When this flag is False, some PyTorch warnings may only appear how many times per process?",
        "Y": "once per process",
        "Z": "When this flag is False (default) then some PyTorch warnings may only",
        "source": "https//pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack"
    },
    {
        "X": "Why do PyTorch warnings only appear once per process?",
        "Y": "excessive warning information",
        "Z": "When this flag is False (default) then some PyTorch warnings may only",
        "source": "https//pytorch.org/docs/stable/generated/torch.set_warn_always.html#torch.set_warn_always"
    },
    {
        "X": "What causes PyTorch warnings to always appear?",
        "Y": "Setting it to True",
        "Z": "When this flag is False (default) then some PyTorch warnings may only",
        "source": "https//pytorch.org/docs/stable/generated/torch.set_warn_always.html#torch.set_warn_always"
    },
    {
        "X": "What flag is used to force warnings to always be emitted?",
        "Y": "b(bool)",
        "Z": "When this flag is False (default) then some PyTorch warnings may only",
        "source": "https//pytorch.org/docs/stable/generated/torch.set_warn_always.html#torch.set_warn_always"
    },
    {
        "X": "What is the default value for the highest integer to be drawn from the distribution?",
        "Y": "0. high(int)",
        "Z": "Returns a tensor with the same shape as Tensorinputfilled with",
        "source": "https//pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like"
    },
    {
        "X": "Low(int,optional) \u2013 what integer to be drawn from the distribution?",
        "Y": "Lowest integer",
        "Z": "Returns a tensor with the same shape as Tensorinputfilled with",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is high(int)?",
        "Y": "One above the highest integer",
        "Z": "high(int) \u2013 One above the highest integer to be drawn from the distribution. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.",
        "source": "https//pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like"
    },
    {
        "X": "What is returned with the same data and number of elements as input?",
        "Y": "a tensor",
        "Z": "Returns a tensor with the same data and number of elements asinput,",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "What will the returned tensor be if it is not a view of input?",
        "Y": "a copy",
        "Z": "Returns a tensor with the same data and number of elements asinput,",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "What does Seetorch.Tensor.view() return when it is possible to return?",
        "Y": "a view",
        "Z": "Returns a tensor with the same data and number of elements asinput,",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "What is the name of the function that determines when it is possible to return a view?",
        "Y": "Seetorch.Tensor.view()",
        "Z": "Seetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remaining",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "What is the tensor to be reshaped shape?",
        "Y": "input(Tensor)",
        "Z": "Seetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remaining",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "When does Seetorch.Tensor.view() occur?",
        "Y": "when it is possible to return a view",
        "Z": "Seetorch.Tensor.view()on when it is possible to return a view. A single dimension may be -1, in which case it\u2019s inferred from the remaining",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "What is the tensor to be reshaped shape(tuple of python",
        "Y": "input(Tensor)",
        "Z": "Returns a tensor with the same data and number of elements asinput,",
        "source": "https//pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "If the left boundary is closed, what is the default?",
        "Y": "Ifrightis False",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "Returns what with the same size as input?",
        "Y": "a new tensor",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What does the right returned index satisfie?",
        "Y": "False boundaries",
        "Z": "right returned index satisfies False boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i] True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i] input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What satisfies False boundaries?",
        "Y": "right returned index",
        "Z": "right returned index satisfies False boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i] True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i] input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What must the boundaries(Tensor) contain?",
        "Y": "monotonically increasing sequence",
        "Z": "input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "Return index satisfies what?",
        "Y": "False boundaries",
        "Z": "returned index satisfies False boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i] True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i] input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What is input(TensororScalar) containing the search value(s)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i] input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What type of boundaries[i-1]input[m][n]?",
        "Y": "False",
        "Z": "False boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i] True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i] input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What is input(TensororScalar)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What must a 1-D tensor contain?",
        "Y": "monotonically increasing sequence",
        "Z": "boundaries[i-1]<input[m][n]...[l][x]<=boundaries[i] True boundaries[i-1]<=input[m][n]...[l][x]<boundaries[i] input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What type of tensor must contain a monotonically increasing sequence?",
        "Y": "1-D",
        "Z": "boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What is the input(TensororScalar) containing the search value(s)?",
        "Y": "N-D tensor or a Scalar",
        "Z": "input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What tensor must contain a monotonically increasing sequence?",
        "Y": "1-D",
        "Z": "input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What does out_int32(bool,optional) indicate?",
        "Y": "output data type",
        "Z": "input(TensororScalar) \u2013 N-D tensor or a Scalar containing the search value(s). boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What type of sequence must a 1-D tensor contain?",
        "Y": "monotonically increasing",
        "Z": "boundaries(Tensor) \u2013 1-D tensor, must contain a monotonically increasing sequence. out_int32(bool,optional) \u2013 indicate the output data type. torch.int32 if True, torch.int64 otherwise.",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What is the size ofboundaries?",
        "Y": "one pass the last index",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "If False, gets what for each value ininputfromboundaries?",
        "Y": "lower bound index",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "If False, gets what instead of the lower bound index for each value ininputfromboundaries?",
        "Y": "upper bound index",
        "Z": "right(bool,optional) \u2013 if False, return the first suitable location that is found. If True, return the",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "PyTorch provides several features for working with what language?",
        "Y": "C++",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "At what level is the following support available?",
        "Y": "high level",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What are you looking for?",
        "Y": "PyTorch C++ API docs",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What company provides several features for working with C++?",
        "Y": "PyTorch",
        "Z": "Note If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does PyTorch provide for working with C++?",
        "Y": "several features",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "At what level of support is PyTorch available?",
        "Y": "high level",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where are TorchScript models saved from?",
        "Y": "Python",
        "Z": "Loading serialized TorchScript models saved from Python Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is used for preprocessing TorchScript input?",
        "Y": "C++ Tensor API",
        "Z": "Loading serialized TorchScript models saved from Python Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where are TorchScript models saved?",
        "Y": "Python",
        "Z": "Loading serialized TorchScript models saved from Python Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is one way to do simple model modifications if needed?",
        "Y": "Loading serialized TorchScript models saved from Python",
        "Z": "Loading serialized TorchScript models saved from Python Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is used for preprocessing TorchScript models?",
        "Y": "C++ Tensor API",
        "Z": "Loading serialized TorchScript models saved from Python Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Doing what if needed?",
        "Y": "simple model modifications",
        "Z": "Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is used for preprocessing input?",
        "Y": "C++ Tensor API",
        "Z": "Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the C++ Tensor API used for?",
        "Y": "preprocessing",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Doing what if needed (e.g. pulling out submodules)?",
        "Y": "simple model modifications",
        "Z": "Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is used for preprocessing?",
        "Y": "C++ Tensor API",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the API used for input and preprocessing?",
        "Y": "C++ Tensor API",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What looks and behaves the same as the Python API?",
        "Y": "C++ tensor indexing API",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find details on the use of the C++ tensor indexing API?",
        "Y": "pytorch.org/cppdocs/notes/tensor_indexing.html",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Most of the tensor and autograd operations in PyTorch Python API are also available in what API?",
        "Y": "C++",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the tensor and autograd operations in PyTorch?",
        "Y": "torch",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "C++ tensor indexing API looks and behaves the same as what API?",
        "Y": "Python",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the C++ tensor indexing API?",
        "Y": "https",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What are some examples of tensormethods?",
        "Y": "add/reshape/clone",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What language has a tensor indexing API?",
        "Y": "C++",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What API looks and behaves the same as the Python API?",
        "Y": "C++ tensor indexing API",
        "Z": "C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "The tensor autograd APIs and thetorch",
        "Y": "dynamic neural networks",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find more details on the tensor autograd APIs?",
        "Y": "https",
        "Z": "C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find more information about the tensor autograd APIs?",
        "Y": "https",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What APIs are crucial for building dynamic neural networks in C++ frontend?",
        "Y": "tensor autograd",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find more information about the tensor autograd APIs and thetorch",
        "Y": "https",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What might be undesirable in workflows where the model has to be authored in C++?",
        "Y": "Python component",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "In what script is authoring a neural net model done?",
        "Y": "TorchScript",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "In what language can a neural net model be authored?",
        "Y": "C++",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What API does astorch",
        "Y": "Python",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the model authoring and training API?",
        "Y": "PyTorch C++",
        "Z": "For an overview of the PyTorch C++ model authoring and training API, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the C++ model authoring and training API?",
        "Y": "PyTorch",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What can be found at http",
        "Y": "a detailed tutorial",
        "Z": "For a detailed tutorial on how to use the API, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is available at http",
        "Y": "a detailed tutorial",
        "Z": "For a detailed tutorial on how to use the API, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does pytorch.org provide for components such as",
        "Y": "Docs",
        "Z": "Docs for components such astorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find docs for components such as",
        "Y": "api/library_root.html",
        "Z": "Docs for components such astorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What library contains all of the above C++ APIs?",
        "Y": "libtorch",
        "Z": "For guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "How many types of libtorch binaries are provided on Linux?",
        "Y": "two",
        "Z": "For guidance on how to install and link with libtorch (the library that contains all of the above C++ APIs), please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Supportsbroadcasting to what shape?",
        "Y": "common shape",
        "Z": "Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise. Supportsbroadcasting to a common shape,",
        "source": "https//pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign"
    },
    {
        "X": "What are the magnitudes of a floating-point tensor?",
        "Y": "input(Tensor)",
        "Z": "Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise. Supportsbroadcasting to a common shape,",
        "source": "https//pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign"
    },
    {
        "X": "What contains value(s) whose signbit(s) are applied to the magnitudes ininput?",
        "Y": "other(TensororNumber)",
        "Z": "Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise. Supportsbroadcasting to a common shape,",
        "source": "https//pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign"
    },
    {
        "X": "What does value represent in the given dimensiondim?",
        "Y": "thekth smallest element of each row of theinputtensor",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch._assert.html#torch._assert"
    },
    {
        "X": "Andindicesis what of each element found?",
        "Y": "index location",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What returns the smallest element of each row of the inputtensor in the given dimensiondim?",
        "Y": "a namedtuple",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What is the index location of each element found?",
        "Y": "Andindices",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What happens ifdimis not given?",
        "Y": "the last dimension of theinputis chosen",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "Ifdimis not given, the last dimension of theinputis chosen?",
        "Y": "IfkeepdimisTrue",
        "Z": "Ifdimis not given, the last dimension of theinputis chosen. IfkeepdimisTrue, both thevaluesandindicestensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What is another name for dimis squeezed?",
        "Y": "seetorch.squeeze()",
        "Z": "IfkeepdimisTrue, both thevaluesandindicestensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What type of tensor is input?",
        "Y": "CUDA",
        "Z": "Ifdimis not given, the last dimension of theinputis chosen. IfkeepdimisTrue, both thevaluesandindicestensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "If both thevaluesandindicestensors are the same size as input, what is the default?",
        "Y": "IfkeepdimisTrue",
        "Z": "IfkeepdimisTrue, both thevaluesandindicestensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "If inputis a CUDA tensor and there are what, this function may nondeterministically returnindices for any of them",
        "Y": "multiple validkth values",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What is the smallest element in a CUDA tensor?",
        "Y": "k",
        "Z": "IfkeepdimisTrue, both thevaluesandindicestensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "Inputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically do what for",
        "Y": "returnindices",
        "Z": "Note Wheninputis a CUDA tensor and there are multiple validkth values, this function may nondeterministically returnindicesfor any of them. input(Tensor) \u2013 the input tensor. k(int) \u2013 k for the k-th smallest element dim(int,optional) \u2013 the dimension to find the kth value along keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor)",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "Out(tuple,optional) \u2013 the output tuple of (Tensor, LongTensor) can",
        "Y": "output buffers",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What is the return matrixUis upper-triangular?",
        "Y": "IfupperisTrue",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "IfupperisTrue, the returned matrixLis is what?",
        "Y": "lower-triangular",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the return matrixLis lower-triangular?",
        "Y": "IfupperisFalse",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the name of the warning that the matrix is lower-triangular?",
        "Y": "Warning",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "IfupperisTrue, andAAAis a batch of symmetric positive-definite matrices, then the returned tensor",
        "Y": "upper-triangular Cholesky factors",
        "Z": "IfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "IfupperisTrue, andAAAis a batch of symmetric positive-definite matrices?",
        "Y": "Warning",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is a batch of symmetric positive-definite matrices?",
        "Y": "IfupperisTrue",
        "Z": "IfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "When will the returned tensor be composed of lower-triangular Cholesky factors of each of the individual matrices?",
        "Y": "whenupperisFalse",
        "Z": "IfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What should be replaced with a replacement for torch.linalg.cholesky()?",
        "Y": "L=torch.cholesky(A)",
        "Z": "IfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "WhenupperisFalse, the returned tensor will be composed of what?",
        "Y": "lower-triangular Cholesky factors",
        "Z": "IfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What should be replaced with torch.linalg.cholesky()?",
        "Y": "L=torch.cholesky(A)",
        "Z": "IfupperisTrue, andAAAis a batch of symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What flag indicates whether to return a lower or upper triangular matrix?",
        "Y": "upper",
        "Z": "L=torch.cholesky(A)should be replaced with U=torch.cholesky(A,upper=True)should be replaced with input(Tensor) \u2013 the input tensorAAAof size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the default value of the flag that indicates whether to return a lower or upper triangular matrix?",
        "Y": "False",
        "Z": "torch.cholesky()is deprecated in favor oftorch.linalg.cholesky()and will be removed in a future PyTorch release. L=torch.cholesky(A)should be replaced with U=torch.cholesky(A,upper=True)should be replaced with input(Tensor) \u2013 the input tensorAAAof size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What should be replaced with U=torch.cholesky(A,upper=True)?",
        "Y": "input(Tensor)",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the input tensorAAAof size(,n,n)(*, n, n)(,",
        "Y": "input(Tensor)",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the default value for the output matrix?",
        "Y": "False out(Tensor,optional)",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What does the histogram of a tensor do?",
        "Y": "Computes the histogram of a tensor",
        "Z": "Computes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum and",
        "source": "https//pytorch.org/docs/stable/generated/torch.histc.html#torch.histc"
    },
    {
        "X": "Ifminandmaxare both zero, what are the minimum and maximum values of the data used?",
        "Y": "Ifminandmaxare both zero",
        "Z": "Computes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum and",
        "source": "https//pytorch.org/docs/stable/generated/torch.histc.html#torch.histc"
    },
    {
        "X": "What is the histogram represented as?",
        "Y": "a tensor Tensor",
        "Z": "Computes the histogram of a tensor. The elements are sorted into equal width bins betweenminandmax. Ifminandmaxare both zero, the minimum and",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "Where are these features sometimes found?",
        "Y": "run-time flags",
        "Z": "Prototype",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What does Thetorchtextpackage contain?",
        "Y": "Package Reference PyTorch Libraries",
        "Z": "Prototype",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What is the name of the search page?",
        "Y": "Index Module Index Search Page",
        "Z": "Index Module Index Search Page",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What is the tensor to compare other(Tensororfloat)?",
        "Y": "input(Tensor)",
        "Z": "Computes element-wise equality The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis equal tootherand False elsewhere Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.eq.html#torch.eq"
    },
    {
        "X": "What is a boolean tensor that is?",
        "Y": "True",
        "Z": "Computes element-wise equality The second argument can be a number or a tensor whose shape isbroadcastablewith the first argument. input(Tensor) \u2013 the tensor to compare other(Tensororfloat) \u2013 the tensor or value to compare out(Tensor,optional) \u2013 the output tensor. A boolean tensor that is True whereinputis equal tootherand False elsewhere Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.eq.html#torch.eq"
    },
    {
        "X": "Returns what with a dimension of size one inserted at the specified position?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with a dimension of size one inserted at the",
        "source": "https//pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"
    },
    {
        "X": "What does the returned tensor share with this tensor?",
        "Y": "underlying data",
        "Z": "Returns a new tensor with a dimension of size one inserted at the",
        "source": "https//pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"
    },
    {
        "X": "What value within the range [-input.dim()-1,input.dim()+1]can be used?",
        "Y": "Adimvalue",
        "Z": "Returns a new tensor with a dimension of size one inserted at the",
        "source": "https//pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"
    },
    {
        "X": "What will correspond tounsqueeze()applied atdim=dim+input.dim()+1?",
        "Y": "Negativedim",
        "Z": "Returns a new tensor with a dimension of size one inserted at the",
        "source": "https//pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"
    },
    {
        "X": "What is the numerical rank of?",
        "Y": "2-D tensor",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"
    },
    {
        "X": "The method to compute the matrix rank is done using what by default?",
        "Y": "SVD",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "The computation of the rank is done by obtaining what?",
        "Y": "eigenvalues",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "What is the threshold below which the singular values (or the eigenvalues whensymmetricisTrue) are considered to be 0.",
        "Y": "tolis",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "If what is not specified, tolis the threshold below which the singular values (or the eigenvalues whensymmetricisTrue) are",
        "Y": "Iftolis",
        "Z": "tolis the threshold below which the singular values (or the eigenvalues",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "What is deprecated in favor oftorch.linalg.matrix_rank()?",
        "Y": "torch.matrix_rank()",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "What was the parametersymmetric renamed to?",
        "Y": "intorch.linalg.matrix_rank()tohermitian",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "What is the input 2-D tensor tol(float,optional) \u2013 the tolerance value?",
        "Y": "input(Tensor)",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "What does the documentation of this method provide?",
        "Y": "exact semantics",
        "Z": "If there are multiple maximal values then the indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "If there are multiple maximal values, the indices of the first maximal value are returned.",
        "Y": "multiple maximal values",
        "Z": "This is the second value returned bytorch.max(). See its",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "Returns the indices of the maximum value of all elements in the inputtensor. This is the second value returned?",
        "Y": "bytorch.max()",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor. This is the second value returned bytorch.max(). See its",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "See its documentation for the exact what of this method?",
        "Y": "semantics",
        "Z": "input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What are returned if there are multiple maximal values?",
        "Y": "the indices of the first maximal value",
        "Z": "If there are multiple maximal values then the indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What does the documentation of bytorch.max() provide?",
        "Y": "exact semantics",
        "Z": "This is the second value returned bytorch.max(). See its",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What method returns the indices of the maximum values of a tensor across a dimension?",
        "Y": "bytorch.max()",
        "Z": "If there are multiple maximal values then the indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What is the second value returned bytorch.max()?",
        "Y": "Returns the indices of the maximum values of a tensor across a dimension",
        "Z": "input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What do you need to know about the second value returned bytorch.max()?",
        "Y": "semantics",
        "Z": "This is the second value returned bytorch.max(). See its",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What is returned if there are multiple maximal values?",
        "Y": "the indices of the first maximal value",
        "Z": "Note If there are multiple maximal values then the indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What method returns the second value of the indices of the maximum values of a tensor across a dimension?",
        "Y": "bytorch.max()",
        "Z": "Note If there are multiple maximal values then the indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What returns the argmax of the flattened input?",
        "Y": "IfNone",
        "Z": "input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "What is the name of the method that returns the indices of the maximum values of a tensor across a dimension?",
        "Y": "bytorch.max()",
        "Z": "If there are multiple maximal values then the indices of the first maximal value are returned. input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "Returns the indices of the maximum values of a tensor across a dimension. This is the second value returned?",
        "Y": "bytorch.max()",
        "Z": "input(Tensor) \u2013 the input tensor. Example",
        "source": "https//pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "Fills the tensor with numbers drawn from what distribution?",
        "Y": "Cauchy",
        "Z": "Fills the tensor with numbers drawn from the Cauchy distribution",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_"
    },
    {
        "X": "What does angle return pi for?",
        "Y": "negative real numbers",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Note Starting in PyTorch 1.8, angle returns pi for negative real numbers,",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_"
    },
    {
        "X": "The function would return zero for all real numbers and not propagate what?",
        "Y": "floating-point NaNs",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Note Starting in PyTorch 1.8, angle returns pi for negative real numbers,",
        "source": "https//pytorch.org/docs/stable/generated/torch.angle.html#torch.angle"
    },
    {
        "X": "What is an example of a function that returns zero for all real numbers and not propagate floating-point NaNs?",
        "Y": "Example",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Note Starting in PyTorch 1.8, angle returns pi for negative real numbers,",
        "source": "https//pytorch.org/docs/stable/generated/torch.angle.html#torch.angle"
    },
    {
        "X": "Writes all values from the tensorsrcintoselfat the indices specified in what?",
        "Y": "theindextensor",
        "Z": "Writes all values from the tensorsrcintoselfat the indices",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the output index for each value insrc specified by?",
        "Y": "its index insrcfordimension!=dim",
        "Z": "Writes all values from the tensorsrcintoselfat the indices",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "Selfis updated as",
        "Y": "ingather()",
        "Z": "Writes all values from the tensorsrcintoselfat the indices",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What must the values of index be as forgather()?",
        "Y": "between0andself.size(dim)-1inclusive",
        "Z": "Moreover, as forgather(), the values ofindexmust be",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What must be the values of index as forgather()?",
        "Y": "between0andself.size(dim)-1inclusive",
        "Z": "Moreover, as forgather(), the values ofindexmust be",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "When indices are not unique, the behavior is what?",
        "Y": "non-deterministic",
        "Z": "When indices are not unique, the behavior is non-deterministic (one of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What does the optionalreduceargument allow specification of?",
        "Y": "optional reduction operation",
        "Z": "The backward pass is implemented only forsrc.shape==index.shape. Additionally accepts an optionalreduceargument that allows",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "For each value insrc, the reduction operation is applied to what?",
        "Y": "index inself",
        "Z": "When indices are not unique, the behavior is non-deterministic (one of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What operation is applied to all values in the tensorsrcintoselfat the indicies specified in the index?",
        "Y": "optional reduction operation",
        "Z": "When indices are not unique, the behavior is non-deterministic (one of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What is the multiplication operation used to update the index inself?",
        "Y": "3-D tensor and reduction",
        "Z": "Additionally accepts an optionalreduceargument that allows",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What is the default value of start(float)?",
        "Y": "Default",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep. Step is",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "How many tensors does the function return?",
        "Y": "1",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep. Step is",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is step?",
        "Y": "the gap between two values in the tensor",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep. Step is",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is the behavior of this function inconsistent with?",
        "Y": "Python\u2019s range builtin",
        "Z": "Warning This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What language has a range builtin?",
        "Y": "Python",
        "Z": "Warning This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is step(float)?",
        "Y": "the gap between each pair of adjacent points",
        "Z": "start(float) \u2013 the starting value for the set of points. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "Usetorch.arange() is inconsistent with what programming language's range builtin?",
        "Y": "Python",
        "Z": "This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "Why is usetorch.arange() deprecated?",
        "Y": "its behavior is inconsistent with Python\u2019s range builtin",
        "Z": "This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is the ending value for the set of points step(float)?",
        "Y": "end(float)",
        "Z": "end(float) \u2013 the ending value for the set of points step(float) \u2013 the gap between each pair of adjacent points. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is the default value for the end of a set of points?",
        "Y": "Default",
        "Z": "end(float) \u2013 the ending value for the set of points step(float) \u2013 the gap between each pair of adjacent points. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is equivalent to callinginput.expand(shape)?",
        "Y": "Broadcastsinput",
        "Z": "Broadcastsinputto the shapeshape.",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is the equivalent to Broadcastsinput to the shapeshape?",
        "Y": "callinginput.expand(shape)",
        "Z": "Broadcastsinputto the shapeshape.",
        "source": "https//pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"
    },
    {
        "X": "What is another name for details?",
        "Y": "Seeexpand()",
        "Z": "Broadcastsinputto the shapeshape.",
        "source": "https//pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"
    },
    {
        "X": "What is the new shape?",
        "Y": "shape",
        "Z": "Broadcastsinputto the shapeshape.",
        "source": "https//pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"
    },
    {
        "X": "What does shape(list, tuple, ortorch.Size) do?",
        "Y": "Example",
        "Z": "Broadcastsinputto the shapeshape.",
        "source": "https//pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"
    },
    {
        "X": "Returns True what if the data type ofinputis a complex data type?",
        "Y": "if the data type ofinputis a complex data type",
        "Z": "Returns True if the data type ofinputis a complex data type i.e.,",
        "source": "https//pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"
    },
    {
        "X": "What is an example of a tensor with all the dimensions ofinputof size1removed?",
        "Y": "shape",
        "Z": "Returns a tensor with all the dimensions ofinputof size1removed. For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.is_complex.html#torch.is_complex"
    },
    {
        "X": "What is done only in the given dimension whendimis given?",
        "Y": "squeeze operation",
        "Z": "Returns a tensor with all the dimensions ofinputof size1removed. For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What does squeeze(input,0) leave unchanged?",
        "Y": "tensor",
        "Z": "For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What returns a tensor with all the dimensions ofinputof size1removed?",
        "Y": "Returns a tensor with all the dimensions ofinputof size1removed",
        "Z": "Returns a tensor with all the dimensions ofinputof size1removed. For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "Whendimis given, what operation is done only in the given dimension?",
        "Y": "squeeze operation",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What is theouttensor of shape?",
        "Y": "ifinputis of shape",
        "Z": "For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "Whendimis given, what is done only in the given dimension?",
        "Y": "squeeze operation",
        "Z": "For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What does a squeeze operation do when a tensor is of shape?",
        "Y": "Note",
        "Z": "For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What is an example of a squeeze operation?",
        "Y": "ifinputis of shape",
        "Z": "For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What is a squeeze operation done only in the given dimension?",
        "Y": "Note",
        "Z": "For example, ifinputis of shape",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "Whendimis is a squeeze operation done only in the given dimension?",
        "Y": "given",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What does a squeeze operation only in the given dimension do?",
        "Y": "Ifinputis of shape",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "The returned tensor shares storage with what?",
        "Y": "the input tensor",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What is the name of the warning that the input tensor shares the storage with the output tensor?",
        "Y": "Warning",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What is the input of a tensor?",
        "Y": "shape",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What changes the contents of the other?",
        "Y": "changing the contents of one",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What is the name of the warning that is given when a tensor shares the storage with the input tensor?",
        "Y": "Warning",
        "Z": "Whendimis given, a squeeze operation is done only in the given",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "The returned tensor shares the storage with what?",
        "Y": "the input tensor",
        "Z": "Note The returned tensor shares the storage with the input tensor,",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What does the tensor have that can lead to unexpected errors?",
        "Y": "a batch dimension of size 1",
        "Z": "Note The returned tensor shares the storage with the input tensor,",
        "source": "https//pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "What will fork_rng() emit if your machine has a lot of devices?",
        "Y": "a warning",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "If you explicitly specify devices, this warning will be what?",
        "Y": "suppressed",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is iterable of CUDA IDs?",
        "Y": "devices",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What does devices(iterable of CUDA IDs) represent?",
        "Y": "CUDA devices",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What function will run very slowly if your machine has a lot of devices?",
        "Y": "fork_rng()",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What are CUDA devices for which to fork the RNG?",
        "Y": "devices",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "If enabled(bool) is true, the RNG is not forked.",
        "Y": "ifFalse",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is enabled(bool) used for?",
        "Y": "convenience argument",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is the state of the random number generator?",
        "Y": "atorch.ByteTensor",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What returns the initial seed for generating random numbers as a Pythonlong?",
        "Y": "atorch.Generatorobject",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is a convenience argument for easily disabling the context manager without having to delete it and unindent your Python code under it?",
        "Y": "enabled",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is enabled(bool) a convenience argument for?",
        "Y": "disabling the context manager",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "The value must be within what range?",
        "Y": "inclusive range",
        "Z": "Returns the random number generator state as atorch.ByteTensor. Returns the initial seed for generating random numbers as a",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is used to seed the RNG?",
        "Y": "a 64 bit number",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is the multiplication of a product of Householder matrices with a general matrix?",
        "Y": "matrix-matrix",
        "Z": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What does the multiplication of amnm times nmnmatrixC(given byother) with?",
        "Y": "a matrixQ",
        "Z": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is represented using Householder reflectors?",
        "Y": "Orthogonal or Unitary Matrices",
        "Z": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What does the matrix-matrix multiplication of a product of Householder matrices with a general matrix do?",
        "Y": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix",
        "Z": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is used to represent Q?",
        "Y": "Householder reflectors",
        "Z": "Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix. Multiplies am\u00d7nm \\times nm\u00d7nmatrixC(given byother) with a matrixQ,",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "Supports inputs of float, double, cfloat and what other dtype?",
        "Y": "cdouble",
        "Z": "Supports inputs of float, double, cfloat and cdouble dtypes.",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What type of inputs does it support?",
        "Y": "batched inputs",
        "Z": "Supports inputs of float, double, cfloat and cdouble dtypes.",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition?",
        "Y": "torch.geqrf()",
        "Z": "torch.geqrf()can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition. input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is tau(Tensor)?",
        "Y": "tensor of shape",
        "Z": "Supports inputs of float, double, cfloat and cdouble dtypes.",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What can be used to form the Householder representation of matrixQ?",
        "Y": "the QR decomposition",
        "Z": "See also torch.geqrf()can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition. input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is the tensor of shape where*is zero or more batch dimensions andmnequals tomorndepending on theleft",
        "Y": "input(Tensor)",
        "Z": "torch.geqrf()can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition. input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What controls the order of multiplication?",
        "Y": "left(bool)",
        "Z": "input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is the tensor of shape(*, m, n) where*is zero or more batch dimensions?",
        "Y": "other(Tensor)",
        "Z": "torch.geqrf()can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition. input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What can be used to form the Householder representation of matrixQfrom the QR decomposition?",
        "Y": "torch.geqrf()",
        "Z": "torch.geqrf()can be used to form the Householder representation(input, tau)of matrixQfrom the QR decomposition. input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is a tensor of shape where*is zero or more batch dimensions?",
        "Y": "tau",
        "Z": "input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What controls whether the matrixQis conjugate transposed or not?",
        "Y": "transpose",
        "Z": "input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is the output Tensor?",
        "Y": "Tensor",
        "Z": "input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is the output Tensor ignored?",
        "Y": "ifNone",
        "Z": "input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is the default value of the output Tensor?",
        "Y": "Default",
        "Z": "input(Tensor) \u2013 tensor of shape(*, mn, k)where*is zero or more batch dimensions",
        "source": "https//pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"
    },
    {
        "X": "What is the name of Alias fortorch.linalg.pinv()?",
        "Y": "Alias fortorch.linalg.pinv()",
        "Z": "Alias fortorch.linalg.pinv()",
        "source": "https//pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse"
    },
    {
        "X": "What is not guaranteed to produce 100% reproducible results across?",
        "Y": "PyTorch releases, individual commits, or different platforms",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse"
    },
    {
        "X": "Results may not be reproducible between CPU and GPU executions even when using what?",
        "Y": "identical seeds",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Completely reproducible results are not guaranteed across what releases, individual commits, or different platforms?",
        "Y": "PyTorch releases",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What can happen between CPU and GPU executions?",
        "Y": "results may not be reproducible",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is one of the steps you can take to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyT",
        "Y": "Warning",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What can you do to limit the number of sources of nondeterministic behavior for a specific platform, device, and PyTorch release?",
        "Y": "limit the number of sources of nondeterministic behavior",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What will result in the same result?",
        "Y": "multiple calls to those operations",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is another word for nondeterministic behavior?",
        "Y": "Warning",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Are deterministic operations faster or slower than nondeterministic operations?",
        "Y": "slower",
        "Z": "Warning Deterministic operations are often slower than nondeterministic operations, so",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What does python use to seed the global NumPy RNG?",
        "Y": "NumPy",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Which operations are often slower than nondeterministic operations?",
        "Y": "Deterministic operations",
        "Z": "Deterministic operations are often slower than nondeterministic operations, so",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What may some applications and libraries use, but not the global RNG?",
        "Y": "NumPy Random Generator objects",
        "Z": "You can usetorch.manual_seed()to seed the RNG for all devices (both",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "For what devices can you usetorch.manual_seed() to seed the RNG?",
        "Y": "all devices",
        "Z": "You can usetorch.manual_seed()to seed the RNG for all devices (both",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What may some applications and libraries use, but not the global NumPy RNG?",
        "Y": "NumPy Random Generator objects",
        "Z": "For custom operators, you might need to set python seed as well",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Where can you find information about libraries that use random number generators?",
        "Y": "the documentation",
        "Z": "However, some applications and libraries may use NumPy Random Generator objects,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What can you do if you are using other libraries that use random number generators?",
        "Y": "set consistent seeds",
        "Z": "If you are using any other libraries that use random number generators, refer to",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What do other libraries use?",
        "Y": "random number generators",
        "Z": "If you are using any other libraries that use random number generators, refer to",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the cost of disabling the benchmarking feature?",
        "Y": "reduced performance",
        "Z": "Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If you do not need what feature of your application, performance might improve if the benchmarking feature is enabled withtorch.backends.",
        "Y": "reproducibility across multiple executions",
        "Z": "Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What setting does the benchmarking feature differ from?",
        "Y": "thetorch.backends.cudnn.deterministicsetting",
        "Z": "Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What setting is different from thetorch.backends.cudnn.deterministicsetting discussed below?",
        "Y": "withtorch.backends.cudnn.benchmark=True",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the benchmark setting different from?",
        "Y": "thetorch.backends.cudnn.deterministicsetting",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If you do not need reproducibility across multiple executions of your application, then performance might improve if the benchmarking feature is enabled?",
        "Y": "withtorch.backends.cudnn.benchmark=True",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What setting is different from withtorch.backends.cudnn.benchmark=True?",
        "Y": "thetorch.backends.cudnn.deterministicsetting",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What setting is different from the one discussed below?",
        "Y": "thetorch.backends.cudnn.deterministicsetting",
        "Z": "Note that this setting is different from thetorch.backends.cudnn.deterministicsetting discussed below. torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What type of algorithm does torch.use_algorithms?",
        "Y": "deterministic",
        "Z": "torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What type of algorithms does torch.use?",
        "Y": "deterministic",
        "Z": "torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What do you need if an operation does not act correctly according to the documentation?",
        "Y": "a deterministic implementation of an operation that does not have one",
        "Z": "Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What documentation contains a full list of affected operations?",
        "Y": "fortorch.use_deterministic_algorithms()",
        "Z": "Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What implementation oftorch.Tensor.index_add_() is nondeterministic?",
        "Y": "CUDA",
        "Z": "Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "When is the alternate deterministic implementation oftorch.bmm() used?",
        "Y": "when the deterministic flag is turned on",
        "Z": "For example, running the nondeterministic CUDA implementation oftorch.Tensor.index_add_()will throw an error",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Whentorch.bmm() is called with sparse-dense CUDA tensors, what does it typically use",
        "Y": "a nondeterministic algorithm",
        "Z": "Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is called with sparse-dense CUDA tensors?",
        "Y": "Whentorch.bmm()",
        "Z": "Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If you are using CUDA tensors and your CUDA version is what?",
        "Y": "10.2 or greater",
        "Z": "Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Whentorch.bmm() is called with what type of CUDA tensors?",
        "Y": "sparse-dense",
        "Z": "Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "When should you set the environment variableCUBLAS_WORKSPACE_CONFIG?",
        "Y": "if you are using CUDA tensors",
        "Z": "Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the CUDA version of CUDA?",
        "Y": "10.2 or greater",
        "Z": "Furthermore, if you are using CUDA tensors, and your CUDA version is 10.2 or greater, you",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What does disabling ensures that CUDA selects the same algorithm each time an application is run?",
        "Y": "CUDA convolution benchmarking",
        "Z": "While disabling CUDA convolution benchmarking (discussed above) ensures that",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What does disabling CUDA convolution benchmarking ensure?",
        "Y": "CUDA selects the same algorithm each time an application is run",
        "Z": "While disabling CUDA convolution benchmarking (discussed above) ensures that",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What operations will behave deterministically, unless eithertorch.use_deterministic_algorithms() is set?",
        "Y": "other PyTorch operations",
        "Z": "While disabling CUDA convolution benchmarking (discussed above) ensures that",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "For custom operators, you might need to set what kind of seed as well?",
        "Y": "python",
        "Z": "For custom operators, you might need to set python seed as well",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the equivalent to calling.tensor_split?",
        "Y": "torch",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "What is used to split a tensor with three or more dimensions into multiple tensors depthwise?",
        "Y": "indices_or_sections",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "What is the difference between a split and a view ofinput?",
        "Y": "ifindices_or_sectionsis an integer it must evenly divide the split dimension",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "What casting rules are described in the type promotiondocumentation?",
        "Y": "PyTorch",
        "Z": "Determines if a type conversion is allowed under PyTorch casting rules",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "From(dpython",
        "Y": "originaltorch.dtype",
        "Z": "Determines if a type conversion is allowed under PyTorch casting rules",
        "source": "https//pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast"
    },
    {
        "X": "To what does dpython",
        "Y": "targettorch.dtype",
        "Z": "Determines if a type conversion is allowed under PyTorch casting rules",
        "source": "https//pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast"
    },
    {
        "X": "What is an example of a PyTorch casting rule?",
        "Y": "Example",
        "Z": "Determines if a type conversion is allowed under PyTorch casting rules",
        "source": "https//pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast"
    },
    {
        "X": "What input types does complexA support?",
        "Y": "float, double, cfloat and cdouble dtypes",
        "Z": "For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What type of matrices does complexA support?",
        "Y": "batches of matrices",
        "Z": "For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the name of the function that computes the sign and natural logarithm of the absolute value of the determinant of a square",
        "Y": "Note",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What does complexA return?",
        "Y": "the angle and the natural logarithm of the modulus of the determinant",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What does complexA support?",
        "Y": "batches of matrices",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the name of the feature that supports batches of matrices?",
        "Y": "Note",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What types of inputs does torch.linalg.det() support?",
        "Y": "float, double, cfloat and cdouble dtypes",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What does torch.lu() do when inputs are on a CUDA device?",
        "Y": "synchronizes that device with the CPU",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is ignored by torch.linalg.det?",
        "Y": "ifNone",
        "Z": "Note This function is computed usingtorch.lu().",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the default value of torch.lu()?",
        "Y": "Default",
        "Z": "This function is computed usingtorch.lu().",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is ignored by torch.lu()?",
        "Y": "ifNone",
        "Z": "This function is computed usingtorch.lu().",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the tensor of shape(*, n, n)where*is zero or more batch dimensions?",
        "Y": "A(Tensor)",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "Out(tuple,optional) \u2013 what?",
        "Y": "output tuple of two tensors",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "Ignored what if a tuple of two tensors is output tuple of two tensor",
        "Y": "ifNone",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the default value for a tuple of two tensors?",
        "Y": "Default",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What will always be real-valued, even whenAis complex?",
        "Y": "logabsdet",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What will have the same dtype asA?",
        "Y": "sign",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is an example of a real-valued tuple?",
        "Y": "Examples",
        "Z": "Note The determinant can be recovered assign * exp(logabsdet). Note When a matrix has a determinant of zero, it returns(0, -inf). See also torch.linalg.det()computes the determinant of square matrices. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions. out(tuple,optional) \u2013 output tuple of two tensors. Ignored ifNone. Default",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the tensor of sizeendstartstepleftlceil fractextend -",
        "Y": "1-D",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the scale of dtype and torch.complex128 condition(BoolTensor) \u2013 When True (",
        "Y": "x",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What function returns a tensor of shape equal to the broadcasted shape of condition,x,y?",
        "Y": "alsotorch.nonzero()",
        "Z": "Return a tensor of elements selected from eitherxory, depending oncondition. The operation is defined as",
        "source": "https//pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "What is returned when the boundaries of the buckets are set byboundaries?",
        "Y": "the indices of the buckets to which each value in theinputbelongs",
        "Z": "Returns the indices of the buckets to which each value in theinputbelongs, where the",
        "source": "https//pytorch.org/docs/stable/generated/torch.randint_like.html#torch.randint_like"
    },
    {
        "X": "What features does PyTorch provide for working with C++?",
        "Y": "several features",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "What allows PyTorch models defined in Python to be serialized and then loaded and run in C++?",
        "Y": "TorchScript",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What language can you define your models in?",
        "Y": "Python",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What API is used to construct the input and do preprocessing?",
        "Y": "C++ Tensor API",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "In what language can you define your PyTorch models?",
        "Y": "Python",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What API does PyTorch use to do preprocessing?",
        "Y": "C++ Tensor API",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "In what language are PyTorch models defined?",
        "Y": "Python",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What language can TorchScript models be defined in?",
        "Y": "Python",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is used to construct the input and do preprocessing?",
        "Y": "C++ Tensor API",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What walks through interfacing TorchScript with OpenCV?",
        "Y": "TheExtending TorchScript with Custom C++ Operatorstutorial",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "C++ classes and structs can be bound into TorchScript through what type of interface?",
        "Y": "pybind11",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the tensor and autograd operations in PyTorch Python API?",
        "Y": "torch",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the full list of methods available in PyTorch?",
        "Y": "https",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is an example of a tensor and autograd operation in PyTorch?",
        "Y": "torch",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What can be bound into TorchScript through a pybind11-like interface?",
        "Y": "C++ classes and structs",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Most of the tensor and autograd operations in PyTorch Python API are also available in what?",
        "Y": "C++ API",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Most of the tensor and autograd operations in PyTorch Python API are also available in what language?",
        "Y": "C++",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find more details on the tensor autograd APIs and thetorch",
        "Y": "https",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What are some examples of Torch",
        "Y": "add/reshape/clone",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What script does authoring a neural net model need to be done in?",
        "Y": "TorchScript",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What API does the astorch",
        "Y": "Python",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "C++ tensor indexing API looks and behaves the same as what?",
        "Y": "Python API",
        "Z": "C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What script does the author infer in C++ use?",
        "Y": "TorchScript",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "In what language can a model be authored?",
        "Y": "C++",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is an example of a Python component that needs to be authored in C++?",
        "Y": "undesirable",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "On what system are two types of libtorch binaries provided?",
        "Y": "Linux",
        "Z": "For an overview of the PyTorch C++ model authoring and training API, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What can TorchScript be augmented with user-supplied code?",
        "Y": "custom operators and custom classes",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "From what languages can custom operators and classes be invoked in TorchScript code?",
        "Y": "Python or from C++",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "If thevaluesandindicestensors are the same size as input, what is the default?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Computes what decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices",
        "Y": "Cholesky",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "IfupperisTrue, the returned matrixUis what?",
        "Y": "upper-triangular",
        "Z": "Computes the Cholesky decomposition of a symmetric positive-definite",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the name of the matrixUis upper-triangular?",
        "Y": "IfupperisTrue",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "WhenupperisFalse, the returned tensor will be composed of what of each of the individual matrices?",
        "Y": "lower-triangular Cholesky factors",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "Which flag indicates whether to return a upper or lower triangular matrix?",
        "Y": "Default",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "IfupperisTrue, the returned tensor will be composed of what of each of the individual matrices?",
        "Y": "upper-triangular Cholesky factors",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "IfupperisTrue, the returned tensor will be composed of what?",
        "Y": "lower-triangular Cholesky factors",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What classification will these features be maintained long-term and there should generally be no major performance limitations or gaps in documentation?",
        "Y": "Stable",
        "Z": "Stable",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What is the name of the library that contains Thetorchtextpackage?",
        "Y": "Package Reference PyTorch Libraries",
        "Z": "Stable",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What is not specified?",
        "Y": "Iftolis",
        "Z": "Returns the numerical rank of a 2-D tensor. The method to compute the",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What operation is used to reduce a 3-D tensor?",
        "Y": "multiplication",
        "Z": "Moreover, as forgather(), the values ofindexmust be",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "Reducing with the addition operation is the same as what?",
        "Y": "usingscatter_add_()",
        "Z": "When indices are not unique, the behavior is non-deterministic (one of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What is the behavior of usetorch.arange() inconsistent with?",
        "Y": "Python\u2019s range builtin",
        "Z": "This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is the default value of the return tensor?",
        "Y": "ifNone",
        "Z": "This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What does the function infer the data type from the other input arguments?",
        "Y": "Ifdtypeis not given",
        "Z": "This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What function operates on all devices?",
        "Y": "fork_rng()",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "If you explicitly specify devices, this warning will be suppressed?",
        "Y": "enabled(bool)",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is a convenience argument for?",
        "Y": "disabling the context manager",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "Returns the initial seed for generating random numbers as a Pythonlong. Returns atorch.Generatorobject.",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "Returns the initial seed for generating random numbers as a Pythonlong. Returns atorch.Generatorobject. what?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What are remapped to positive values with the formula0xfff_fff_fff_ffff + seed",
        "Y": "Negative inputs",
        "Z": "devices(iterable of CUDA IDs) \u2013 CUDA devices for which to fork",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What does new_state(torch.ByteTensor) do?",
        "Y": "Sets the random number generator state",
        "Z": "enabled(bool) \u2013 ifFalse, the RNG is not forked.  This is a convenience",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What happens when using identical seeds?",
        "Y": "results may not be reproducible between CPU and GPU executions",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How can you avoid using nondeterministic algorithms for some operations?",
        "Y": "multiple calls to those operations, given the same inputs, will produce the same result",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What operations are often slower than nondeterministic operations?",
        "Y": "Warning Deterministic operations",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Are Deterministic operations faster or slower than nondeterministic operations?",
        "Y": "slower",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Deterministic operations are often what?",
        "Y": "slower",
        "Z": "Warning Deterministic operations are often slower than nondeterministic operations, so",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If you are using other libraries that use random number generators, refer to the documentation for those libraries to see how to do what for them?",
        "Y": "set consistent seeds",
        "Z": "You can usetorch.manual_seed()to seed the RNG for all devices (both",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What library can be a source of nondeterminism across multiple executions of an application?",
        "Y": "cuDNN library",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What can an optional feature run when a cuDNN convolution is called with a new set of size parameters?",
        "Y": "multiple convolution algorithms",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Due to benchmarking noise and different hardware, the benchmark may select what on subsequent runs?",
        "Y": "different algorithms",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the name of the benchmarking feature that cuDNN disables?",
        "Y": "withtorch.backends.cudnn.benchmark=Falsecauses",
        "Z": "If you are using any other libraries that use random number generators, refer to",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If you do not need reproducibility across multiple executions of your application, what might improve if the benchmarking feature is enabled?",
        "Y": "performance",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What setting is different from the benchmarking feature discussed below?",
        "Y": "thetorch.backends.cudnn.deterministicsetting",
        "Z": "Disabling the benchmarking feature withtorch.backends.cudnn.benchmark=Falsecauses cuDNN to deterministically select an algorithm, possibly at the cost of reduced",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What lets you configure PyTorch to use deterministic algorithms instead of nondeterministic ones?",
        "Y": "torch.use_deterministic_algorithms()",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If you do not need reproducibility across multiple executions of your application, performance might improve if what feature is enabled?",
        "Y": "benchmarking",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What documentation does PyTorch use for a full list of affected operations?",
        "Y": "fortorch.use_deterministic_algorithms()",
        "Z": "torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What type of implementation does PyTorch not have?",
        "Y": "determinism",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What does torch.use_deterministic_algorithms() let you configure PyTorch to use instead of nondeterministic ones?",
        "Y": "deterministic algorithms",
        "Z": "Note that this setting is different from thetorch.backends.cudnn.deterministicsetting discussed below. torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What implementation of oftorch.Tensor.index_add_() will throw an error?",
        "Y": "CUDA implementation",
        "Z": "Note that this setting is different from thetorch.backends.cudnn.deterministicsetting discussed below. torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What type of algorithms does torch.use_algorithms() let you configure PyTorch to use instead of nondeterministic ones?",
        "Y": "deterministic",
        "Z": "torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If an operation does not act correctly according to the documentation, or if you need a what implementation of an operation that does not have one, please",
        "Y": "deterministic",
        "Z": "torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the current version of CUDA?",
        "Y": "10.2 or greater",
        "Z": "Please check the documentation fortorch.use_deterministic_algorithms()for a full list of affected operations. If an operation does not act correctly",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How will other PyTorch operations behave?",
        "Y": "deterministically",
        "Z": "Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the argument for indices_or_sections?",
        "Y": "intorch.tensor_split()",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What are the parameters for a view of an existingtorch.Tensorinput?",
        "Y": "specifiedsize,strideandstorage_offset",
        "Z": "Random sampling creation ops are listed underRandom samplingand",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Where is aTensor created from?",
        "Y": "anumpy.ndarray",
        "Z": "Random sampling creation ops are listed underRandom samplingand",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor does anumpy.ndarray return?",
        "Y": "a tensor filled with the scalar value0",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the return value of a tensor filled with the scalar value0?",
        "Y": "a tensor filled with the scalar value0, with the same size asinput",
        "Z": "Random sampling creation ops are listed underRandom samplingand",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the size of the scalar value in the tensor?",
        "Y": "the same size asinput",
        "Z": "Random sampling creation ops are listed underRandom samplingand",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the view of an existingtorch.Tensorinput have?",
        "Y": "specifiedsize,strideandstorage_offset",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of the tensor created from?",
        "Y": "anumpy.ndarray",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns a tensor filled with the scalar value1?",
        "Y": "a tensor filled with the scalar value1, with the same size asinput",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size asinput.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What size is the tensor of sizeendstartstep?",
        "Y": "1-D",
        "Z": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor is created when values are evenly spaced fromstarttoend?",
        "Y": "one-dimensional",
        "Z": "Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the tensor with ones on the diagonal and zeros elsewhere?",
        "Y": "2-D tensor",
        "Z": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when a tensor is filled with uninitialized data?",
        "Y": "an uninitialized tensor with the same size asinput",
        "Z": "Returns a tensor filled with the scalar value1, with the same size asinput.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor returns ones on the diagonal and zeros elsewhere?",
        "Y": "2-D tensor",
        "Z": "Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning fromstart.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230astepend\u2212start\u200b\u230b+1with values fromstarttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor is returned with ones on the diagonal and zeros elsewhere?",
        "Y": "2-D",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns an uninitialized tensor?",
        "Y": "an uninitialized tensor with the same size asinput",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the tensor of?",
        "Y": "sizesizefilled withfill_value",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns a tensor of sizesizefilled withfill_value?",
        "Y": "a tensor with the same size asinputfilled withfill_value",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Where are the values of the one-dimensional tensor of sizesteps evenly spaced?",
        "Y": "fromstarttoend",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of the logarithmic scale used to create a one-dimensional tensor of sizesteps?",
        "Y": "basebase",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced fromstarttoend, inclusive.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned by dequantizing a quantized Tensor?",
        "Y": "fp32 Tensor",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor is converted to a quantized tensor with given scale and zero point?",
        "Y": "float",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly spaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale with basebase.   Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.   Returns a tensor filled with uninitialized data.   Returns an uninitialized tensor with the same size asinput.   Returns a tensor filled with uninitialized data.   Creates a tensor of sizesizefilled withfill_value.   Returns a tensor with the same size asinputfilled withfill_value.   Converts a float tensor to a quantized tensor with given scale and zero point.   Converts a float tensor to a per-channel quantized tensor with given scales and zero points.   Returns an fp32 Tensor by dequantizing a quantized Tensor   Constructs a complex tensor with its real part equal torealand its imaginary part equal toimag.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does it do to a given sequence ofseqtensors in a given dimension?",
        "Y": "Concatenates the given sequence ofseqtensors in the given dimension",
        "Z": "Concatenates the given sequence ofseqtensors in the given dimension.   Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What splits a tensor into multiple tensors depthwise according toindices_or_sections?",
        "Y": "Splitsinput",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "How is a new tensor created?",
        "Y": "horizontally stacking the tensors intensors",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_section",
        "Y": "Splitsinput",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the index of a new tensor that indexes theinputtensor along dimensiondimusing the entries inindex",
        "Y": "aLongTensor",
        "Z": "Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the new tensor that returns a new tensor that is a narrowed version ofinputtensor",
        "Y": "narrowed version",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does it do when a tensor is split into chunks?",
        "Y": "Gathers values along an axis specified bydim",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is a splitsinput?",
        "Y": "a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections",
        "Z": "Splits a tensor into a specific number of chunks.   Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "How does Alias oftorch create a new tensor?",
        "Y": "horizontally stacking",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "What happens to the dimension(s) ofinput?",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor is returned by Alias fortorch.movedim()?",
        "Y": "narrowed version",
        "Z": "Splitsinput, a tensor with three or more dimensions, into multiple tensors depthwise according toindices_or_sections.   Creates a new tensor by horizontally stacking the tensors intensors.   Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the Out-of-place version oftorch.Tensor.scatter_()?",
        "Y": "Out-of-place version oftorch.Tensor.scatter_()",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Where do tensors stack depthwise?",
        "Y": "third axis",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Splits the tensor into what?",
        "Y": "chunks",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of a tensor that splits a tensor into chunks?",
        "Y": "Stack",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is another name for tensors in sequence horizontally?",
        "Y": "Stack",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the Alias fortorch.transpose() do?",
        "Y": "Concatenates a sequence of tensors along a new dimension",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the index of the new tensor?",
        "Y": "aLongTensor",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the function that returns a sequence of tensors along a new dimension?",
        "Y": "Concatenates a sequence of tensors along a new dimension",
        "Z": "Returns a new tensor which indexes theinputtensor along dimensiondimusing the entries inindexwhich is aLongTensor.   Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the aBoolTensor do?",
        "Y": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What dimensions does the Alias fortorch.transpose() transpose?",
        "Y": "0 and 1",
        "Z": "Returns a new 1-D tensor which indexes theinputtensor according to the boolean maskmaskwhich is aBoolTensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does return a new tensor with the elements ofinputat the given indices?",
        "Y": "a new tensor with the elements ofinputat the given indices",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of indices are selected?",
        "Y": "1-dimensional indices",
        "Z": "Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of indices are used to select values from?",
        "Y": "1-dimensional indices",
        "Z": "Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What are all of the sub-tensors of a tensor?",
        "Y": "views ofinput",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of the function that converts a tensor into a new dimension?",
        "Y": "Alias fortorch.transpose()",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What are the dimensions of the tensor?",
        "Y": "0 and 1",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does return a tensor with the elements ofinputat the given indices?",
        "Y": "a new tensor with the elements ofinputat the given indices",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns a tensor that is what version of input?",
        "Y": "transposed",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What happens when a tensor dimension is removed?",
        "Y": "Removes a tensor dimension",
        "Z": "Returns a tensor with the same data and number of elements asinput, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.   Splits a tensor into multiple sub-tensors, all of which are views ofinput, along dimensiondimaccording to the indices or number of sections specified byindices_or_sections.   Constructs a tensor by repeating the elements ofinput.   Returns a tensor that is a transposed version ofinput.   Removes a tensor dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned whose mean and standard deviation are given?",
        "Y": "a tensor of random numbers drawn from separate normal distributions",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as atorch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns a tensor filled with random integers generated how?",
        "Y": "uniformly",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the random number generator do?",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "Z": "Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the function that returns a tensor where each row containsnum_samplesindices sampled from the multinomi",
        "Y": "Draws binary random numbers (0 or 1) from a Bernoulli distribution",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor is generated uniformly betweenlow(inclusive) andhigh(exclusive)?",
        "Y": "tensor filled with random integers",
        "Z": "Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is a tensor with the same size asinput?",
        "Y": "a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)",
        "Z": "Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when a tensor is filled with random integers?",
        "Y": "a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive)",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with what?",
        "Y": "mean 0 and variance 1.",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns a tensor with the same size asinput?",
        "Y": "random permutation of integers from0ton-1",
        "Z": "Returns a tensor of the same size asinputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of distribution is a tensor filled with random numbers from?",
        "Y": "uniform distribution",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the normal distribution of a tensor with the same size as input?",
        "Y": "mean 0 and variance 1.",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when a tensor is filled with random numbers from a normal distribution?",
        "Y": "random permutation of integers from0ton-1",
        "Z": "Returns a tensor with the same size asinputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What are a few more in-place random sampling functions defined on?",
        "Y": "Tensors",
        "Z": "Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What are some in-place random sampling functions defined on?",
        "Y": "Tensors",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of element is drawn from the geometric distribution?",
        "Y": "geometric",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of the function that returns a tensor with the same size as input?",
        "Y": "in-place version oftorch.normal()",
        "Z": "Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) andhigh(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned by a tensor with the same size as input?",
        "Y": "random permutation of integers from0ton-1",
        "Z": "Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size asinputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.acosh() multiply the result by?",
        "Y": "scalarvalue",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.asin() return?",
        "Y": "inverse hyperbolic sine",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What Computes the inverse cosine of each element ininput?",
        "Y": "Alias fortorch.abs()",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the element-wise multiplication oftensor1bytensor2 multiply the result by?",
        "Y": "scalarvalue",
        "Z": "Alias fortorch.abs()   Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.acosh do?",
        "Y": "Adds the scalarotherto each element of the inputinputand returns a new resulting tensor",
        "Z": "Computes the inverse cosine of each element ininput.   Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.acos() multiply the result by?",
        "Y": "scalarvalue",
        "Z": "Alias fortorch.acos().   Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor does Alias fortorch.acosh() return?",
        "Y": "inverse hyperbolic cosine",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the inverse hyperbolic cosine of the elements of input?",
        "Y": "arcsine",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput.   Alias fortorch.acosh().   Adds the scalarotherto each element of the inputinputand returns a new resulting tensor.   Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Computes the element-wise angle of the given inputtensor in what units?",
        "Y": "radians",
        "Z": "Performs the element-wise division oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.atan() do?",
        "Y": "Computes the bitwise AND ofinputandother",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.atan() compute?",
        "Y": "bitwise OR ofinputandother",
        "Z": "Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the bitwise of the given input tensor?",
        "Y": "XOR ofinputandother",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "In what unit is the element-wise angle of the given inputtensor?",
        "Y": "radians",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does it do to the bitwise NOT of the given input tensor?",
        "Y": "Computes the bitwise AND ofinputandother",
        "Z": "Performs the element-wise multiplication oftensor1bytensor2, multiply the result by the scalarvalueand add it toinput.   Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the element-wise angle of inputi/otheri?",
        "Y": "arctangent",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor does Alias fortorch.clamp() compute?",
        "Y": "bitwise NOT",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the bitwise of inputandother?",
        "Y": "XOR",
        "Z": "Computes the element-wise angle (in radians) of the giveninputtensor.   Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tangent does Alias fortorch.atan() return?",
        "Y": "hyperbolic",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.asin() do?",
        "Y": "Computes the bitwise AND ofinputandother",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.asin() compute?",
        "Y": "bitwise OR ofinputandother",
        "Z": "Returns a new tensor with the arcsine  of the elements ofinput.   Alias fortorch.asin().   Returns a new tensor with the inverse hyperbolic sine of the elements ofinput.   Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the bitwise ofinputandother?",
        "Y": "XOR",
        "Z": "Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the element-wise ofinputi/otheri?",
        "Y": "arctangent",
        "Z": "Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.asinh() compute?",
        "Y": "bitwise OR ofinputandother",
        "Z": "Alias fortorch.asinh().   Returns a new tensor with the arctangent  of the elements ofinput.   Alias fortorch.atan().   Returns a new tensor with the inverse hyperbolic tangent of the elements ofinput.   Alias fortorch.atanh().   Element-wise arctangent ofinputi/otheri\\text{input}_{i} / \\text{other}_{i}inputi\u200b/otheri\u200bwith consideration of the quadrant.   Computes the bitwise NOT of the given input tensor.   Computes the bitwise AND ofinputandother.   Computes the bitwise OR ofinputandother.   Computes the bitwise XOR ofinputandother.   Returns a new tensor with the ceil of the elements ofinput, the smallest integer greater than or equal to each element.   Clamps all elements ininputinto the range[min,max].   Alias fortorch.clamp().   Computes the element-wise conjugate of the giveninputtensor.   Create a new floating-point tensor with the magnitude ofinputand the sign ofother, elementwise.   Returns a new tensor with the cosine  of the elements ofinput.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when the input tensor is tested toTrue?",
        "Y": "the minimum value of each slice of theinputtensor in the given dimension(s)dim",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does return the maximum value of all elements in theinputtensor?",
        "Y": "the minimum value of all elements in theinputtensor",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim. Returns what?",
        "Y": "p-norm",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the log of summed exponentials of each row of theinputtensor return?",
        "Y": "the mean value of all elements in theinputtensor",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does return the mean value of all elements in theinputtensor?",
        "Y": "the median of the values ininput",
        "Z": "Returns the indices of the maximum value of all elements in theinputtensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when the log of summed exponentials of each row of theinputtensor is returned?",
        "Y": "the mean value of all elements in theinputtensor",
        "Z": "Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does return ignoringNaNvalues?",
        "Y": "the median of the values ininput",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns the mode value of each row of theinputtensor in the given dimensiondim?",
        "Y": "matrix norm or vector norm",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns what of all elements in the inputtensor?",
        "Y": "product",
        "Z": "Returns the maximum value of each slice of theinputtensor in the given dimension(s)dim.   Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when the log of summed exponentials of each row of theinputtensor in the given dimensiondim is returned?",
        "Y": "the mean value of all elements in theinputtensor",
        "Z": "Returns the minimum value of each slice of theinputtensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when the sum of all elements is returned?",
        "Y": "the product of all elements in theinputtensor",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What quantiles of each row of theinputtensor along the dimensiondim is computed?",
        "Y": "q-th",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when calculating the p-norm of (input-other)?",
        "Y": "the mean value of all elements in theinputtensor",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when the input tensor is not used?",
        "Y": "the median of the values ininput",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does this return, ignoringNaNvalues?",
        "Y": "the median of the values ininput",
        "Z": "the input tensor.   Returns the maximum value of all elements in theinputtensor.   Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned when calculating the q-th quantiles of each row of theinputtensor along the dimensiondim?",
        "Y": "the product of all elements in theinputtensor",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the variant of oftorch.quantile() do?",
        "Y": "ignores",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the function ignoreNaNvalues?",
        "Y": "the median of the values ininput",
        "Z": "Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Whose correction will be used if unbiasedisTrue?",
        "Y": "Bessel",
        "Z": "Returns the minimum value of all elements in theinputtensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is a variant of oftorch.quantile()?",
        "Y": "Computes the q-th quantiles of each row of theinputtensor along the dimensiondim",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of theinputtensor in the given dimensiondim.   Returns the mean value of all elements in theinputtensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Whose correction will be used to calculate the standard deviation if unbiasedisTrue?",
        "Y": "Bessel",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does not returnNaNvalues?",
        "Y": "the median of the values ininput",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What will be used to calculate the standard deviation if unbiasedisTrue?",
        "Y": "Bessel\u2019s correction",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Bessel's correction do?",
        "Y": "Eliminates all but the first element from every consecutive group of equivalent elements",
        "Z": "Returns the median of the values ininput, ignoringNaNvalues.   Returns a namedtuple(values,indices)wherevaluesis the mode value of each row of theinputtensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in theinputtensor.   Computes the q-th quantiles of each row of theinputtensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   IfunbiasedisTrue, Bessel\u2019s correction will be used.   IfunbiasedisTrue, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in theinputtensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   IfunbiasedisTrue, Bessel\u2019s correction will be used.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What sort a tensor along a given dimension in ascending order by value?",
        "Y": "indices",
        "Z": "This function checks if allinputandothersatisfy the condition",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What element represents if each element ofinputis \u201cclose\u201d to the corresponding element ofother?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What condition does this function check if each element ofinputis?",
        "Y": "infinite",
        "Z": "This function checks if allinputandothersatisfy the condition",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does this function do if each element ofinputis negative infinity or not?",
        "Y": "Tests",
        "Z": "This function checks if allinputandothersatisfy the condition",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns a new tensor with what representation if each element ofinputis NaN or not?",
        "Y": "boolean elements",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of the function that computes the indices that sort a tensor along a given dimension?",
        "Y": "Alias fortorch.ge()",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What function returns a new tensor with boolean elements if each element ofinputis \u201cclose\u201d to the corresponding",
        "Y": "Alias fortorch.gt()",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Is each element ofinput infinite or negative infinity?",
        "Y": "infinite",
        "Z": "Returns the indices that sort a tensor along a given dimension in ascending order by value.   Computes element-wise equality   Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does valuesis thekth smallest element of each row of theinputtensor in the given dimensiondim?",
        "Y": "a namedtuple(values,indices)",
        "Z": "Trueif two tensors have the same size and elements,Falseotherwise.   Computesinput\u2265other\\text{input} \\geq \\text{other}input\u2265otherelement-wise.   Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What function computes input>othertextinput > textotherinput>otherelement-wise?",
        "Y": "Alias fortorch.ge()",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the new tensor return if each element isfiniteor not?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.le() test if each element ofinputis?",
        "Y": "positive infinity",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.le() do if each element ofinputis negative infinity or not?",
        "Y": "Tests",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What elements represent if each element ofinputis real-valued or not?",
        "Y": "boolean elements",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is returned whenvaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim?",
        "Y": "a namedtuple(values,indices)",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does Alias fortorch.le() do?",
        "Y": "Alias fortorch.le()",
        "Z": "Alias fortorch.ge().   Computesinput>other\\text{input} > \\text{other}input>otherelement-wise.   Alias fortorch.gt().   Returns a new tensor with boolean elements representing if each element ofinputis \u201cclose\u201d to the corresponding element ofother.   Returns a new tensor with boolean elements representing if each element isfiniteor not.   Tests if each element ofinputis infinite (positive or negative infinity) or not.   Tests if each element ofinputis positive infinity or not.   Tests if each element ofinputis negative infinity or not.   Returns a new tensor with boolean elements representing if each element ofinputis NaN or not.   Returns a new tensor with boolean elements representing if each element ofinputis real-valued or not.   Returns a namedtuple(values,indices)wherevaluesis thekth smallest element of each row of theinputtensor in the given dimensiondim.   Computesinput\u2264other\\text{input} \\leq \\text{other}input\u2264otherelement-wise.   Alias fortorch.le().",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "Returns what of vectors in dimensiondimofinputandother?",
        "Y": "cross product",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the cross product of elements ofinputin the dimensiondim?",
        "Y": "cumulative product",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is computed along the given dimension?",
        "Y": "n-th forward difference",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a namedtuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a namedtuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    Ifinputis a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What reduces the number of steps required to perform a batch matrix-matrix product of matrices stored inbatch1andb",
        "Y": "add step",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the matrix-vector product of vectorsvec1andvec2 perform?",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the batch matrix-matrix product of?",
        "Y": "matrices stored ininputandmat2",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What decomposition is computed for batches of symmetric positive-definite matrices?",
        "Y": "Cholesky",
        "Z": "Performs a batch matrix-matrix product of matrices stored inbatch1andbatch2, with a reduced add step (all matrix multiplications get accumulated along the first dimension).   Performs a matrix multiplication of the matricesmat1andmat2.   Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What product is added to the matrixinput?",
        "Y": "outer-product of vectorsvec1andvec2",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What performs a batch matrix-matrix product of matrices inbatch1 andbatch2?",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What decomposition of a symmetric positive-definite matrix is computed for batches of symmetric positive-definite matrices?",
        "Y": "Cholesky",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the function do when it computes the inverse of a symmetric positive-definite matrix?",
        "Y": "returns matrixinv",
        "Z": "Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What performs a matrix-vector product of?",
        "Y": "matrixmatand the vectorvec",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does it perform?",
        "Y": "batch matrix-matrix product of matrices inbatch1andbatch2",
        "Z": "Performs a matrix-vector product of the matrixmatand the vectorvec.   Performs the outer-product of vectorsvec1andvec2and adds it to the matrixinput.   Performs a batch matrix-matrix product of matrices inbatch1andbatch2.   Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns the Cholesky factoruuu?",
        "Y": "returns matrixinv",
        "Z": "Performs a batch matrix-matrix product of matrices stored ininputandmat2.   Returns the matrix product of theNNN2-D tensors.   Computes the Cholesky decomposition of a symmetric positive-definite matrixAAAor for batches of symmetric positive-definite matrices.   Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does the function compute of a real square matrix?",
        "Y": "eigenvalues and eigenvectors",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What does it compute of a matrix or batches of matricesA?",
        "Y": "LU factorization",
        "Z": "Computes the inverse of a symmetric positive-definite matrixAAAusing its Cholesky factoruuu",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What returns the LU solve of the linear systemAx=bAx = bAx=busing the partially pivoted?",
        "Y": "LU factorization of A fromtorch.lu()",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the ar systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the dot product of two tensors?",
        "Y": "Matrix product of two tensors",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the ar systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What is the name of the function that calculates the dot product of two tensors?",
        "Y": "Alias fortorch.linalg.matrix_power()",
        "Z": "Computes the dot product of two 1D tensors.   Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the ar systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What type of tensor does Alias fortorch.linalg.matrix_power() return?",
        "Y": "2-D",
        "Z": "Computes the eigenvalues and eigenvectors of a real square matrix.   This is a low-level function for calling LAPACK\u2019s geqrf directly.   Alias oftorch.outer().   Computes the dot product for 1D tensors.   Alias fortorch.linalg.inv()   Alias fortorch.linalg.det()   Calculates log determinant of a square matrix or batches of square matrices.   Alias fortorch.linalg.slogdet()   Computes the solution to the least squares and least norm problems for a full rank matrixAAAof size(m\u00d7n)(m \\times n)(m\u00d7n)and a matrixBBBof size(m\u00d7k)(m \\times k)(m\u00d7k).   Computes the LU factorization of a matrix or batches of matricesA.   Returns the LU solve of the ar systemAx=bAx = bAx=busing the partially pivoted LU factorization of A fromtorch.lu().   Unpacks the data and pivots from a LU factorization of a tensor into tensorsLandUand a permutation tensorPsuch thatLU_data,LU_pivots=(P@L@U).lu().   Matrix product of two tensors.   Alias fortorch.linalg.matrix_power()   Returns the numerical rank of a 2-D tensor.",
        "source": "https//pytorch.org/docs/stable/torch.html#random-sampling"
    },
    {
        "X": "What language does TorchScript allow PyTorch models defined in Python to be serialized and then loaded and run in?",
        "Y": "C++",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What can you do with models exported via TorchScript?",
        "Y": "no-Python execution",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the TorchScript C++ API used for?",
        "Y": "Loading serialized TorchScript models saved from Python",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is one of the ways you can interact with TorchScript?",
        "Y": "Constructing the input and doing preprocessing using C++ Tensor API",
        "Z": "If you are looking for the PyTorch C++ API docs, directly gohere. PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What language does TorchScript allow PyTorch models to be serialized and then loaded and run in?",
        "Y": "C++",
        "Z": "PyTorch provides several features for working with C++, and it\u2019s best to choose from them based on your needs. At a high level, the following support is available",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does TorchScript allow you to do in production or embedded environments?",
        "Y": "no-Python execution",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is one of the ways you can interact with the TorchScript execution engine?",
        "Y": "Constructing the input and doing preprocessing using C++ Tensor API",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What language are PyTorch models defined in?",
        "Y": "Python",
        "Z": "TorchScriptallows PyTorch models defined in Python to be serialized and then loaded and run in C++ capturing the model code via compilation or tracing its execution. You can learn more in theLoading a TorchScript Model in C++ tutorial. This means you can define your models in Python as much as possible, but subsequently export them via TorchScript for doing no-Python execution in production or embedded environments. The TorchScript C++ API is used to interact with these models and the TorchScript execution engine, including",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What can TorchScript be augmented with user-supplied code through?",
        "Y": "custom operators and custom classes",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does TheExtending TorchScript with Custom C++ Operatorstutorial walk through interfacing TorchScript with?",
        "Y": "OpenCV",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What type of interface allows C++ classes and structs to be bound into TorchScript?",
        "Y": "pybind11",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where are most of the tensor and autograd operations available in the C++ API?",
        "Y": "PyTorch Python API",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is one of the tensor methods available in the C++ API?",
        "Y": "torch",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find the full list of tensor methods?",
        "Y": "https",
        "Z": "Doing simple model modifications if needed (e.g. pulling out submodules) Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find the full list of methods available?",
        "Y": "https",
        "Z": "Constructing the input and doing preprocessing using C++ Tensor API TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is an example of a tensormethod?",
        "Y": "torch",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "Where can you find the full list of methods available in TorchScript?",
        "Y": "https",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does the C++ tensor indexing API look and behave like?",
        "Y": "Python API",
        "Z": "Most of the tensor and autograd operations in PyTorch Python API are also available in the C++ API. These include",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the Torch method?",
        "Y": "torch",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What language does the author in TorchScript, infer in C++ workflow require the model to be authored in?",
        "Y": "C++",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is undesirable in a workflow where the model has to be authored in C++?",
        "Y": "a Python component",
        "Z": "torch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "In what language does TorchScript author and train a neural net model?",
        "Y": "C++",
        "Z": "C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does the \"author in\" workflow require model authoring to be done in?",
        "Y": "TorchScript",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is an example of a case where a model has to be authored in C++?",
        "Y": "in workflows where a Python component is undesirable",
        "Z": "C++ tensor indexing API that looks and behaves the same as the Python API. For details on its usage, please see",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is an overview of?",
        "Y": "PyTorch C++ model authoring and training API",
        "Z": "The tensor autograd APIs and thetorch",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What is the name of the API that you can find at http",
        "Y": "PyTorch C++ model authoring and training API",
        "Z": "The \u201cauthor in TorchScript, infer in C++\u201d workflow requires model authoring to be done in TorchScript.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "From what languages can custom operators and classes be invoked in TorchScript code run?",
        "Y": "Python or from C++",
        "Z": "TorchScript can be augmented with user-supplied code through custom operators and custom classes.",
        "source": "https//pytorch.org/docs/stable/cpp_index.html"
    },
    {
        "X": "What does values represent in the given dimensiondim?",
        "Y": "thekth smallest element of each row of theinputtensor",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch._assert.html#torch._assert"
    },
    {
        "X": "What does indexes return of each element found?",
        "Y": "index location",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "When are both thevaluesandindicestensors the same size as input?",
        "Y": "IfkeepdimisTrue",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "Wheninputis a what, this function may nondeterministically returnindices for any of them?",
        "Y": "CUDA tensor",
        "Z": "Returns a namedtuple(values,indices)wherevaluesis thekth",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What returns the matrixUis upper-triangular and the decomposition has the form IfupperisFalse?",
        "Y": "IfupperisTrue",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "What is the input tensorAAAof size(,n,n)?",
        "Y": "input(Tensor)",
        "Z": "IfupperisFalse, the returned matrixLis lower-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is the default value for the return matrix?",
        "Y": "Default",
        "Z": "IfupperisTrue, the returned matrixUis upper-triangular, and",
        "source": "https//pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "What is tagged as Beta because the API may change based on user feedback?",
        "Y": "Beta",
        "Z": "Stable",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What classification does PyTorch commit to seeing Beta features through?",
        "Y": "Stable classification",
        "Z": "This library is part of thePyTorchproject. PyTorch is an open source",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What type of feature is tagged as because the API may change based on user feedback?",
        "Y": "Beta",
        "Z": "Features described in this documentation are classified by release status",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What classification do we commit to seeing a feature through for Beta features?",
        "Y": "the Stable classification",
        "Z": "Features described in this documentation are classified by release status",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "Prototype features are typically not available as part of binary distributions like what?",
        "Y": "PyPI or Conda",
        "Z": "Features described in this documentation are classified by release status",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What is the classification of features that will be maintained long-term?",
        "Y": "Stable",
        "Z": "Stable",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "What is a package reference?",
        "Y": "PyTorch Libraries",
        "Z": "Stable",
        "source": "https//pytorch.org/text/stable"
    },
    {
        "X": "Computes the entropy oninput?",
        "Y": "elementwise",
        "Z": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "Is this module in BETA?",
        "Y": "New functions are still being added",
        "Z": "This module is in BETA. New functions are still being added, and some",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the name of the documentation of each function that may change in future PyTorch releases?",
        "Y": "the documentation of each function for details",
        "Z": "This module is in BETA. New functions are still being added, and some",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the name of the function that computes the error function of input?",
        "Y": "Computes the error function ofinput",
        "Z": "This module is in BETA. New functions are still being added, and some",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the exponential function of input?",
        "Y": "base two",
        "Z": "Computes the complementary error function ofinput.",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the complementary error function?",
        "Y": "complementary error function ofinput",
        "Z": "Computes the complementary error function ofinput.",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "Computes the natural of what of the absolute value of the gamma function oninput?",
        "Y": "logarithm",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What function provides greater precision for small values of x?",
        "Y": "exp(x) - 1",
        "Z": "This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What natural function is computed of the absolute value of the gamma function oninput?",
        "Y": "logarithm",
        "Z": "Example",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is an example of a function that computes the base two exponential function of the gamma function oninput?",
        "Y": "Computesinput*log1p(other)with the following cases",
        "Z": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is an example of a Computesinput*log1p?",
        "Y": "Computesinput*log1p(other)with the following cases",
        "Z": "Example",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What does input(NumberorTensor) represent?",
        "Y": "Multiplier",
        "Z": "Example",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the reverse operation of the manner described ingather()?",
        "Y": "Writes all values from the tensorsrcintoselfat the indices specified in theindextensor",
        "Z": "Writes all values from the tensorsrcintoselfat the indices",
        "source": "https//pytorch.org/docs/stable/special.html#torch.special.exp2"
    },
    {
        "X": "What is the reverse operation of the manner described?",
        "Y": "ingather()",
        "Z": "Writes all values from the tensorsrcintoselfat the indices",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What should all have the same number of dimensions?",
        "Y": "self,indexandsrc",
        "Z": "This is the reverse operation of the manner described ingather(). self,indexandsrc(if it is a Tensor) should all have",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "Does indexandsrcdo broadcast?",
        "Y": "not broadcast",
        "Z": "For a 3-D tensor,selfis updated as",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What happens when indices are not unique?",
        "Y": "the gradient will be incorrect",
        "Z": "This is the reverse operation of the manner described ingather(). self,indexandsrc(if it is a Tensor) should all have",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What is the manner described ingather()?",
        "Y": "reverse operation",
        "Z": "This is the reverse operation of the manner described ingather(). self,indexandsrc(if it is a Tensor) should all have",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What does indexandsrcdo do?",
        "Y": "not broadcast",
        "Z": "This is the reverse operation of the manner described ingather(). self,indexandsrc(if it is a Tensor) should all have",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What allows specification of an optional reduction operation?",
        "Y": "optionalreduceargument",
        "Z": "Moreover, as forgather(), the values ofindexmust be",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What is the index inself specified by?",
        "Y": "index insrcfordimension!=dimand by the corresponding value inindexfordimension=dim",
        "Z": "Moreover, as forgather(), the values ofindexmust be",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What is the tensor?",
        "Y": "3-D",
        "Z": "Moreover, as forgather(), the values ofindexmust be",
        "source": "https//pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "What language's range builtin is inconsistent with this function?",
        "Y": "Python",
        "Z": "This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "What is the default for the layout of returned Tensor?",
        "Y": "Default",
        "Z": "Warning This function is deprecated and will be removed in a future release because its behavior is inconsistent with",
        "source": "https//pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "If you explicitly specify devices, this warning will be suppressed what?",
        "Y": "enabled(bool)",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What is the reason for disabling the context manager without having to delete it and unindent your Python code under it?",
        "Y": "convenience argument",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What does fork_rng() do?",
        "Y": "Sets the seed for generating random numbers",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "What does fork_rng() return?",
        "Y": "atorch.Generatorobject",
        "Z": "Forks the RNG, so that when you return, the RNG is reset",
        "source": "https//pytorch.org/docs/stable/random.html"
    },
    {
        "X": "Results may not be reproducible between what two executions?",
        "Y": "CPU and GPU executions",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What can you do to ensure reproducible results across platforms, devices, and PyTorch releases?",
        "Y": "limit the number of sources of nondeterministic behavior",
        "Z": "Completely reproducible results are not guaranteed across PyTorch releases,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What are often slower than nondeterministic operations?",
        "Y": "Warning Deterministic operations",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What can you configure to avoid using nondeterministic algorithms for some operations?",
        "Y": "PyTorch",
        "Z": "However, there are some steps you can take to limit the number of sources of",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What do some applications and libraries need to be seeded consistently?",
        "Y": "NumPy Random Generator objects",
        "Z": "For custom operators, you might need to set python seed as well",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Where can you find information about other libraries that use random number generators?",
        "Y": "the documentation",
        "Z": "For custom operators, you might need to set python seed as well",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the cuDNN library used for?",
        "Y": "CUDA convolution operations",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "When a cuDNN convolution is called with a new set of size parameters, what can run multiple convolution algorithms?",
        "Y": "an optional feature",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What causes a benchmark to select different algorithms on subsequent runs?",
        "Y": "benchmarking noise and different hardware",
        "Z": "However, some applications and libraries may use NumPy Random Generator objects,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What happens when a benchmark is used on the same machine?",
        "Y": "benchmark may select different algorithms on subsequent runs",
        "Z": "If you are using any other libraries that use random number generators, refer to",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Why is benchmarking noise and different hardware important?",
        "Y": "benchmark may select different algorithms on subsequent runs, even on the same machine",
        "Z": "The cuDNN library, used by CUDA convolution operations, can be a source of nondeterminism",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What might happen if the benchmarking feature is enabled withtorch.backends.cudnn.benchmark=True",
        "Y": "performance might improve",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What is the difference between the benchmarking feature and the other setting discussed below?",
        "Y": "different from thetorch.backends.cudnn.deterministicsetting",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What does torch.use_deterministic_algorithms allow PyTorch to use instead of nondeterministic ones?",
        "Y": "deterministic algorithms",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If an operation does not act correctly according to the documentation, or if you need what, please submit an issue",
        "Y": "a deterministic implementation of an operation that does not have one",
        "Z": "However, if you do not need reproducibility across multiple executions of your application,",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "If an operation does not act correctly according to the documentation, or if you need what, please submit an issue?",
        "Y": "a deterministic implementation of an operation that does not have one",
        "Z": "Note that this setting is different from thetorch.backends.cudnn.deterministicsetting discussed below. torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "When will the alternative deterministic implementation of bmm() be used?",
        "Y": "when the deterministic flag is turned on",
        "Z": "torch.use_deterministic_algorithms()lets you configure PyTorch to use",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "Whentorch.bmm() is called with sparse-dense CUDA tensors, it typically uses a",
        "Y": "deterministic",
        "Z": "Whentorch.bmm()is called with sparse-dense CUDA tensors it typically uses a",
        "source": "https//pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "What does torch.linalg.det() compute?",
        "Y": "the sign and natural logarithm of the absolute value of the determinant of a square matrix",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "What types of inputs does this function support?",
        "Y": "float, double, cfloat and cdouble dtypes",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is ignored?",
        "Y": "ifNone",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the default value of the function that computes the sign and logarithm of the absolute value of the determinant of a square",
        "Y": "Default",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What is the name of the function that computes the sign and logarithm of the absolute value of the determinant of a square matrix",
        "Y": "tuple",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What does logabsdet always be?",
        "Y": "real-valued",
        "Z": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix. For complexA, it returns the angle and the natural logarithm of the modulus of the",
        "source": "https//pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "What does the PyTorch Timer do when necessary?",
        "Y": "synchronize asynchronous CUDA functions",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Timer do when necessary?",
        "Y": "synchronize asynchronous CUDA functions",
        "Z": "Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Timer perform?",
        "Y": "warmups, set threadpool, and synchronize asynchronous CUDA functions when\nnecessary.",
        "Z": "Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can one do when defining a Timer?",
        "Y": "optionally specify label,sub_label,description, andenv",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Timer can run a statement under what?",
        "Y": "Callgrind",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What fields are included in the representation of result object and by theCompareclass to group and display results for comparison?",
        "Y": "optionally specifylabel,sub_label,description, and env",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are the fields included in the representation of result object and by theCompareclass for comparison?",
        "Y": "specify label, sub_label, description, and env.",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a Timer specific constructor?",
        "Y": "PyTorch",
        "Z": "When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is setup?",
        "Y": "Optional setup code",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "In what language is global_setup only used?",
        "Y": "C++",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "In addition to wall times, Timer can run a statement under what?",
        "Y": "Callgrind",
        "Z": "In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What are the PyTorch Timer specific constructor arguments?",
        "Y": "label,sub_label,description,env,num_threads",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the optional setup code used to define variables?",
        "Y": "instmt global_setup",
        "Z": "In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the timer constructor arguments?",
        "Y": "stmt",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to define variables?",
        "Y": "in stmt global_setup",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What language is global_setup only used in?",
        "Y": "C++",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the PyTorch Timer specific constructor arguments?",
        "Y": "stmt",
        "Z": "stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the optional setup code used to define variables?",
        "Y": "in stmt global_setup",
        "Z": "Helper class for measuring execution time of PyTorch statements. For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What specific constructor arguments do labels,sub_label,description,env,num_threads, and num_thread",
        "Y": "PyTorch Timer",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is placed at the top level of the file for things like#includestatements?",
        "Y": "global_setup",
        "Z": "global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the specific constructor arguments?",
        "Y": "PyTorch Timer",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What type of setup code is used to define variables used instmt global_setup?",
        "Y": "Optional setup code",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What type of timer returns the current time?",
        "Y": "Callable",
        "Z": "For a full tutorial on how to use this class, see:https://pytorch.org/tutorials/recipes/recipes/benchmark.html The PyTorch Timer is based ontimeit.Timer(and in fact usestimeit.Timerinternally), but with several key differences: Timer will perform warmups (important as some elements of PyTorch are\nlazily initialized), set threadpool size so that comparisons are\napples-to-apples, and synchronize asynchronous CUDA functions when\nnecessary. When measuring code, and particularly complex kernels / models,\nrun-to-run variation is a significant confounding factor. It is\nexpected that all measurements should include replicates to quantify\nnoise and allow median computation, which is more robust than mean.\nTo that effect, this class deviates from thetimeitAPI by\nconceptually mergingtimeit.Timer.repeatandtimeit.Timer.autorange.\n(Exact algorithms are discussed in method docstrings.) The timeit method is replicated for cases where an adaptive strategy is not\ndesired. When defining a Timer, one can optionally specifylabel,sub_label,description, andenv. (Defined later) These fields are included in\nthe representation of result object and by theCompareclass to group\nand display results for comparison. In addition to wall times, Timer can run a statement under Callgrind\nand report instructions executed. Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What was built without CUDA or there was no GPU present?",
        "Y": "PyTorch",
        "Z": "label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the code snippet to be run in a loop and timed?",
        "Y": "stmt",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the code used to define variables in PyTorch?",
        "Y": "in stmt global_setup",
        "Z": "setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does timer return?",
        "Y": "current time",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What happens if PyTorch was built without GPU?",
        "Y": "synchronize CUDA",
        "Z": "timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a code snippet to be run in a loop and timed?",
        "Y": "stmt",
        "Z": "stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the code that is placed at the top level of the file for things like#includestatements?",
        "Y": "global_setup",
        "Z": "stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What was built without CUDA or there is no GPU present?",
        "Y": "PyTorch",
        "Z": "setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "If PyTorch was built without what?",
        "Y": "CUDA",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What returns the current time?",
        "Y": "Callable",
        "Z": "setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to define variables used instmt global_setup?",
        "Y": "Optional setup code",
        "Z": "setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "If PyTorch was built without GPU present, what will it do?",
        "Y": "synchronize CUDA",
        "Z": "setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Code which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time.",
        "Y": "global_setup",
        "Z": "global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "If PyTorch was built without GPU present, what would it do?",
        "Y": "synchronize CUDA",
        "Z": "global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What defines the global variables whenstmtis being executed?",
        "Y": "globals",
        "Z": "globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is globals the other method for?",
        "Y": "providing variables which stmt needs",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a string that summarizes stmt?",
        "Y": "label",
        "Z": "stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why would one set label to \u201cReLU(x + 1)\u201d?",
        "Y": "improve readability",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the string that summarizes stmt?",
        "Y": "sub_label",
        "Z": "stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What type of label summarizes stmt?",
        "Y": "String",
        "Z": "label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What would one set label to to improve readability?",
        "Y": "ReLU(x + 1)",
        "Z": "label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does sub_label provide to disambiguate measurements with identical stmt or label?",
        "Y": "supplemental information",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is another name for a sub_label?",
        "Y": "int",
        "Z": "Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a String that summarizes stmt?",
        "Y": "label",
        "Z": "label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What provides supplemental information to disambiguate measurements with identical stmt or label?",
        "Y": "sub_label",
        "Z": "label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is an example of a sub_label?",
        "Y": "float",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a description of a stmt?",
        "Y": "description",
        "Z": "label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What type of description is used to disambiguate measurements?",
        "Y": "description",
        "Z": "sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is one way to disambiguate measurements with identical stmt or label?",
        "Y": "Provide supplemental information",
        "Z": "Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to disambiguate measurements when printing Measurements or summarizing usingCompare?",
        "Y": "description",
        "Z": "Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is provided to disambiguate measurements with identical stmt or label?",
        "Y": "supplemental information",
        "Z": "Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a supplemental information used to disambiguate measurements?",
        "Y": "description",
        "Z": "Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used when printing Measurements or summarizing usingCompare?",
        "Y": "ReLU(x + 1): (int)",
        "Z": "\u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to distinguish measurements with identical label and sub_label?",
        "Y": "String",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the principal use of description?",
        "Y": "to signal to Compare the columns of data",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What would one set description based on to create a table of the form?",
        "Y": "input size",
        "Z": "description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is description also included when printing?",
        "Y": "Measurement",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What might one set it based on to create a table of the form?",
        "Y": "the input size",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "When is description included?",
        "Y": "when printing a Measurement",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Description is included when printing a what?",
        "Y": "Measurement",
        "Z": "description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What would one set the description based on to create a table of the form?",
        "Y": "input size",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "When printing a document, what is the use ofdescription included in?",
        "Y": "Measurement",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "The principal use of descriptionis to signal what?",
        "Y": "to Compare the columns of data",
        "Z": "Directly analogous totimeit.Timerconstructor arguments: stmt,setup,timer,globals PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is included when printing a Measurement?",
        "Y": "Compare",
        "Z": "using Compare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is also included when printing a?",
        "Y": "Measurement",
        "Z": "usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does compare treat Measurements with when merging replicate runs?",
        "Y": "different env specification",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "When is usingCompare included?",
        "Y": "when printing a Measurement",
        "Z": "usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What tag indicates that otherwise identical tasks were run in different environments?",
        "Y": "env",
        "Z": "PyTorch Timer specific constructor arguments: label,sub_label,description,env,num_threads stmt\u2013 Code snippet to be run in a loop and timed. setup\u2013 Optional setup code. Used to define variables used instmt global_setup\u2013 (C++ only)\nCode which is placed at the top level of the file for things like#includestatements. timer\u2013 Callable which returns the current time. If PyTorch was built\nwithout CUDA or there is no GPU present, this defaults totimeit.default_timer; otherwise it will synchronize CUDA before\nmeasuring the time. globals\u2013 A dict which defines the global variables whenstmtis being\nexecuted. This is the other method for providing variables whichstmtneeds. label\u2013 String which summarizes stmt. For instance, ifstmtis\n\u201ctorch.nn.functional.relu(torch.add(x, 1, out=out))\u201d\none might set label to \u201cReLU(x + 1)\u201d to improve readability. sub_label\u2013 Provide supplemental information to disambiguate measurements\nwith identical stmt or label. For instance, in our example\nabove sub_label might be \u201cfloat\u201d or \u201cint\u201d, so that it is easy\nto differentiate:\n\u201cReLU(x + 1): (float)\u201d \u201dReLU(x + 1): (int)\u201d\nwhen printing Measurements or summarizing usingCompare. description\u2013 String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "env- This tag indicates that otherwise identical tasks were run in what environment?",
        "Y": "different",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the size of the PyTorch thread pool when executing stmt?",
        "Y": "num_threads",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is important as both a key inference workload and a good indicator of intrinsic algorithmic efficiency?",
        "Y": "Single threaded performace",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What tries to utilize all cores?",
        "Y": "PyTorch threadpool size",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does blocked_autorange do?",
        "Y": "Measure many replicates while keeping timer overhead to a minimum",
        "Z": "num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What executes the following pseudo-code at a high level?",
        "Y": "blocked_autorange",
        "Z": "num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the default for the PyTorch threadpool size?",
        "Y": "one",
        "Z": "num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does the default PyTorch threadpool size try to utilize?",
        "Y": "all cores",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use ofdescriptionis to signal toComparethe columns of data. For instance one might set it\nbased on the input size  to create a table of the form: usingCompare. It is also included when printing a Measurement. env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Measure how many replicates while keeping timer overhead to a minimum.",
        "Y": "many replicates",
        "Z": "num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does blocked_autorange execute at a high level?",
        "Y": "pseudo-code",
        "Z": "num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the goal of blocked_autorange?",
        "Y": "Measure many replicates while keeping timer overhead to a minimum",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What variable does blocked_autorange execute the following pseudo-code?",
        "Y": "the variableblock_sizein the inner loop",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How many competing objectives must a block size balance?",
        "Y": "two",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How do you measure many replicates?",
        "Y": "keeping timer overhead to a minimum",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What variable is in the inner loop of blocked_autorange?",
        "Y": "variableblock_size",
        "Z": "Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What results in more replicates and generally better statistics?",
        "Y": "A small block size",
        "Z": "A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is in the inner loop of blocked_autorange?",
        "Y": "variableblock_size",
        "Z": "At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is in the inner loop?",
        "Y": "variableblock_size",
        "Z": "Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How many competing objectives must the choice of block size balance?",
        "Y": "two",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does a large block size amortize better?",
        "Y": "cost of timer invocation",
        "Z": "Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is non-trivial?",
        "Y": "CUDA syncronization time",
        "Z": "Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What variable is in the inner loop?",
        "Y": "variableblock_size",
        "Z": "Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does a large block size do to the cost oftimerinvocation?",
        "Y": "amortizes",
        "Z": "A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "A large block size better amortizes what?",
        "Y": "cost of timer invocation",
        "Z": "A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What percentage of the computation is timer overhead?",
        "Y": "0.1%",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the value of block_size used for?",
        "Y": "main measurement loop",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is non-trivial (order single to low double digit microseconds) and would otherwise bias the measurement?",
        "Y": "CUDA syncronization time",
        "Z": "A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does blocked_autorange set?",
        "Y": "block_size",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is blocked_autorange used for?",
        "Y": "main measurement loop",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the value of blocked_autorange used for?",
        "Y": "main measurement loop",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "A Measurement object can be used to do what?",
        "Y": "compute statistics",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is an example of a measurement object that can be used to compute statistics?",
        "Y": "median",
        "Z": "A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used to collect instruction counts?",
        "Y": "Callgrind",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does blocked_autorange set by running a warmup period?",
        "Y": "block_size",
        "Z": "Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What contains measured runtimes and repetition counts?",
        "Y": "A Measurement object",
        "Z": "env\u2013 This tag indicates that otherwise identical tasks were run in\ndifferent environments, and are therefore not equivilent, for\ninstance when A/B testing a change to a kernel.Comparewill\ntreat Measurements with differentenvspecification as distinct\nwhen merging replicate runs. num_threads\u2013 The size of the PyTorch threadpool when executingstmt. Single\nthreaded performace is important as both a key inference workload\nand a good indicator of intrinsic algorithmic efficiency, so the\ndefault is set to one. This is in contrast to the default PyTorch\nthreadpool size which tries to utilize all cores. Measure many replicates while keeping timer overhead to a minimum. At a high level, blocked_autorange executes the following pseudo-code: Note the variableblock_sizein the inner loop. The choice of block\nsize is important to measurement quality, and must balance two\ncompeting objectives: A small block size results in more replicates and generally\nbetter statistics. A large block size better amortizes the cost oftimerinvocation, and results in a less biased measurement. This is\nimportant because CUDA syncronization time is non-trivial\n(order single to low double digit microseconds) and would\notherwise bias the measurement. blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a measurement object that can be used to compute statistics?",
        "Y": "median",
        "Z": "blocked_autorange sets block_size by running a warmup period,\nincreasing block size until timer overhead is less than 0.1% of\nthe overall computation. This value is then used for the main\nmeasurement loop. A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What can the A Measurement object be used for?",
        "Y": "compute statistics",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is another name for an A Measurement object?",
        "Y": "median",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What tool can be used to collect instruction counts?",
        "Y": "Callgrind",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What object contains measured runtimes and repetition counts?",
        "Y": "A Measurement object",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What tool is used to collect instruction counts?",
        "Y": "Callgrind",
        "Z": "Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Callgrind do?",
        "Y": "Collect instruction counts",
        "Z": "Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does Callgrind collect?",
        "Y": "instruction counts",
        "Z": "Collect instruction counts using Callgrind.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is deterministic?",
        "Y": "instruction counts",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Instruction counts are ideal for what?",
        "Y": "detailed performance analysis",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is a separate process so that Valgrind can instrument the program?",
        "Y": "runs stmt in",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Performance is severely degraded due to what?",
        "Y": "instrumentation",
        "Z": "Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What must be installed in order to use?",
        "Y": "method valgrind",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What must be installed in order to use methodvalgrind?",
        "Y": "callgrind_control, and callgrind_annotate",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "By default, what will be collected and cached to indicate how many instructions are from the Python loop which drivesstmt?",
        "Y": "a profile",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What provides basic facilities for analyzing and manipulating results?",
        "Y": "A Callgrind Stats object",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does ACallgrindStatsobject do?",
        "Y": "Mirrors the semantics of timeit.Timer.timeit()",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the result of?",
        "Y": "Timer measurement",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What provides instruction counts and some basic facilities for analyzing and manipulating results?",
        "Y": "A Callgrind Stats object",
        "Z": "In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "A Callgrind Stats object mirrors the semantics of what?",
        "Y": "timeit.Timer.timeit()",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the name of the main statement?",
        "Y": "stmt",
        "Z": "By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does A Callgrind Stats object mirror?",
        "Y": "timeit.Timer.timeit()",
        "Z": "ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling:",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does stmt stand for?",
        "Y": "main statement",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does this class store?",
        "Y": "one or more measurements of a given statement",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does this class provide downstream consumers?",
        "Y": "several convenience methods",
        "Z": "A Measurement object that contains measured runtimes and\nrepetition counts, and can be used to compute statistics.\n(mean, median, etc.) Collect instruction counts using Callgrind. Unlike wall times, instruction counts are deterministic\n(modulo non-determinism in the program itself and small amounts of\njitter from the Python interpreter.) This makes them ideal for detailed\nperformance analysis. This method runsstmtin a separate process\nso that Valgrind can instrument the program. Performance is severely\ndegraded due to the instrumentation, howevever this is ameliorated by\nthe fact that a small number of iterations is generally sufficient to\nobtain good measurements. In order to to use this methodvalgrind,callgrind_control, andcallgrind_annotatemust be installed. Because there is a process boundary between the caller (this process)\nand thestmtexecution,globalscannot contain arbitrary in-memory\ndata structures. (Unlike timing methods) Instead, globals are\nrestricted to builtins,nn.Modules\u2019s, and Torch Scripted functions/modules\nto reduce the surprise factor from serialization and subsequent\ndeserialization. TheGlobalsBridgeclass provides more detail on this\nsubject. Take particular care with nn.Modules: they rely on pickle and\nyou may need to add an import tosetupfor them to transfer properly. By default, a profile for an empty statement will be collected and\ncached to indicate how many instructions are from the Python loop which\ndrivesstmt. ACallgrindStatsobject which provides instruction counts and\nsome basic facilities for analyzing and manipulating results. Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the semantics of timeit.Timer.timeit()?",
        "Y": "Mirrors",
        "Z": "Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does timeit.Timer.timeit() do?",
        "Y": "Execute the main statement (stmt)numbertimes",
        "Z": "Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Who does this class provide convenience methods for?",
        "Y": "downstream consumers",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates)",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is used for merging replicates?",
        "Y": "Convenience method",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the semantics of?",
        "Y": "timeit.Timer.timeit()",
        "Z": "Mirrors the semantics of timeit.Timer.timeit(). Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What does this class store of a given statement?",
        "Y": "one or more measurements",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates)",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What will extrapolate times tonumber_per_run=1 and will not transfer any metadata?",
        "Y": "Merge",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet. The significant figure estimation used in conjunction with thetrim_sigfigmethod to provide a more human interpretable data\nsummary. __repr__ does not use this method; it simply displays raw\nvalues. Significant figure estimation is intended forCompare. Top level container for Callgrind results collected by Timer. Manipulation is generally done using the FunctionCounts class, which is\nobtained by callingCallgrindStats.stats(\u2026). Several convenience\nmethods are provided as well; the most significant isCallgrindStats.as_standardized(). Strip library names and some prefixes from function strings. When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: Stripping prefixes can ameliorate this issue by regularizing the\nstrings and causing better cancellation of equivilent call sites\nwhen diffing. Returns the total number of instructions executed.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why does Merge not transfer any metadata?",
        "Y": "differ between replicates",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates)",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What is the main statement called?",
        "Y": "stmt",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates) Approximate significant figure estimate. This property is intended to give a convenient way to estimate the\nprecision of a measurement. It only uses the interquartile region to\nestimate statistics to try to mitigate skew from the tails, and\nuses a static z value of 1.645 since it is not expected to be used\nfor small values ofn, so z can approximatet.",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "Why will Merge not transfer any metadata?",
        "Y": "it might differ between replicates",
        "Z": "Execute the main statement (stmt)numbertimes.https://docs.python.org/3/library/timeit.html#timeit.Timer.timeit The result of a Timer measurement. This class stores one or more measurements of a given statement. It is\nserializable and provides several convenience methods\n(including a detailed __repr__) for downstream consumers. Convenience method for merging replicates. Merge will extrapolate times tonumber_per_run=1and will not\ntransfer any metadata. (Since it might differ between replicates)",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "What type of object does obj have to implement write and flush?",
        "Y": "a file-like object",
        "Z": "See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What does a PathLike object contain?",
        "Y": "a file name",
        "Z": "See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What does PyTorch preserve across serialization?",
        "Y": "storage sharing",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "See Saving and loading tensors preserves what for more details?",
        "Y": "views",
        "Z": "See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What is another name for saving and loading tensors?",
        "Y": "Note",
        "Z": "See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What can be specified to override the default protocol?",
        "Y": "pickle_protocol",
        "Z": "pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What preserves viewsfor more details?",
        "Y": "See Saving and loading tensors",
        "Z": "pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What is a common PyTorch convention to save tensors using.pt file extension?",
        "Y": "Note",
        "Z": "f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What type of object has to implement write and flush?",
        "Y": "a file-like object",
        "Z": "f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What type of object contains a file name pickle_module?",
        "Y": "a string or os.PathLike object",
        "Z": "f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What is a common PyTorch convention to save tensors using?",
        "Y": ".pt file extension",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What module can be specified to override the default protocol?",
        "Y": "pickle_module",
        "Z": "pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What does PyTorch preserves storage sharing across serialization?",
        "Y": "Note",
        "Z": "pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What preserves views for more details?",
        "Y": "See Saving and loading tensors",
        "Z": "Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What release of PyTorch switchedtorch.saveto use a new zipfile-based file format?",
        "Y": "1.6",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What does PyTorch.saveto pass if you want to use the old format?",
        "Y": "kwarg_use_new_zipfile_serialization=False",
        "Z": "Saves an object to a disk file. See also:Saving and loading tensors obj\u2013 saved object f\u2013 a file-like object (has to implement write and flush) or a string or\nos.PathLike object containing a file name pickle_module\u2013 module used for pickling metadata and objects pickle_protocol\u2013 can be specified to override the default protocol Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "What is an example of a file format that PyTorch.saveto use a new zipfile-based file format?",
        "Y": "Example",
        "Z": "Note A common PyTorch convention is to save tensors using .pt file extension. Note PyTorch preserves storage sharing across serialization. See Saving and loading tensors preserves viewsfor more details. Note The 1.6 release of PyTorch switchedtorch.saveto use a new\nzipfile-based file format.torch.loadstill retains the ability to\nload files in the old format. If for any reason you wanttorch.saveto use the old format, pass the kwarg_use_new_zipfile_serialization=False. Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "Set_grad_enabled will enable or disable grads based on what?",
        "Y": "argumentmode",
        "Z": "set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What can set_grad_enabled be used as?",
        "Y": "context-manager or as a function",
        "Z": "set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What type of context manager is set_grad_enabled?",
        "Y": "thread local",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What does mode(bool) do to enable grad (True) or disable (False)?",
        "Y": "Flag",
        "Z": "set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What can mode(bool) be used for?",
        "Y": "conditionally enable gradients",
        "Z": "set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What do you need to do with the setting of grad_enabled?",
        "Y": "Note",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What will enable or disable grads based on its argumentmode?",
        "Y": "set_grad_enabled",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What does set_grad_enabled not affect?",
        "Y": "computation in other threads",
        "Z": "set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What is used to flag whether to enable grad (True) or disable (False)?",
        "Y": "mode(bool)",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What can mode(bool) be used to do?",
        "Y": "conditionally enable gradients",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "Set_grad_enabled can be used as what?",
        "Y": "context-manager or as a function",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "Does this context manager affect computation in other threads?",
        "Y": "it will not affect computation in other threads",
        "Z": "This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What is an example of a mechanism that can enable or disable gradients locally?",
        "Y": "set_grad_enabled.",
        "Z": "Context-manager that sets gradient calculation to on or off. set_grad_enabledwill enable or disable grads based on its argumentmode.\nIt can be used as a context-manager or as a function. This context manager is thread local; it will not affect computation\nin other threads. mode(bool) \u2013 Flag whether to enable grad (True), or disable\n(False). This can be used to conditionally enable\ngradients. Note set_grad_enabled is one of several mechanisms that can enable or\ndisable gradients locally seeLocally disabling gradient computationfor\nmore information on how they compare. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "What is the behavior similar to python'sitertools.combinationswhenwith_replacementis set toFals",
        "Y": "Compute combinations of lengthrrrof the given tensor",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "What is the name of the 1D vector?",
        "Y": "input(Tensor)",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "What is the number of elements to combine?",
        "Y": "r(int,optional)",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "What is the number of elements to combine with_replacement(boolean,optional)?",
        "Y": "r(int,optional)",
        "Z": "Compute combinations of lengthrrrof the given tensor. The behavior is similar to\npython\u2019sitertools.combinationswhenwith_replacementis set toFalse, anditertools.combinations_with_replacementwhenwith_replacementis set toTrue. input(Tensor) \u2013 1D vector. r(int,optional) \u2013 number of elements to combine with_replacement(boolean,optional) \u2013 whether to allow duplication in combination A tensor equivalent to converting all the input tensors into lists, doitertools.combinationsoritertools.combinations_with_replacementon these\nlists, and finally convert the resulting list into tensor. Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "What performs a product of the matrixinputand the vectorvec?",
        "Y": "matrix-vector",
        "Z": "Performs a matrix-vector product of the matrixinputand the vectorvec. If input is a(n\u00d7m)(n \\times m)(n\u00d7m)tensor,vecis a 1-D tensor of\nsizemmm,outwill be 1-D of sizennn. Note This function does notbroadcast. input(Tensor) \u2013 matrix to be multiplied vec(Tensor) \u2013 vector to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv"
    },
    {
        "X": "Does this function broadcast?",
        "Y": "does not broadcast",
        "Z": "Performs a matrix-vector product of the matrixinputand the vectorvec. If input is a(n\u00d7m)(n \\times m)(n\u00d7m)tensor,vecis a 1-D tensor of\nsizemmm,outwill be 1-D of sizennn. Note This function does notbroadcast. input(Tensor) \u2013 matrix to be multiplied vec(Tensor) \u2013 vector to be multiplied out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv"
    },
    {
        "X": "What does return in the given dimension(s)dim?",
        "Y": "the maximum value of each slice of the input tensor",
        "Z": "Returns the maximum value of each slice of the input tensor in the given\ndimension(s)dim. Note amax/aminsupports reducing on multiple dimensions, amax/amindoes not return indices, amax/aminevenly distributes gradient between equal values,\nwhilemax(dim)/min(dim)propagates gradient only to a single\nindex in the source tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax"
    },
    {
        "X": "What distributes gradient between equal values?",
        "Y": "amax/aminevenly",
        "Z": "Returns the maximum value of each slice of the input tensor in the given\ndimension(s)dim. Note amax/aminsupports reducing on multiple dimensions, amax/amindoes not return indices, amax/aminevenly distributes gradient between equal values,\nwhilemax(dim)/min(dim)propagates gradient only to a single\nindex in the source tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax"
    },
    {
        "X": "What function is deprecated and will be removed in a future PyTorch release?",
        "Y": "Alias oftorch.outer()",
        "Z": "Alias oftorch.outer(). Warning This function is deprecated and will be removed in a future PyTorch release.\nUsetorch.outer()instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"
    },
    {
        "X": "What is the replacement for oftorch.outer()?",
        "Y": "Usetorch.outer()",
        "Z": "Alias oftorch.outer(). Warning This function is deprecated and will be removed in a future PyTorch release.\nUsetorch.outer()instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"
    },
    {
        "X": "What is the name of the function oftorch.outer()?",
        "Y": "Alias",
        "Z": "Alias oftorch.outer(). Warning This function is deprecated and will be removed in a future PyTorch release.\nUsetorch.outer()instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"
    },
    {
        "X": "In what release will oftorch.outer() be removed?",
        "Y": "PyTorch",
        "Z": "Alias oftorch.outer(). Warning This function is deprecated and will be removed in a future PyTorch release.\nUsetorch.outer()instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"
    },
    {
        "X": "What function will be removed in a future PyTorch release?",
        "Y": "Usetorch.outer()",
        "Z": "Alias oftorch.outer(). Warning This function is deprecated and will be removed in a future PyTorch release.\nUsetorch.outer()instead.",
        "source": "https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"
    },
    {
        "X": "What is created whose values are evenly spaced frombasestarttextbasetextstartbasestarttobaseendtextbase",
        "Y": "a one-dimensional tensor of sizesteps",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly\nspaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale\nwith base base. That is, the values are: Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is created when sizesteps are evenly spaced frombasestarttextbasetextstartbasestarttobaseendtext",
        "Y": "a one-dimensional tensor",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly\nspaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale\nwith base base. That is, the values are: Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "Is not providing a value forsteps deprecated?",
        "Y": "Warning Not providing a value for steps is deprecated",
        "Z": "Warning Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "For backwards compatibility, not providing a value forstepswill create a tensor with how many elements?",
        "Y": "100 elements",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "In a future PyTorch release, failing to provide a value forstepswill throw what?",
        "Y": "a runtime error",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "For backwards compatibility, not providing a value forstepswill create what?",
        "Y": "a tensor with 100 elements",
        "Z": "Creates a one-dimensional tensor of sizestepswhose values are evenly\nspaced frombasestart{{\\text{{base}}}}^{{\\text{{start}}}}basestarttobaseend{{\\text{{base}}}}^{{\\text{{end}}}}baseend, inclusive, on a logarithmic scale\nwith base base. That is, the values are: Warning Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "Why is not providing a value forsteps deprecated?",
        "Y": "not reflected in the documented function signature",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is the starting value for the set of points end(float)?",
        "Y": "start(float)",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "Is providing a value forsteps deprecated?",
        "Y": "Not providing a value for steps is deprecated",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What default uses the global default dtype when bothstartandendare real?",
        "Y": "if None",
        "Z": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is start(float)?",
        "Y": "starting value for the set of points end(float)",
        "Z": "start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "dtype(torch.dpython:type,optional) \u2013 what to perform the computation in?",
        "Y": "the data type",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What does torch.get_default_dtype() use when bothstartandendare real?",
        "Y": "global default dtype",
        "Z": "start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is the ending value for the set of points steps(int)?",
        "Y": "end(float)",
        "Z": "end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What does torch.get_default_dtype() use when both startandendare real?",
        "Y": "global default dtype",
        "Z": "steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is the size of the constructed tensor base?",
        "Y": "steps",
        "Z": "steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What default uses the global default dtype?",
        "Y": "if None",
        "Z": "Not providing a value forstepsis deprecated. For backwards\ncompatibility, not providing a value forstepswill create a tensor\nwith 100 elements. Note that this behavior is not reflected in the\ndocumented function signature and should not be relied on. In a future\nPyTorch release, failing to provide a value forstepswill throw a\nruntime error. start(float) \u2013 the starting value for the set of points end(float) \u2013 the ending value for the set of points steps(int) \u2013 size of the constructed tensor base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is the base of the logarithm function?",
        "Y": "base(float,optional)",
        "Z": "base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "Out(Tensor,optional) - what is the output of the logarithm function?",
        "Y": "output tensor",
        "Z": "base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is base(float,optional)?",
        "Y": "base of the logarithm function",
        "Z": "base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is the default dtype used when both startandendare real?",
        "Y": "Default: if None",
        "Z": "base(float,optional) \u2013 base of the logarithm function. Default:10.0. out(Tensor,optional) \u2013 the output tensor. dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What type of dtype is used when both startandendare real?",
        "Y": "Default",
        "Z": "dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What does dtype(torch.dpython:type,optional) represent?",
        "Y": "the data type to perform the computation in",
        "Z": "dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "What is the default value for the global default dtype?",
        "Y": "if None",
        "Z": "dtype(torch.dpython:type,optional) \u2013 the data type to perform the computation in.\nDefault: if None, uses the global default dtype (see torch.get_default_dtype())\nwhen bothstartandendare real,\nand corresponding complex dtype when either is complex. layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "The output is mapped to what for which the output is mapped to+ INF?",
        "Y": "+ INF",
        "Z": "Returns a new tensor with the inverse hyperbolic cosine of the elements ofinput. Note The domain of the inverse hyperbolic cosine is[1, inf)and values outside this range\nwill be mapped toNaN, except for+ INFfor which the output is mapped to+ INF. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh"
    },
    {
        "X": "What does torch.package add for creating hermetic packages containing arbitrary PyTorch code?",
        "Y": "torch.packageadds support",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can hermetic packages be used for?",
        "Y": "to load and execute models at a later date or on a different machine",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does this document contain that will help you learn more about torch.package?",
        "Y": "tutorials, how-to guides, explanations, and an API reference",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of packages does torch.package add support for?",
        "Y": "hermetic",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can hermetic packages be used for at a later date or on a different machine?",
        "Y": "to load and execute models",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does this document contain?",
        "Y": "tutorials, how-to guides, explanations, and an API reference",
        "Z": "This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Torch.package depend on?",
        "Y": "the pickle module",
        "Z": "This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you only do when using the picklemodule?",
        "Y": "unpackage data you trust",
        "Z": "This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Do you unpack data that could have come from an untrusted source or that could have been tampered with?",
        "Y": "Never",
        "Z": "This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does this module depend on?",
        "Y": "the pickle module",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the module depend on?",
        "Y": "unpackage data you trust",
        "Z": "Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you never unpackage data that could have come from an untrusted source or that could have been tampered with?",
        "Y": "Never unpackage data that could have come from an untrusted source, or that could have been tampered with",
        "Z": "torch.packageadds support for creating hermetic packages containing arbitrary\nPyTorch code. These packages can be saved, shared, used to load and execute models\nat a later date or on a different machine, and can even be deployed to production usingtorch::deploy. This document contains tutorials, how-to guides, explanations, and an API reference that\nwill help you learn more abouttorch.packageand how to use it. Warning This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find more information about the pickle module?",
        "Y": "the documentation for the pickle module",
        "Z": "This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do you see what is inside a package?",
        "Y": "Packaging your first model",
        "Z": "This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the only thing that is not secure?",
        "Y": "unpackage data you trust",
        "Z": "This module depends on the pickle module which is is not secure. Only unpackage data you trust. It is possible to construct malicious pickle data which willexecute arbitrary code during unpickling.\nNever unpackage data that could have come from an untrusted source, or that could have been tampered with. For more information, review the documentation for the pickle module. Tutorials Packaging your first model How do I\u2026 See what is inside a package?",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a first model need to see?",
        "Y": "what is inside a package",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do I include with my package and access them later?",
        "Y": "arbitrary resources",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How is a class packaged?",
        "Y": "Customize",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I test in my source code?",
        "Y": "whether or not it is executing inside a package",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do I do to a package?",
        "Y": "Patch code into a package",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do I do from packaged code?",
        "Y": "Access package contents",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the difference between packaged and non-packaged code?",
        "Y": "non-packaged code",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do I package a Torch Script module?",
        "Y": "Package a Torch Script module",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a package for?",
        "Y": "Torch Script module",
        "Z": "Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that contains a Torch Script module?",
        "Y": "torch.packageFormat",
        "Z": "Tutorials Packaging your first model How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do I do?",
        "Y": "See what is inside a package",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of code is included in a package?",
        "Y": "Patch code",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of code does a package differ from?",
        "Y": "non-packaged code",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that contains your code\u2019s dependencies?",
        "Y": "torch.package",
        "Z": "How do I\u2026 See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you see in a package?",
        "Y": "what is inside a package",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should I include with my package?",
        "Y": "arbitrary resources",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I do from a package?",
        "Y": "Access package contents",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Distinguish between packaged code and what?",
        "Y": "non-packaged code",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is one way to package a Torch Script module?",
        "Y": "Re-export an imported object",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is included in a package?",
        "Y": "Patch code",
        "Z": "See what is inside a package? Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Howtorch.packagefinds your code\u2019s dependencies?",
        "Y": "Steps",
        "Z": "Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should I include with my package and access them later?",
        "Y": "arbitrary resources",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can I access from packaged code?",
        "Y": "package contents",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the document that describes how a package is packaged?",
        "Y": "torch.packageFormat Overview",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Torch Script do?",
        "Y": "Re-export an imported object",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Torch Script do to a package?",
        "Y": "Patch code",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you access package contents from?",
        "Y": "packaged code",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you distinguish between?",
        "Y": "packaged code and non-packaged code",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.package find your code's dependencies?",
        "Y": "Dependency Management",
        "Z": "torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you do from packaged code?",
        "Y": "Access package contents",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of code is different from packaged code?",
        "Y": "non-packaged code",
        "Z": "Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can I access package contents from?",
        "Y": "packaged code",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the difference between packaged code and non-packaged code?",
        "Y": "non-packaged code",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where is my source code tested to see if it is executing?",
        "Y": "inside a package",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the difference between packaged code and?",
        "Y": "non-packaged code",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that finds your code\u2019s dependencies?",
        "Y": "torch.packageFormat Overview",
        "Z": "Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Torch Script use to access package contents?",
        "Y": "packaged code",
        "Z": "Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.packageFormat Overview find your code's dependencies?",
        "Y": "Dependency Management",
        "Z": "Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you access from packaged code?",
        "Y": "package contents",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does package a?",
        "Y": "Torch Script module",
        "Z": "Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.package distinguish between?",
        "Y": "packaged code and non-packaged code",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.package package?",
        "Y": "Torch Script module",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What two types of code does Torch distinguish?",
        "Y": "packaged code and non-packaged code",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Torch Script do to differentiate between packaged code and non-packaged code?",
        "Y": "Re-export an imported object",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a packager do to distinguish between packages and non-packaged code?",
        "Y": "Package a Torch Script module",
        "Z": "Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you package?",
        "Y": "Torch Script module",
        "Z": "Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you need to know to create and use Torch packages?",
        "Y": "the basic API",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do to re-export an imported object?",
        "Y": "Package a Torch Script module",
        "Z": "Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where is a tutorial that guides you through packaging and unpackaging a simple model available?",
        "Y": "Colab",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the basic API for?",
        "Y": "creating and using Torch packages",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do you package a Torch Script module?",
        "Y": "Package a Torch Script module",
        "Z": "Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the basic API for creating and using?",
        "Y": "Torch packages",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "After completing this tutorial, you will be familiar with the basic API for what?",
        "Y": "creating and using Torch packages",
        "Z": "torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Howtorch.package find your code's dependencies?",
        "Y": "Dependency Management torch",
        "Z": "Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Howtorch.packagefinds your code's dependencies?",
        "Y": "Dependency Management torch.packagesharp edges",
        "Z": "Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the basic API for creating and using Torch packages?",
        "Y": "basic",
        "Z": "Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is torch.packagesharp edges?",
        "Y": "Dependency Management",
        "Z": "Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will you be familiar with the basic API for creating and using?",
        "Y": "Torch packages",
        "Z": "API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the container format for a torch.package?",
        "Y": "a torch.package is ZIP",
        "Z": "API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are some common ways to interact with ZIP files?",
        "Y": "Some common ways to interact with ZIP files",
        "Z": "Include arbitrary resources with my package and access them later? Customize how a class is packaged? Test in my source code whether or not it is executing inside a package? Patch code into a package? Access package contents from packaged code? Distinguish between packaged code and non-packaged code? Re-export an imported object? Package a Torch Script module? Explanation torch.packageFormat Overview Howtorch.packagefinds your code\u2019s dependencies Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What keeps packages isolated from each other?",
        "Y": "Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference",
        "Z": "Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "After completing this exercise, you will be familiar with the basic API for creating and using what?",
        "Y": "Torch packages",
        "Z": "Dependency Management torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the container format for Torch packages?",
        "Y": "a torch.packageis ZIP",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a common way to interact with a ZIP file?",
        "Y": "unzipmy_package.ptwill unzip thetorch.packagearchive to disk",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a tutorial that guides you through packaging and unpackaging a simple model available on Colab?",
        "Y": "API Reference",
        "Z": "API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will unzip thetorch.packagearchive to disk?",
        "Y": "unzipmy_package.pt",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What format does a torch.package use?",
        "Y": "The container format",
        "Z": "Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the container format for?",
        "Y": "a torch.package is ZIP",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a common way to interact with ZIP files?",
        "Y": "unzipmy_package.ptwill unzip thetorch.packagearchive to disk",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What provides a standard way to read and write ZIP archive contents?",
        "Y": "Python zip file module",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does vim have the ability to do?",
        "Y": "natively read ZIP archives",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do with files?",
        "Y": "edit files and :write them back into the archive",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who has the ability to natively read ZIP archives?",
        "Y": "vim",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do with ZIP files?",
        "Y": "edit files and :write them back into the archive",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What unzips thetorch.packagearchive to disk?",
        "Y": "unzipmy_package.ptwill",
        "Z": "unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can vim do with files?",
        "Y": "edit files and :writethem back into the archive",
        "Z": "unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can vim do?",
        "Y": "edit files and :writethem back into the archive",
        "Z": "The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importer and Package Exporter provide?",
        "Y": "a file_structure() method",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a simple directory structure that you can use to explore the current contents of a torch.package?",
        "Y": "The Folder object",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Which two packages provide afile_structure()method?",
        "Y": "Package Importer and Package Exporter",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does The Folder object do?",
        "Y": "The Folder object itself is directly printable and will print out a file tree representation",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What arguments are used to filter what is returned?",
        "Y": "glob-styleincludeandexcludefiltering arguments",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another way to query Folder objects?",
        "Y": "the has_file()method",
        "Z": "Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is directly printable and will print out a file tree representation?",
        "Y": "The Folder object",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What method can you use to query Folder objects?",
        "Y": "the has_file()method",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What exposes three methods that allow you to save Python objects, text, and binary data to a package?",
        "Y": "Package Exporter",
        "Z": "Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What exposes complementary methods called load_pickle,load_textandload_binary?",
        "Y": "Package Importer",
        "Z": "Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What exposes complementary methods calledload_pickle,load_textandload_binary?",
        "Y": "Package Importer",
        "Z": "Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What allows for the customization of how classes are packaged?",
        "Y": "torch.package",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How is the customization of how classes are packaged accessed?",
        "Y": "defining the method__reduce_package__on a class and by defining a corresponding de-packaging function",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is similar to torch.package?",
        "Y": "defining__reduce__for Python\u2019s normal pickling process",
        "Z": "torch.packagesharp edges Howtorch.packagekeeps packages isolated from each other API Reference A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages. The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: unzipmy_package.ptwill unzip thetorch.packagearchive to disk, where you can freely inspect its contents. The Python zip file module provides a standard way to read and write ZIP archive contents. vim has the ability to natively read ZIP archives. You can even edit files and :writethem back into the archive! Package Importer and Package Exporterprovide afile_structure()method, which will return a printable\nand queryableFolderobject. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package. The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-styleincludeandexcludefiltering arguments. Output: You can also query Folder objects with the has_file()method. Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the step that allows you to load Python objects, text and binary data from a package?",
        "Y": "Steps",
        "Z": "Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is used for the customization of how classes are packaged?",
        "Y": "torch.package allows",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.packageallow for the customization of how classes are packaged?",
        "Y": "torch.package allows",
        "Z": "Package Exporterexposes three methods,save_pickle,save_textandsave_binarythat allow you to save\nPython objects, text, and binary data to a package. Package Importerexposes complementary methods namedload_pickle,load_textandload_binarythat allow you to load\nPython objects, text and binary data from a package. torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is accessed through defining the method__reduce_package__ on a class and by defining?",
        "Y": "a corresponding de-packaging function",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What language's normal pickling process is similar to defining__reduce__for?",
        "Y": "Python",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is similar to defining__reduce__for?",
        "Y": "Python\u2019s normal pickling process",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.package allow for the customization of how classes are packaged?",
        "Y": "Steps",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the method__reduce_package__(?",
        "Y": "self,exporter:Package Exporter",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should the method return a tuple of?",
        "Y": "de-packaging function",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who calls the method when it encounters an instance of the target class?",
        "Y": "the Package Exporter",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should return a tuple of with the arguments needed to invoke the de-packaging function?",
        "Y": "de-packaging function",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Define the method__reduce_package__(self,exporter:Package Exporter)on what?",
        "Y": "the target class",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the corresponding de-packaging function returned by the method__reduce_package__?",
        "Y": "tuple",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you define for the class?",
        "Y": "de-packaging function",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should a de-packaging function do?",
        "Y": "reconstruct and return an instance of the class",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be a Package Importer instance?",
        "Y": "first parameter",
        "Z": "Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a Package Importer add to every module that it initializes?",
        "Y": "attribute__torch_package",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can your code check for the presence of the attribute__torch_package__to every module that it initializes?",
        "Y": "whether it is executing in a packaged context or not",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the basic API for creating and using packages?",
        "Y": "Torch",
        "Z": "A tutorial that guides you through packaging and unpackaging a simple model is availableon Colab.\nAfter completing this exercise, you will be familiar with the basic API for creating and using\nTorch packages.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What attribute does a Package Importer add to every module that it initializes?",
        "Y": "attribute__torch_package__",
        "Z": "a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can your code check for the presence of this attribute to determine?",
        "Y": "whether it is executing in a packaged context or not",
        "Z": "a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where is the attribute__torch_package__imported from?",
        "Y": "a torch.package",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined. a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where is the code imported from?",
        "Y": "a torch.package",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is it good practice to have code that behaves differently depending on whether it's packaged or not?",
        "Y": "it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can code that behaves differently depending on whether it's packaged or not lead to?",
        "Y": "hard-to-debug issues",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do if your package is intended to be heavily used?",
        "Y": "restructuring your code",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the package that your code is imported from?",
        "Y": "a torch.package",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is it called to have code that behaves differently depending on whether it's packaged or not?",
        "Y": "it\u2019s bad practice",
        "Z": "a Package Importerwill add the attribute__torch_package__to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Exporter offer that allows one to save arbitrary Python source code to a module of your choosing?",
        "Y": "asave_source_string()method",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code. To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse. To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is it bad practice to have code that does what depending on whether it's packaged or not?",
        "Y": "behaves differently",
        "Z": "Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What program allows you to save arbitrary Python source code to a module of your choosing?",
        "Y": "Package Exporter",
        "Z": "Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importerimplement for accessing resources from inside a package?",
        "Y": "The import lib.resources API",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code. To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse. To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What allows access to resources from within a package?",
        "Y": "The import lib.resources API",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What offers asave_source_string() method?",
        "Y": "Package Exporter",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a torch.package. Warning: in general, it\u2019s bad practice to have code that behaves differently depending on whether it\u2019s packaged or not. This can lead to\nhard-to-debug issues that are sensitive to how you imported your code. If your package is intended to be heavily used, consider restructuring\nyour code so that it behaves the same way no matter how it was loaded. Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What allows access to resources from within packaged code?",
        "Y": "The import lib.resources API",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code. To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse. To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why is theimportlib.resources the recommended way to access package contents from within packaged code?",
        "Y": "complies with the Python standard",
        "Z": "Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is it possible to access from within packaged code?",
        "Y": "parentPackage Importer instance",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing. Package Importerimplements The import lib.resources API for accessing resources from inside a package. The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code. To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse. To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does importlib.resources comply with?",
        "Y": "Python standard",
        "Z": "Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code. To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the Importlib.resourcesAPI allow access to from within packaged code?",
        "Y": "parent Package Importer instance",
        "Z": "The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why is usingimportlib.resources the recommended way to access package contents from within packaged code?",
        "Y": "complies with the Python standard",
        "Z": "The import lib.resources API allows access to resources from within packaged code. Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another way to access package contents from within packaged code?",
        "Y": "parent Package Importer instance",
        "Z": "Usingimportlib.resourcesis the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent Package Importer instance itself from within\npackaged code. To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is used to tell if an object's code is from a torch.package?",
        "Y": "the torch.package.is_from_package()function",
        "Z": "To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an object is from a package but its definition is from a module markedexternor fromstdlib, what will this check return",
        "Y": "returns False",
        "Z": "To tell if an object\u2019s code is from a torch.package, use thetorch.package.is_from_package()function.\nNote: if an object is from a package but its definition is from a module markedexternor fromstdlib,\nthis check will returnFalse.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must the newPackage Exporter aware of the originalPackage Importers find for your object's dependencies?",
        "Y": "source code",
        "Z": "To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you use to package a Torch Script model?",
        "Y": "same save_pickle and load_pickle APIs",
        "Z": "To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How is saving Torch Script objects that are attributes or submodules supported?",
        "Y": "no extra work",
        "Z": "To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies. To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of function should be defined for the class?",
        "Y": "de-packaging",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should the de-packaging function do?",
        "Y": "reconstruct and return an instance of the class",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should the function signature's first parameter be?",
        "Y": "a Package Importer instance",
        "Z": "torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method__reduce_package__on a class and by defining a corresponding de-packaging function. This is similar to defining__reduce__for\nPython\u2019s normal pickling process. Steps: Define the method__reduce_package__(self,exporter:Package Exporter)on the target class. This method should do the work to save the class instance inside of the package, and should return a tuple of the corresponding de-packaging function with the arguments needed to invoke the de-packaging function. This method is called by the Package Exporterwhen it encounters an instance of the target class. Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be the first parameter of a de-packaging function?",
        "Y": "a Package Importer instance",
        "Z": "Define a de-packaging function for the class. This de-packaging function should do the work to reconstruct and return an instance of the class. The function signature\u2019s first parameter should be a Package Importer instance, and the rest of the parameters are user defined.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What package offers asave_source_string() method?",
        "Y": "Package Exporter",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Exporter offer?",
        "Y": "asave_source_string()method",
        "Z": "Package Exporteroffers asave_source_string()method that allows one to save arbitrary Python source code to a module of your choosing.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who previously imported an object?",
        "Y": "a Package Importer",
        "Z": "To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must the newPackage Exporteraware of the originalPackage Importers find for your object\u2019s dependencies?",
        "Y": "source code",
        "Z": "To re-export an object that was previously imported by a Package Importer, you must make the newPackage Exporteraware of the originalPackage Importerso that it can find source code for your object\u2019s dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the benefit of saving Torch Script objects that are attributes or submodules?",
        "Y": "no extra work",
        "Z": "To package a Torch Script model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving Torch Script objects that are attributes or submodules is supported as well with no extra work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a ZIP archive that uses the.ptextension?",
        "Y": "a torch.packagefile",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What kind of files are placed in the.data/. User files?",
        "Y": "Framework files",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of everything else in the a torch.packagefile?",
        "Y": "User files",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a fully packaged ResNet model?",
        "Y": "fully packaged ResNet model fromtorchvision",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of files are placed in the.data/. User files?",
        "Y": "Framework files",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of everything else inside the a torch.packagefile?",
        "Y": "User files",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a fully packaged fromtorchvision look like?",
        "Y": "ResNet model",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is placed in the.data/?",
        "Y": "Framework files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of everything else in a ResNet model?",
        "Y": "User files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who owns the data/directory of a fully packaged ResNet model?",
        "Y": "torch.package",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who makes no guarantees about the contents of.data/?",
        "Y": "Thetorch.packageformat",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are placed in the.data/?",
        "Y": "Framework files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of everything else in PyTorch?",
        "Y": "User files",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What makes no guarantees about the contents of.data/?",
        "Y": "Thetorch.packageformat",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is everything else in a ResNet model?",
        "Y": "User files",
        "Z": "User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who owns the.data/directory?",
        "Y": "torch.package",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What currently contains the following items?",
        "Y": "the.data/directory",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who owns The.data/directory?",
        "Y": "torch.package",
        "Z": "As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What contains the following items?",
        "Y": "the.data/directory",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who knows how to load a package?",
        "Y": "thetorch.packageimport infrastructures",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does *.storage contain?",
        "Y": "serialized tensor data",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the version number for the serialized format?",
        "Y": "version",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is used to import externalmodules?",
        "Y": "the loading environment\u2019s system importer",
        "Z": "As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is serialized tensor data?",
        "Y": "*.storage",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a version number for?",
        "Y": "serialized format",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be used to import externmodules?",
        "Y": "loading environment\u2019s system importer",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is stored in *.storage?",
        "Y": "serialized tensor data",
        "Z": "Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a version number for the serialized format?",
        "Y": "version number",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of data is stored in extern_modules?",
        "Y": "serialized tensor data",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who put all other files in the archive?",
        "Y": "a user",
        "Z": "All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the layout of a Pythonregular package?",
        "Y": "identical",
        "Z": "The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do if you want to get a deeper understanding of how Python packaging works?",
        "Y": "double-check implementation details with thePython reference documentation",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does the layout compare to a Pythonregular package?",
        "Y": "identical",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find out more about how Python packaging works?",
        "Y": "Python reference documentation",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does the layout of the archive compare to a Pythonregular package?",
        "Y": "identical",
        "Z": "All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is this essay up to date or out of date?",
        "Y": "slightly out of date",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Exporter use to pickle the object?",
        "Y": "a save_pickle",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Exporter use to parse the pickle bytecode?",
        "Y": "thepickletoolsstandard library module",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why is this essay slightly out of date?",
        "Y": "double-check implementation details with thePython reference documentation",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who will pickle the object normally when you issue a save_pickle(obj,...)call?",
        "Y": "Package Exporter",
        "Z": "extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When you issue what call,Package Exporter will pickle the object normally?",
        "Y": "a save_pickle",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What describes where to find the implementation of the object's type?",
        "Y": "a GLOBAL opcode",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find more information about pickling and the pickle format?",
        "Y": "Python docs",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who will pickle the object when you issue a save_pickle(obj,...)call?",
        "Y": "Package Exporter",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "For more information about pickling and the pickle format, please consult what?",
        "Y": "Python docs",
        "Z": "a torch.packagefile is a ZIP archive which conventionally uses the.ptextension. Inside the ZIP archive, there are two kinds of files: Framework files, which are placed in the.data/. User files, which is everything else. As an example, this is what a fully packaged ResNet model fromtorchvisionlooks like: The.data/directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThetorch.packageformat makes no guarantees about the contents of.data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load oldertorch.packages). Currently, the.data/directory contains the following items: version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will gather up allGLOBALops and mark them as dependencies of your pickled object?",
        "Y": "dependency resolver",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The dependency resolver will mark allGLOBALops as what?",
        "Y": "dependencies",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the dependency resolver mark allGLOBALops as?",
        "Y": "dependencies of your pickled object",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.packagewalk when a module is identified as a dependency?",
        "Y": "python AST representation",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What representation does torch.packagewalk when a module is identified as a dependency?",
        "Y": "python AST",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "In what way are the imported modules parsed?",
        "Y": "AST walking",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are some of the standard forms that are supported by Torch.package?",
        "Y": "fromximporty,importz,fromwimportvasu",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What walking way is used to parse dependencies?",
        "Y": "AST",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What has limited support for the__import__(...)syntax?",
        "Y": "AST parsing",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where should you not expect dynamic imports to be detected?",
        "Y": "bytorch.package",
        "Z": "When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What automatically finds the Python modules that your code and objects depend on?",
        "Y": "torch.package",
        "Z": "version: a version number for the serialized format, so that thetorch.packageimport infrastructures knows how to load this package. extern_modules: a list of modules that are consideredextern:class:`Package Importer`.``externmodules will be imported using the loading environment\u2019s system importer. *.storage: serialized tensor data. All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation). When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the process called that finds the Python modules that your code and objects depend on?",
        "Y": "dependency resolution",
        "Z": "torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must you specify for each module that the dependency resolver finds?",
        "Y": "an action to take",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are the allowed actions?",
        "Y": "intern: put this module into the package",
        "Z": "Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What finds the Python modules that your code and objects depend on?",
        "Y": "torch.package",
        "Z": "torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that is put into the package?",
        "Y": "intern",
        "Z": "intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does extern do?",
        "Y": "declare this module as an external dependency of the package",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another name for stub out a module?",
        "Y": "mock",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will depending on this module do during package export?",
        "Y": "raise an error",
        "Z": "torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the action that puts a module into the package?",
        "Y": "intern",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does extern declare this module as?",
        "Y": "external dependency",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action stubs out the module?",
        "Y": "mock",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will depend on this module raise during package export?",
        "Y": "an error",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action that removes or changes dependencies in your code?",
        "Y": "Refactoring",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action is allowed to put a module into the package?",
        "Y": "intern",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action declares this module as an external dependency of the package?",
        "Y": "extern",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action is allowed to stub out a module?",
        "Y": "mock",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will dependencies on this module do during package export?",
        "Y": "raise an error",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action that removes or changes dependencies in your code that is not technically part of oftorch?",
        "Y": "Refactoring",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does intern do?",
        "Y": "put this module into the package",
        "Z": "intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that stubs out?",
        "Y": "mock",
        "Z": "intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module that is declared as an external dependency of the package?",
        "Y": "extern",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another action that is not technically part of of torch?",
        "Y": "Refactoring",
        "Z": "deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you do to declare this module as an external dependency of the package?",
        "Y": "declare this module as an external dependency of the package",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will raise an error during depending on this module?",
        "Y": "package export",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will dependencies do during package export?",
        "Y": "raise an error",
        "Z": "deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action that removes or changes dependencies in your code that is not technically part of oftorch.package?",
        "Y": "Refactoring",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Actions are defined on what?",
        "Y": "entire Python modules",
        "Z": "Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Is there a way to package a function or class from a module and leave the rest out?",
        "Y": "There is no way to package \u201cjust\u201d a function or class from module and leave the rest out",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why is there no way to package just a function or class from module and leave the rest out?",
        "Y": "by design",
        "Z": "The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why is there no way to package just a function or class from a module and leave out the rest?",
        "Y": "Python does not offer clean boundaries between objects defined in a module",
        "Z": "Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the only defined unit of dependency organization?",
        "Y": "a module",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is refactoring?",
        "Y": "remove or change the dependencies in your code",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Actions are only defined on what?",
        "Y": "entire Python modules",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the only defined unit of dependency organization in Python?",
        "Y": "module",
        "Z": "extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why is there no way to package \u201cjust\u201d a function or class from module and leave the rest out?",
        "Y": "by design",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Python does not offer what between objects defined in a module?",
        "Y": "clean boundaries",
        "Z": "Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the term for removing or changing dependencies in your code?",
        "Y": "Refactoring",
        "Z": "Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is important to note about actions in Python?",
        "Y": "actions are only defined on entire Python modules",
        "Z": "Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the reason that Python does not offer clean boundaries between objects defined in a module?",
        "Y": "by design",
        "Z": "Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Python not offer clean boundaries between objects defined in a module?",
        "Y": "Python does not offer clean boundaries between objects defined in a module",
        "Z": "Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are actions applied to modules using?",
        "Y": "patterns",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Patterns can be either module names (\"foo.bar\") or what?",
        "Y": "globs",
        "Z": "The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package. torch.packageautomatically finds the Python modules that your code and objects depend on. This process is called dependency resolution.\nFor each module that the dependency resolver finds, you must specify an action to take. The allowed actions are: intern: put this module into the package. extern: declare this module as an external dependency of the package. mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How do you associate a pattern with an action?",
        "Y": "methods onPackage Importer",
        "Z": "mock: stub out this module. deny: depending on this module will raise an error during package export. Finally, there is one more important action that is not technically part oftorch.package: Refactoring: remove or change the dependencies in your code. Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s whattorch.packageuses. Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module matches a pattern, what is applied to it?",
        "Y": "corresponding action",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What action is taken if a module matches a pattern?",
        "Y": "first action",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is intern-ed, it will be what?",
        "Y": "placed into the package",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module matches a pattern, the corresponding action is applied to it?",
        "Y": "a module matches a pattern",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be checked in the order that they were defined?",
        "Y": "patterns",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a module is interned?",
        "Y": "it will be placed into the package",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action that is placed into the package?",
        "Y": "your model code",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What would you need to package a ResNet from Torchvision?",
        "Y": "to intern the module torchvision.models.resnet",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is applied if a module matches a pattern?",
        "Y": "corresponding action",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is interned, it will be what?",
        "Y": "placed into the package",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is included in the package if a module is interned?",
        "Y": "model code",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module you want to package from Torchvision?",
        "Y": "torchvision.models.resnet",
        "Z": "Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like\"foo.**\"). You associate a pattern\nwith an action using methods onPackage Importer, e.g. If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens when a module is placed into a package?",
        "Y": "module is intern-ed",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be placed into the package if a module is interned?",
        "Y": "your model code",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What would you need to do if you were trying to package a ResNet from Torchvision?",
        "Y": "to intern the module torchvision.models.resnet",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is placed into the package when a module is interned?",
        "Y": "model code",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the action that you want to package?",
        "Y": "model code",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will you need if you are trying to package a ResNet fromtorchvision?",
        "Y": "to intern the module torchvision.models.resnet",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the module you need to package a ResNet fromtorchvision?",
        "Y": "torchvision.models.resnet",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who will look inside your package for an intern-ed module?",
        "Y": "Package Importer",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will happen if Package Importer can't find that module?",
        "Y": "an error will be raised",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will Package Importer only use if you have my_interned_moduleavailable in both your package and the loading environment",
        "Y": "version in your package",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the only source module that can be intern-ed?",
        "Y": "Python",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What other types of modules will raise an error if you attempt to intern them?",
        "Y": "C extension modules and bytecode modules",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do C extension modules and bytecode modules need to be?",
        "Y": "be mock-ed or extern-ed",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a module is extern-ed?",
        "Y": "it will not be packaged",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is extern-ed, it will be added to what list for this package?",
        "Y": "external dependencies",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find the list of external dependencies for a package?",
        "Y": "on package_exporter.extern_modules",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be intern-ed?",
        "Y": "Python source modules",
        "Z": "On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are two examples of modules that will raise an error if you attempt tointern?",
        "Y": "C extension modules and bytecode modules",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do C extension modules and bytecode modules need to do?",
        "Y": "be mock-ed or extern-ed",
        "Z": "This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find a list of external dependencies for a module?",
        "Y": "on package_exporter.extern_modules",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is extern-ed, it will be added to a list of what?",
        "Y": "external dependencies",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When does Package Importer use the default Python importer to find a module?",
        "Y": "when time packaged code tries to import anextern-ed module",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be raised if Package Importer can't find that module?",
        "Y": "an error",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will use the default Python importer to find anextern-ed module?",
        "Y": "Package Importer",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What language does Package Importer use to find anextern-ed module?",
        "Y": "Python",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you depend on within your package without having to package them too?",
        "Y": "third-party libraries",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you depend on without having to package them?",
        "Y": "third-party libraries",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if an external library changes in a backwards-incompatible way?",
        "Y": "your package may fail to load",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do if you need long-term reproducibility for your package?",
        "Y": "limit your use of extern",
        "Z": "If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do without having to package them?",
        "Y": "depend on third-party libraries likenumpyandscipy",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What kind of way does an external library change?",
        "Y": "backwards-incompatible",
        "Z": "In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do if you want long-term reproducibility for your package?",
        "Y": "limit your use of extern",
        "Z": "In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will not be packaged if a stub module is used?",
        "Y": "module is mock-ed",
        "Z": "Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be packaged in its place if a module is mock-ed?",
        "Y": "stub module",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will any use of a stub module raise?",
        "Y": "a NotImplementedError",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if a module is mock-ed?",
        "Y": "it will not be packaged",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a module is not packaged, a stub module will be packaged in its place.",
        "Y": "module is mock-ed",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be used for code that you \"know\" won't be needed in the loaded package?",
        "Y": "mock",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is code only used for?",
        "Y": "debugging/training",
        "Z": "In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should mock be used for?",
        "Y": "last resort",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does mock introduce between packaged and non-packaged code?",
        "Y": "behavioral differences",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you refactor your code to do?",
        "Y": "remove unwanted dependencies",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be used for code that you \"know\" will not be needed in the loaded package?",
        "Y": "mock",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of code that you \"know\" will not be needed in the loaded package?",
        "Y": "initialization/configuration code",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does mock introduce between packaged code and non-packaged code?",
        "Y": "behavioral differences",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do to remove unwanted dependencies?",
        "Y": "refactor",
        "Z": "If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the best way to manage dependencies?",
        "Y": "not have dependencies at all",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can code be refactored to do?",
        "Y": "remove unnecessary dependencies",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are clean dependencies generally considered?",
        "Y": "good practices",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are some guidelines for writing code with clean dependencies?",
        "Y": "Include only what you use",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who will try to process unused imports?",
        "Y": "The dependency resolver",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are generally good practices for writing code with?",
        "Y": "clean dependencies",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Do not leave what in our code?",
        "Y": "unused imports",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who is not smart enough to tell that unused imports are indeed unused?",
        "Y": "The dependency resolver",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the dependency resolver do?",
        "Y": "Include only what you use",
        "Z": "Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Do you leave unused imports in our code?",
        "Y": "Do not leave unused imports in our code",
        "Z": "Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the dependency resolver do to your imports?",
        "Y": "Qualify",
        "Z": "Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Instead of writing import foo and later usingfoo.bar.baz, what would you prefer to do?",
        "Y": "writefromfoo.barimportbaz",
        "Z": "In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the writefromfoo.barimportbaz specify?",
        "Y": "real dependency",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you only include in your code?",
        "Y": "what you use",
        "Z": "Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the dependency resolver not know about unused imports?",
        "Y": "Do not leave unused imports",
        "Z": "Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Instead of writing import foo and later usingfoo.bar.baz, what do you prefer?",
        "Y": "writefromfoo.barimportbaz",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is your real dependency?",
        "Y": "foo.bar",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Qualify what?",
        "Y": "imports",
        "Z": "Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a better way to write import foo?",
        "Y": "writefromfoo.barimportbaz",
        "Z": "Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who knows you don't need all offoo?",
        "Y": "the dependency resolver",
        "Z": "Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another name for imports?",
        "Y": "Qualify your imports",
        "Z": "Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Instead of writing import foo and later usingfoo.bar.baz, prefer what?",
        "Y": "writefromfoo.barimportbaz",
        "Z": "On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern. If a module is mock-ed, it will not be packaged. Instead a stub module will be packaged in its place. The stub module will allow you to retrieve\nobjects from it (so thatfrommy_mocked_moduleimportfoowill not error), but any use of that object will raise a NotImplementedError. mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does writefromfoo.barimportbaz do?",
        "Y": "lets the dependency resolver know you don\u2019t need all offoo",
        "Z": "Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do if you want to get a deeper dive into how Python packaging works?",
        "Y": "double-check implementation details with thePython reference documentation",
        "Z": "All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is identical to a Pythonregular package?",
        "Y": "The layout",
        "Z": "All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Why should you consult this essay?",
        "Y": "it\u2019s slightly out of date",
        "Z": "All other files in the archive were put there by a user. The layout is identical to a Pythonregular package. For a deeper dive in how Python packaging works,\nplease consultthis essay(it\u2019s slightly out of date, so double-check implementation details\nwith thePython reference documentation).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you not expect dynamic imports to be detected?",
        "Y": "bytorch.package",
        "Z": "Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What parsing has limited support for the__import__(...)syntax?",
        "Y": "AST",
        "Z": "Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "In general, you should not expect dynamic imports to be detected what?",
        "Y": "bytorch.package",
        "Z": "When you issue a save_pickle(obj,...)call,Package Exporterwill pickle the object normally. Then, it uses thepickletoolsstandard library module to parse the pickle bytecode. In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: The dependency resolver will gather up allGLOBALops and mark them as dependencies of your pickled object.\nFor more information about pickling and the pickle format, please consultthe Python docs. When a Python module is identified as a dependency,torch.packagewalks the module\u2019s python AST representation and looks for import statements with\nfull support for the standard forms:fromximporty,importz,fromwimportvasu, etc. When one of these import statements are\nencountered,torch.packageregisters the imported modules as dependencies that are then themselves parsed in the same AST walking way. Note: AST parsing has limited support for the__import__(...)syntax and does not supportimportlib.import_modulecalls. In general, you should\nnot expect dynamic imports to be detected bytorch.package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is one way to split up large files with unrelated functionality?",
        "Y": "Split up large files with unrelated functionality into smaller ones",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of unrelated functionality does yourutilsmodule contain?",
        "Y": "hodge-podge",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of modules can be packaged independently of each other?",
        "Y": "single-purpose modules",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What allows you to specify groups of modules with a convenient syntax?",
        "Y": "Patterns",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The syntax and behavior of patterns follows what?",
        "Y": "Bazel/Buckglob()",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is one way to split up large files with unrelated functionality into smaller ones?",
        "Y": "Split up large files with unrelated functionality into smaller ones",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If yourutilsmodule contains a hodge-podge of unrelated functionality, what will any module that depends onutils need to",
        "Y": "pull in lots of unrelated dependencies",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should you do instead of splitting up large files with unrelated dependencies?",
        "Y": "define single-purpose modules",
        "Z": "The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the syntax and behavior of patterns?",
        "Y": "follows the Bazel/Buckglob()",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a module that we are trying to match against a pattern called?",
        "Y": "a candidate",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a candidate composed of?",
        "Y": "a list of segments separated by a separator string",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a pattern contain?",
        "Y": "one or more segments",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Segments can be what?",
        "Y": "literal string",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What matches any string, including the empty string?",
        "Y": "wildcard",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the wildcard match against?",
        "Y": "any string, including the empty string",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What matches against zero or more complete segments?",
        "Y": "double wildcard",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a double wildcard match against?",
        "Y": "matchestorchand all its submodules",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "A pattern contains what?",
        "Y": "one or more segments",
        "Z": "Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of string matches exactly?",
        "Y": "literal string",
        "Z": "A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a string contain?",
        "Y": "wildcard",
        "Z": "A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the wildcard match?",
        "Y": "any string, including the empty string",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "A double wildcard matches against what number of complete segments?",
        "Y": "zero",
        "Z": "A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a double wildcard?",
        "Y": "torch",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What matches all of torch's submodules?",
        "Y": "matchestorch.nnortorch.functional",
        "Z": "A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the submodule that matchestorchand all its submodules?",
        "Y": "torch",
        "Z": "Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another name for ortorch?",
        "Y": "nottorch.nn.functional",
        "Z": "A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a string containing?",
        "Y": "wildcard",
        "Z": "A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a double wildcard do?",
        "Y": "matches against zero or more complete segments",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a double wildcard?",
        "Y": "matchestorchand all its submodules",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another name for ortorch torch?",
        "Y": "nottorch.nn.functional",
        "Z": "A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly. A string containing a wildcard (e.g.torch, orfoo*baz*). The wildcard matches any string, including the empty string. A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When specifying actions, you can pass what?",
        "Y": "multiple patterns",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of wildcard matches against zero or more complete segments?",
        "Y": "double wildcard",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What submodule is matchestorchand all its submodules?",
        "Y": "torch",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does a module match against an action?",
        "Y": "if it matches any of the patterns",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you specify to exlcude?",
        "Y": "patterns",
        "Z": "A double wildcard (**). This matches against zero or more complete segments. Examples: torch.**: matchestorchand all its submodules, e.g.torch.nnandtorch.nn.functional. torch.*: matchestorch.nnortorch.functional, but nottorch.nn.functionalortorch torch*.**: matchestorch,torchvision, and all of their submodules When specifying actions, you can pass multiple patterns, e.g. A module will match against this action if it matches any of the patterns. You can also specify patterns to exlcude, e.g.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the only source module that can be interned?",
        "Y": "Python",
        "Z": "If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are two examples of modules that will raise an error if you try tointern them?",
        "Y": "C extension modules and bytecode modules",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be interned?",
        "Y": "Python source modules",
        "Z": "If a module matches a pattern, the corresponding action is applied to it. For a given module, patterns will be checked in the order that they were defined,\nand the first action will be taken. If a module is intern-ed, it will be placed into the package. This action is your model code, or any related code you want to package. For example, if you are trying to package a ResNet fromtorchvision,\nyou will need to intern the module torchvision.models.resnet. On package import, when your packaged code tries to import an intern-ed module, Package Importer will look inside your package for that module.\nIf it can\u2019t find that module, an error will be raised. This ensures that eachPackage Importeris isolated from the loading environment\u2014even\nif you havemy_interned_moduleavailable in both your package and the loading environment,Package Importerwill only use the version in your\npackage. Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed. If a module is extern-ed, it will not be packaged. Instead, it will be added to a list of external dependencies for this package. You can find this\nlist on package_exporter.extern_modules. On package import, when time packaged code tries to import anextern-ed module,Package Importerwill use the default Python importer to find\nthat module, as if you didimportlib.import_module(\"my_externed_module\"). If it can\u2019t find that module, an error will be raised. In this way, you can depend on third-party libraries likenumpyandscipyfrom within your package without having to package them too. Warning: If any external library changes in a backwards-incompatible way, your package may fail to load. If you need long-term reproducibility\nfor your package, try to limit your use of extern.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do C extension modules and bytecode modules need to do to be interned?",
        "Y": "be mock-ed or extern-ed",
        "Z": "Note: Only Python source modules can be intern-ed. Other kinds of modules, like C extension modules and bytecode modules, will raise an error if\nyou attempt to intern them. These kinds of modules need to be mock-ed or extern-ed.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a good way to break up large files with unrelated functionality into smaller ones?",
        "Y": "Split up large files with unrelated functionality into smaller ones",
        "Z": "mockshould be used for code that you \u201cknow\u201d will not be needed in the loaded package, but you still want to available for use in non-packaged contents.\nFor example, initialization/configuration code, or code only used for debugging/training. Warning: In general,mockshould be used as a last resort. It introduces behavioral differences between packaged code and non-packaged code,\nwhich may lead to later confusion. Prefer instead to refactor your code to remove unwanted dependencies. The best way to manage dependencies is to not have dependencies at all! Often, code can be refactored to remove unnecessary dependencies. Here are some\nguidelines for writing code with clean dependencies (which are also generally good practices!): Include only what you use. Do not leave unused imports in our code. The dependency resolver is not smart enough to tell that they are indeed unused,\nand will try to process them. Qualify your imports. For example, instead of writing import foo and later usingfoo.bar.baz, prefer to writefromfoo.barimportbaz. This more\nprecisely specifies your real dependency (foo.bar) and lets the dependency resolver know you don\u2019t need all offoo. Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another. Patterns allow you to specify groups of modules with a convenient syntax. The syntax and behavior of patterns follows the Bazel/Buckglob(). A module that we are trying to match against a pattern is called a candidate. A candidate is composed of a list of segments separated by a\nseparator string, e.g.foo.bar.baz. A pattern contains one or more segments. Segments can be: A literal string (e.g.foo), which matches exactly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will any module that depends onutils need to do if yourutilsmodule contains a hodge-podge of unrelated",
        "Y": "pull in lots of unrelated dependencies",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be packaged independently of one another?",
        "Y": "define single-purpose modules",
        "Z": "Split up large files with unrelated functionality into smaller ones. If yourutilsmodule contains a hodge-podge of unrelated functionality, any module\nthat depends onutilswill need to pull in lots of unrelated dependencies, even if you only needed a small part of it. Prefer instead to define\nsingle-purpose modules that can be packaged independently of one another.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What makes it really easy to bind objects and run code at module-level scope?",
        "Y": "Python",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are bound to names in Python?",
        "Y": "functions and classes",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is introduced when you define an object at module scope with the intention of mutating it?",
        "Y": "mutable global state",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Python makes it really easy to bind objects and run code at what?",
        "Y": "module-level scope",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can reduce boilerplate, allow for open registration into tables, etc.?",
        "Y": "Mutable global state",
        "Z": "Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example: In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Mutable global state can cause complications when used what?",
        "Y": "withtorch.package",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What creates an independent environment for its contents?",
        "Y": "EveryPackage Importer",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can happen when modules are written in a way that assumes shared mutable global state?",
        "Y": "hard-to-debug errors",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Mutable global state can cause what when used withtorch.package?",
        "Y": "complications",
        "Z": "Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When modules are written in a way that assumes shared mutable global state, what can this behavior create?",
        "Y": "hard-to-debug errors",
        "Z": "EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example: In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Any class that you import from a Package Importerwill be what?",
        "Y": "a version of the class specific to that importer",
        "Z": "Python makes it really easy to bind objects and run code at module-level scope. This is generally fine\u2014after all, functions and classes are bound to\nnames this way. However, things become more complicated when you define an object at module scope with the intention of mutating it, introducing mutable\nglobal state. Mutable global state is quite useful\u2014it can reduce boilerplate, allow for open registration into tables, etc. But unless employed very carefully, it can\ncause complications when used withtorch.package. EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a class that you import from a Package Importer?",
        "Y": "MyClassandimport_MyClassarenot the same type",
        "Z": "EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example: In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are MyClassandimport_MyClassnot?",
        "Y": "same type",
        "Z": "In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What might you think it's okay to consider MyClassandimport_MyClass?",
        "Y": "the same class",
        "Z": "In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if Import_MyClassis coming from an older package with an entirely different implementation of MyClass?",
        "Y": "it\u2019s unsafe to consider them the same class",
        "Z": "EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example: In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does each importer have under the hood?",
        "Y": "each importer has a prefix that allows it to uniquely identify classes",
        "Z": "In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does each importer have that allows it to uniquely identify classes?",
        "Y": "a prefix",
        "Z": "EveryPackage Importercreates an independent environment for its contents. This is nice because it means we load multiple packages and ensure\nthey are isolated from each other, but when modules are written in a way that assumes shared mutable global state, this behavior can create hard-to-debug\nerrors. Any class that you import from a Package Importerwill be a version of the class specific to that importer. For example: In this example,MyClassandimport_MyClassarenot the same type. In this specific example,MyClassandimport_MyClasshave exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation whereimport_MyClassis coming from an\nolder package with an entirely different implementation ofMyClass\u2014 in that case, it\u2019s unsafe to consider them the same class. Under the hood, each importer has a prefix that allows it to uniquely identify classes:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is it called when you need to use a class instead of checking that it is of a given type?",
        "Y": "duck typing",
        "Z": "That means you should not expectisinstancechecks to work when one of the arguments if from a package and the other is not. If you need this\nfunctionality, consider the following options: Doing duck typing (just using the class instead of explicitly checking that it is of a given type).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a way to use class instead of explicitly checking that it is of a given type?",
        "Y": "duck typing",
        "Z": "Under the hood, each importer has a prefix that allows it to uniquely identify classes: That means you should not expectisinstancechecks to work when one of the arguments if from a package and the other is not. If you need this\nfunctionality, consider the following options: Doing duck typing (just using the class instead of explicitly checking that it is of a given type).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should not work when one of the arguments if from a package and the other is not?",
        "Y": "expectisinstancechecks",
        "Z": "That means you should not expectisinstancechecks to work when one of the arguments if from a package and the other is not. If you need this\nfunctionality, consider the following options: Doing duck typing (just using the class instead of explicitly checking that it is of a given type).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another option if you need the functionality of isinstancechecks?",
        "Y": "duck typing",
        "Z": "That means you should not expectisinstancechecks to work when one of the arguments if from a package and the other is not. If you need this\nfunctionality, consider the following options: Doing duck typing (just using the class instead of explicitly checking that it is of a given type).",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is done by using the class instead of explicitly checking that it is of a given type?",
        "Y": "duck typing",
        "Z": "Doing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can you do to make the typing relationship an explicit part of the class contract?",
        "Y": "Make the typing relationship an explicit part of the class contract",
        "Z": "Doing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What attribute can be added to a class contract?",
        "Y": "tagself.handler",
        "Z": "Doing duck typing (just using the class instead of explicitly checking that it is of a given type). Make the typing relationship an explicit part of the class contract. For example, you can add an attribute tagself.handler=\"handle_me_this_way\"and have client code check for the value ofhandlerinstead of checking the type directly.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What creates an independent, isolated environment for its modules and objects?",
        "Y": "EachPackage Importerinstance",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does eachPackage Importerinstance do?",
        "Y": "Modules in a package can only import other packaged modules, or modules markedextern",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How many Package Importerinstances can be used to load a single package?",
        "Y": "multiple",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can only import other packaged modules?",
        "Y": "Modules in a package",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If you use what to load a single package, you will get multiple independent environments that do not interact?",
        "Y": "multiple Package Importer instances",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importer implement?",
        "Y": "theimport_moduleand__import__methods",
        "Z": "EachPackage Importerinstance creates an independent, isolated environment for its modules and objects. Modules in a package can only import\nother packaged modules, or modules markedextern. If you use multiple Package Importer instances to load a single package, you will get\nmultiple independent environments that do not interact. This is achieved by extending Python\u2019s import infrastructure with a custom importer.Package Importerprovides the same core API as the import libimporter; namely, it implements theimport_moduleand__import__methods. When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does.import_module() invoke?",
        "Y": "Package Importer",
        "Z": "When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Package Importerpatches the returned module to?",
        "Y": "use self",
        "Z": "When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How does the returned module fulfill future import requests?",
        "Y": "by looking in the package",
        "Z": "When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What prefix does Package Importermangles add to the__name__and__file__ of all imported modules?",
        "Y": "a mangle",
        "Z": "To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of a package that becomestorch_package_0>.torchvision.models.res",
        "Y": "liketorchvision.models.resnet18",
        "Z": "To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What becomestorch_package_0>.torchvision/modules/resnet18.py?",
        "Y": "liketorchvision/models/resnet18.py",
        "Z": "When you invokePackage Importer.import_module(),Package Importerwill construct and return a new module, much as the system importer does.\nHowever,Package Importerpatches the returned module to use self(i.e. thatPackage Importerinstance) to fulfill future import\nrequests by looking in the package rather than searching the user\u2019s Python environment. To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a name that becomestorch_package_0>.torchvision.models.resnet18?",
        "Y": "liketorchvision.models.resnet18",
        "Z": "For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does name mangling help avoid?",
        "Y": "inadvertent punning",
        "Z": "For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can you find developer-facing details about mangling?",
        "Y": "consultmangling.mdintorch/package/",
        "Z": "To avoid confusion (\u201cis thisfoo.barobject the one from my package, or the one from my Python environment?\u201d),Package Importermangles the__name__and__file__of all imported modules, by adding a mangle prefixto them. For__name__, a name liketorchvision.models.resnet18becomes<torch_package_0>.torchvision.models.resnet18. For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a name for__file__?",
        "Y": "liketorchvision/models/resnet18.py",
        "Z": "For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What helps you debug by making it easier to see if a package is referring to a package or not?",
        "Y": "stack traces and print statements",
        "Z": "For__file__, a name liketorchvision/models/resnet18.pybecomes<torch_package_0>.torchvision/modules/resnet18.py. Name mangling helps avoid inadvertent punning of module names between different packages, and helps you debug by making stack traces and print\nstatements more clearly show whether they are referring to packaged code or not. For developer-facing details about mangling, consultmangling.mdintorch/package/.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When is this exception raised?",
        "Y": "when there is an issue with exporting a package",
        "Z": "This exception is raised when there is an issue with exporting a package.Package Exporterwill attempt to gather up all the errors and present\nthem to you at once. This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package. Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When a mock or extern is marked as what?",
        "Y": "allow_empty=False",
        "Z": "This exception is raised when there is an issue with exporting a package.Package Exporterwill attempt to gather up all the errors and present\nthem to you at once. This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of data can be picked up by exporters?",
        "Y": "Python",
        "Z": "This exception is raised when there is an issue with exporting a package.Package Exporterwill attempt to gather up all the errors and present\nthem to you at once. This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is marked when a mock or extern is not matched with any module during packaging?",
        "Y": "as allow_empty=False",
        "Z": "This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Exporters allow you to write packages of code, pickled Python data, and what else into a self-contained package?",
        "Y": "arbitrary binary and text resources",
        "Z": "This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When is this exception thrown?",
        "Y": "when a mock or extern is marked as allow_empty=False",
        "Z": "This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package. Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Exporters allow you to write packages of code, pickled Python data, and arbitrary binary and text resources into what?",
        "Y": "self-contained package",
        "Z": "This exception is raised when there is an issue with exporting a package.Package Exporterwill attempt to gather up all the errors and present\nthem to you at once. This is an exception that is thrown when a mock or extern is marked as allow_empty=False, and is not matched with any module during packaging. Exporters allow you to write packages of code, pickled Python data, and\narbitrary binary and text resources into a self-contained package. Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Imports can load code in what way?",
        "Y": "hermetic",
        "Z": "Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does this allow for the packaging of?",
        "Y": "PyTorch model code and data",
        "Z": "Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can PyTorch model code be used for in the future?",
        "Y": "transfer learning",
        "Z": "Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can load code in a hermetic way?",
        "Y": "Imports",
        "Z": "Imports can load this code in a hermetic way, such that code is loaded\nfrom the package rather than the normal Python import system. This allows\nfor the packaging of PyTorch model code and data so that it can be run\non a server or used in the future for transfer learning. The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The code contained in packages is copied file-by-file from what when it is created?",
        "Y": "the original source",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can future users of a package do to modify the code?",
        "Y": "unzip",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the file format of a package?",
        "Y": "specially organized zip file",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can future users of a package do with the code contained in a package?",
        "Y": "unzip",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are modules explicitly listed as?",
        "Y": "external usingextern()",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What lists all the modules that a package externally depends on?",
        "Y": "fileextern_modulesin the zip archive",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens when a package is copied to another machine?",
        "Y": "fails",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who ensures that code in the module can only be loaded from within the package?",
        "Y": "The importer",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What in the zip archive lists all the modules that a package externally depends on?",
        "Y": "fileextern_modules",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the exporter do when source code is added to a package?",
        "Y": "dependencies=True",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the exporter look for?",
        "Y": "import statements",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does f stand for?",
        "Y": "The location to export to",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be the location to export to?",
        "Y": "a string/Path object",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When an exporter adds what to a package, it can optionally scan it for further code dependencies?",
        "Y": "source code",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the exporter do when source code is added to the package?",
        "Y": "dependencies=True",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the location to export to?",
        "Y": "f\u2013 The location to export to",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can the location to export to be?",
        "Y": "a string/Path object",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the first step in creating an exporter?",
        "Y": "Create an exporter",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of object can contain a filename or a binary I/O object?",
        "Y": "a string/Path object",
        "Z": "f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can an importer search for if a single Importer is passed?",
        "Y": "modules",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be constructed out of a sequence of importers if a sequence of importers are passedsed?",
        "Y": "an Ordered Importer",
        "Z": "When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where can verbose print information about dependency resolution?",
        "Y": "stdout",
        "Z": "f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is verbose useful for?",
        "Y": "tracking down why certain files get included",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does an exporter do?",
        "Y": "Create an exporter",
        "Z": "Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an importer is passed, use that to search for modules.",
        "Y": "a single Importer",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a sequence of importers are passedsed, what will be constructed out of them?",
        "Y": "an Ordered Importer",
        "Z": "f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Print information about dependency resolution to stdout. Useful for tracking down why certain files get included?",
        "Y": "verbose",
        "Z": "f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is verbose useful for tracking down?",
        "Y": "why certain files get included",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a single Importer is passed, use that to search for what?",
        "Y": "modules",
        "Z": "f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can be a filename or a binary I/O object?",
        "Y": "a string/Path object",
        "Z": "f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a single Importer is passed, use that to search for modules.",
        "Y": "importer",
        "Z": "The code contained in packages is copied file-by-file from the original\nsource when it is created, and the file format is a specially organized\nzip file. Future users of the package can unzip the package, and edit the code\nin order to perform custom modifications to it. The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a single Importer is passed, use that to search for what s?",
        "Y": "module",
        "Z": "importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Where does verbose print information about dependency resolution?",
        "Y": "stdout",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a good way to track down why certain files get included?",
        "Y": "Write the package to the filesystem",
        "Z": "importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Any calls to what function are now invalid?",
        "Y": "afterclose()",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a better way to write a package to the filesystem?",
        "Y": "resource guard syntax",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does importer search for if a single Importer is passed?",
        "Y": "module",
        "Z": "importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Print information about dependency resolution to stdout. Useful for tracking down why certain files get included. Write the package to the filesystem.",
        "Y": "verbose",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you do to the filesystem?",
        "Y": "Write the package",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens when a package is written to the filesystem?",
        "Y": "Any calls afterclose()are now invalid",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is preferable to use instead of afterclose()?",
        "Y": "resource guard syntax",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does verbose print information about dependency resolution to?",
        "Y": "stdout",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does verbose do?",
        "Y": "Write the package to the filesystem",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the best way to blocklist modules?",
        "Y": "resource guard syntax",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is raised if a dependency on any matching packages is found?",
        "Y": "aPackagingErroris",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Print information about dependency resolution to stdout. Useful for tracking down why certain files get included.",
        "Y": "verbose",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is preferable to blocklist modules who names match the given glob patterns from the list of modules the package can import?",
        "Y": "resource guard syntax",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the package do?",
        "Y": "Write the package to the filesystem",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does the package do to the filesystem?",
        "Y": "Write the package",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if the package is written to the filesystem?",
        "Y": "Any calls afterclose()are now invalid",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What names match the given glob patterns from the list of modules the package can import?",
        "Y": "Blocklist modules",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does include(Union[List[str],str]) contain?",
        "Y": "string",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is another name for the names of the modules to be externed?",
        "Y": "list of strings",
        "Z": "Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]) be?",
        "Y": "glob-style pattern",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an optional pattern that excludes some patterns that match the include string?",
        "Y": "exclude",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do the names of blocklist modules match from the list of modules the package can import?",
        "Y": "glob patterns",
        "Z": "Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a string e.g. \"my_package.my_subpackage\"?",
        "Y": "include(Union[List[str],str])",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is include(Union[List[str],str])?",
        "Y": "list of strings for the names of the modules to be externed",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]] - A string for the names of the modules to be externed",
        "Y": "glob-style pattern",
        "Z": "The importer for packages ensures that code in the module can only be loaded from\nwithin the package, except for modules explicitly listed as external usingextern().\nThe fileextern_modulesin the zip archive lists all the modules that a package externally depends on.\nThis prevents \u201cimplicit\u201d dependencies where the package runs locally because it is importing\na locally-installed package, but then fails when the package is copied to another machine. When source code is added to the package, the exporter can optionally scan it\nfor further code dependencies (dependencies=True). It looks for import statements,\nresolves relative references to qualified module names, and performs an action specified by the user\n(See:extern(),mock(), andintern()). Create an exporter. f\u2013 The location to export to. Can be a string/Path object containing a filename\nor a binary I/O object. importer\u2013 If a single Importer is passed, use that to search for modules.\nIf a sequence of importers are passsed, an Ordered Importerwill be constructed out of them. verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of pattern can include(Union[List[str],str]) be?",
        "Y": "glob-style pattern",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is include(Union[List[str],str]) a string for the names of the modules to be externed?",
        "Y": "list of strings",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]] be?",
        "Y": "glob-style pattern",
        "Z": "verbose\u2013 Print information about dependency resolution to stdout.\nUseful for tracking down why certain files get included. Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does exclude(Union[List[str],str],str]) match?",
        "Y": "include string",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\ne.g.include='torch.**',exclude='torch.foo'will mock all torch packages except'torch.foo',\nDefault: is[].",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the list of external modules the package can import?",
        "Y": "Includemodulein",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will includemodulein prevent from saving it in the package?",
        "Y": "dependency discovery",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Who will load an external module directly from the standard import system?",
        "Y": "The importer",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must also exist in the process loading the package?",
        "Y": "Code",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does Includemodulein prevent from saving in the package?",
        "Y": "dependency discovery",
        "Z": "Write the package to the filesystem. Any calls afterclose()are now invalid.\nIt is preferable to use resource guard syntax instead: Blocklist modules who names match the given glob patterns from the list of modules the package can import.\nIf a dependency on any matching packages is found, aPackagingErroris raised. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does include(Union[List[str],str],str], or for the names of the modules to be externed",
        "Y": "list of strings",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str],str]) be?",
        "Y": "glob-style pattern",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Includemodulein will prevent what from saving it in the package?",
        "Y": "dependency discovery",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a glob-style pattern?",
        "Y": "include(Union[List[str],str])",
        "Z": "Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does exclude match?",
        "Y": "include string",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. Includemodulein the list of external modules the package can import.\nThis will prevent dependency discovery from saving\nit in the package. The importer will load an external module directly from the standard import system.\nCode for extern modules must also exist in the process loading the package. include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an optional flag that specifies whether the extern modules specified by this call to theexternmethod must be matched to some module during packaging?",
        "Y": "allow_empty",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When is an extern module glob pattern added?",
        "Y": "with allow_empty=False",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an extern module glob pattern is added with allow_empty=False, andclose()is called before any modules match that pattern,",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is guaranteed to only be handed out once for this package?",
        "Y": "Get an id",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How many times is this id guaranteed to be handed out for this package?",
        "Y": "once",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is thrown if an extern module glob pattern is added with allow_empty=False?",
        "Y": "an exception",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How many times is an id guaranteed to be handed out for this package?",
        "Y": "once",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How many times is an id guaranteed to be handed out for a package?",
        "Y": "once",
        "Z": "Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must match someinternpattern in order to be included in the package?",
        "Y": "modules that should be packaged",
        "Z": "Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must a module match in order to be included in the package?",
        "Y": "someinternpattern",
        "Z": "Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of pattern is described inmock()?",
        "Y": "glob-style pattern",
        "Z": "Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How many times is an id guaranteed to be given out for a package?",
        "Y": "once",
        "Z": "Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should be packaged?",
        "Y": "modules",
        "Z": "Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is a string e.g. \u201cmy_package.my_subpackage\u201d?",
        "Y": "include(Union[List[str],str])",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does include(Union[List[str],str]) stand for?",
        "Y": "list of strings",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What can include(Union[List[str],str]] \u2013 A string for the names of the modules to be externed",
        "Y": "glob-style pattern",
        "Z": "Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock().",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What type of pattern can be used to include a list of strings for the names of the modules to be externed?",
        "Y": "glob-style pattern",
        "Z": "include(Union[List[str],str]) \u2013 A string e.g.\"my_package.my_subpackage\", or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as\ndescribed inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the\ninclude string. allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What must be specified in order to be included in a package?",
        "Y": "modules that should be packaged",
        "Z": "Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does exclude(Union[List[str],str]] exclude some patterns that match?",
        "Y": "include string",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an optional flag that specifies whether the intern modules specified by this call to theinternmethod must be matched to some module during packaging?",
        "Y": "allow_empty",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What happens if an internmodule glob pattern is added with allow_empty=False?",
        "Y": "an exception is thrown",
        "Z": "Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If an internmodule glob pattern is added with allow_empty=False, andclose()is called before any modules match that",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What flag specifies whether the intern modules specified by this call to theinternmethod must be matched to some module during packaging?",
        "Y": "allow_empty(bool)",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is thrown if an internmodule glob pattern is added with allow_empty=False?",
        "Y": "an exception",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is added with allow_empty=False?",
        "Y": "an internmodule glob pattern",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will replace some required modules with?",
        "Y": "mock implementation",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Mocked modules will return what for any attribute accessed from it?",
        "Y": "fake object",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will sometimes find files that are imported by model files but whose functionality is never used?",
        "Y": "dependency resolution",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of a file that is imported by model files but never used?",
        "Y": "training helpers",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the function used to do?",
        "Y": "mock this functionality out without having to modify the original code",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the function that includes a mock implementation of a module?",
        "Y": "Union",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What do you replace some required modules with?",
        "Y": "mock implementation",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will a mock implementation return for any attribute accessed from it?",
        "Y": "fake object",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What are examples of files that are imported by model files but whose functionality is never used?",
        "Y": "custom serialization code or training helpers",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the extern modules specified by this call\nto theexternmethod must be matched to some module during packaging. If an extern module glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) before any modules match that pattern, an exception is thrown. If allow_empty=True,\nno such exception is thrown. Get an id. This id is guaranteed to only be handed out once for this package. Specify modules that should be packaged. A module must match someinternpattern in order to be\nincluded in the package and have its dependencies processed recursively. include(Union[List[str],str]) \u2013 A string e.g. \u201cmy_package.my_subpackage\u201d, or list of strings\nfor the names of the modules to be externed. This can also be a glob-style pattern, as described inmock(). exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string. allow_empty(bool) \u2013 An optional flag that specifies whether the intern modules specified by this call\nto theinternmethod must be matched to some module during packaging. If an internmodule glob\npattern is added with allow_empty=False, andclose()is called (either explicitly or via__exit__)\nbefore any modules match that pattern, an exception is thrown. If allow_empty=True, no such exception is thrown. Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does a mock implementation of a dependency resolution function not have to do?",
        "Y": "modify the original code",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the function used to mock functionality out without having to modify the original code?",
        "Y": "include(Union[List[str],str])",
        "Z": "Replace some required modules with a mock implementation.  Mocked modules will return a fake\nobject for any attribute accessed from it. Because we copy file-by-file, the dependency resolution will sometimes\nfind files that are imported by model files but whose functionality is never used\n(e.g. custom serialization code or training helpers).\nUse this function to mock this functionality out without having to modify the original code. include(Union[List[str],str]) \u2013",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the optional pattern that excludes some patterns that match the include string?",
        "Y": "'torch",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\ne.g.include='torch.**',exclude='torch.foo'will mock all torch packages except'torch.foo',\nDefault: is[].",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "Include='torch.', exclude='torch.foo' will mock all torch packages except'torch",
        "Y": "**",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\ne.g.include='torch.**',exclude='torch.foo'will mock all torch packages except'torch.foo',\nDefault: is[].",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an example of an optional pattern that excludes some patterns that match the include string?",
        "Y": "torch",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\ne.g.include='torch.**',exclude='torch.foo'will mock all torch packages except'torch.foo',\nDefault: is[].",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will mock all torch packages except'torch.foo'?",
        "Y": "torch.foo",
        "Z": "exclude(Union[List[str],str]) \u2013 An optional pattern that excludes some patterns that match the include string.\ne.g.include='torch.**',exclude='torch.foo'will mock all torch packages except'torch.foo',\nDefault: is[].",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is an optional flag that specifies whether the mock implementation(s) specified by this call to themock()method must be matched to some module",
        "Y": "allow_empty(bool)",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When is a mock added?",
        "Y": "with allow_empty=False",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a mock has not been matched to a module used by the package being exported, an exception is thrown?",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does it do when a mock is not matched to a module used by the package being exported?",
        "Y": "Registers an extern hook on the exporter",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is thrown if the mock has not been matched to a module used by the package being exported?",
        "Y": "an exception",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "If a mock is added with allow_empty=False, andclose()is called and no such exception is thrown?",
        "Y": "If allow_empty=True",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does register on the exporter?",
        "Y": "an extern hook",
        "Z": "allow_empty(bool) \u2013 An optional flag that specifies whether the mock implementation(s) specified by this call\nto themock()method must be matched to some module during packaging. If a mock is added with allow_empty=False, andclose()is called (either explicitly or via__exit__) and the mock has\nnot been matched to a module used by the package being exported, an exception is thrown.\nIf allow_empty=True, no such exception is thrown. Registers an extern hook on the exporter.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.utils.hooks.RemovableHandle do?",
        "Y": "Save the code formoduleinto the package",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will the hook be called?",
        "Y": "each time a module matches against anextern()pattern",
        "Z": "The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What will be called in order of registration?",
        "Y": "Hooks",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How can a handle be used to remove the added hook?",
        "Y": "callinghandle.remove()",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What registers an intern hook on the exporter?",
        "Y": "torch.utils.hooks.RemovableHandle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will the intern hook be called?",
        "Y": "each time a module matches against an intern()pattern",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should an intern hook have?",
        "Y": "signature",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will the hook be called on the exporter?",
        "Y": "each time a module matches against an intern()pattern",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How are hooks called?",
        "Y": "in order of registration",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The hook will be called each time a module matches against what?",
        "Y": "an intern()pattern",
        "Z": "The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should the hook have when a module matches against an intern()pattern?",
        "Y": "signature",
        "Z": "Registers an extern hook on the exporter. The hook will be called each time a module matches against anextern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How can a handle be used to remove an added hook?",
        "Y": "handle.remove()",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.utils.hooks.RemovableHandle register?",
        "Y": "an intern hook on the exporter",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the handle that can be used to remove the added hook?",
        "Y": "callinghandle.remove()",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What is the name of the handle that can be used to remove the added hook?",
        "Y": "torch.utils.hooks.RemovableHandle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.utils.hooks.RemovableHandle register on the exporter?",
        "Y": "mock hook",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "A handle that can be used to remove the added hook by calling what?",
        "Y": "handle.remove()",
        "Z": "torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The mock hook is called each time a module matches against what?",
        "Y": "amock()pattern",
        "Z": "torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should a mock hook have?",
        "Y": "signature",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When is the mock hook called on the exporter?",
        "Y": "each time a module matches against amock()pattern",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What registers a mock hook on the exporter?",
        "Y": "torch.utils.hooks.RemovableHandle",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "The mock hook will be called each time a module matches against what?",
        "Y": "amock()pattern",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What should the mock hook have?",
        "Y": "signature",
        "Z": "Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature:",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will the mock hook be called?",
        "Y": "each time a module matches against amock()pattern",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does torch.utils.hooks.RemovableHandle save to the package?",
        "Y": "raw bytes",
        "Z": "Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "When will the mock hook on the exporter be called?",
        "Y": "each time a module matches against amock()pattern",
        "Z": "A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers an intern hook on the exporter. The hook will be called each time a module matches against an intern()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Registers a mock hook on the exporter. The hook will be called each time a module matches against amock()pattern.\nIt should have the following signature: Hooks will be called in order of registration. A handle that can be used to remove the added hook by callinghandle.remove(). torch.utils.hooks.RemovableHandle Save raw bytes to the package. package(str) \u2013 The name of module package this resource should go it (e.g.\"my_package.my_subpackage\"). resource(str) \u2013 A unique name for the resource, used to identify it to load. binary(str) \u2013 The data to save. Save the code formoduleinto the package. Code for the module is resolved using the importers path to find the\nmodule object, and then using its__file__attribute to find the source code. module_name(str) \u2013 e.g.my_package.my_subpackage, code will be saved to provide code\nfor this package. dependencies(bool,optional) \u2013 If True, we scan the source for dependencies. Save a python object to the archive using pickle. Equivalent totorch.save()but saving into\nthe archive rather than a stand-alone file. Stanard pickle does not save the code, only the objects.\nIfdependenciesis true, this method will also scan the pickled objects for which modules are required\nto reconstruct them and save the relevant code. To be able to save an object wheretype(obj).__name__ismy_module.MyObject,my_module.MyObjectmust resolve to the class of the object according to theimporterorder. When saving objects that\nhave previously been packaged, the importer\u2019simport_modulemethod will need to be present in theimporterlist\nfor this to work.",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "What does input return?",
        "Y": "the cumulative product of elements ofinputin the dimensiondim",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "What returns the cumulative sum of elements of inputin the dimensiondim?",
        "Y": "If input is",
        "Z": "Returns the cumulative sum of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum"
    },
    {
        "X": "What is the dimension to do the operation over dtype?",
        "Y": "dim(int)",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "If inputis a what, the result will also be a vector of size N, with elements?",
        "Y": "vector of size N",
        "Z": "Returns the cumulative product of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "What is dim(int)?",
        "Y": "the dimension to do the operation over dtype(torch.dtype, optional)",
        "Z": "Returns the cumulative sum of elements ofinputin the dimensiondim. For example, If input is a vector of size N, the result will also be\na vector of size N, with elements. input(Tensor) \u2013 the input tensor. dim(int) \u2013 the dimension to do the operation over dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nIf specified, the input tensor is casted todtypebefore the operation\nis performed. This is useful for preventing data type overflows. Default: None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum"
    },
    {
        "X": "If a matrix on the left is multiplyed by a negative power as what?",
        "Y": "ifn> 0",
        "Z": "Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\na negative power as, ifn> 0: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What computesA.inv() @Bwith a numerically stable algorithm?",
        "Y": "torch.linalg.solve()",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 ifn< 0and the matrixAor any matrix in the\n    batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What is the tensor of shape(*, m, m)where*is zero or more batch dimensions?",
        "Y": "A(Tensor)",
        "Z": "Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\na negative power as, ifn> 0: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What does n(int) represent?",
        "Y": "exponent",
        "Z": "Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\na negative power as, ifn> 0: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What is ignored when using torch.linalg.solve()?",
        "Y": "if None",
        "Z": "Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\na negative power as, ifn> 0: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What is the default value of torch.linalg.solve()?",
        "Y": "Default:None",
        "Z": "Note Consider usingtorch.linalg.solve()if possible for multiplying a matrix on the left by\na negative power as, ifn> 0: It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "Why is it always preferable to usesolve() when possible?",
        "Y": "faster and more numerically stable",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is A(Tensor)?",
        "Y": "tensor of shape",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computing the inverse explicitly. See also torch.linalg.pinv()computes the pseudoinverse (Moore-Penrose inverse) of matrices\nof any shape. torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, n, n)where*is zero or more batch dimensions\nconsisting of invertible matrices. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None.",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "What is the default value of if None?",
        "Y": "Default:None",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 ifn< 0and the matrixAor any matrix in the\n    batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "ifn 0 and any matrix in the batch of matricesAis what?",
        "Y": "not invertible",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 ifn< 0and the matrixAor any matrix in the\n    batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What is an example of a matrix that is not invertible?",
        "Y": "Examples",
        "Z": "It is always prefered to usesolve()when possible, as it is faster and more\nnumerically stable than computingA\u2212nA^{-n}A\u2212nexplicitly. See also torch.linalg.solve()computesA.inv() @Bwith a\nnumerically stable algorithm. A(Tensor) \u2013 tensor of shape(*, m, m)where*is zero or more batch dimensions. n(int) \u2013 the exponent. out(Tensor,optional) \u2013 output tensor. Ignored if None. Default:None. RuntimeError\u2013 ifn< 0and the matrixAor any matrix in the\n    batch of matricesAis not invertible. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "What type of tensor is created when the values are evenly spaced from starttoend?",
        "Y": "one-dimensional",
        "Z": "Create a view of an existingtorch.Tensorinputwith specifiedsize,strideandstorage_offset.   Creates aTensorfrom anumpy.ndarray.   Returns a tensor filled with the scalar value0, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value0, with the same size as input.   Returns a tensor filled with the scalar value1, with the shape defined by the variable argumentsize.   Returns a tensor filled with the scalar value1, with the same size as input.   Returns a 1-D tensor of size\u2308end\u2212startstep\u2309\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil\u2308stepend\u2212start\u200b\u2309with values from the interval[start,end)taken with common differencestepbeginning from start.   Returns a 1-D tensor of size\u230aend\u2212startstep\u230b+1\\left\\lfloor \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rfloor + 1\u230a stepend\u2212start\u200b\u230b+1with values from starttoendwith stepstep.   Creates a one-dimensional tensor of sizestepswhose values are evenly spaced from starttoend, inclusive.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the index of the new tensor that indexes the input tensor along dimensiondimusing the entries inindex?",
        "Y": "a Long Tensor",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Which tensor indexes the input tensor according to the boolean maskmask?",
        "Y": "1-D",
        "Z": "Stack tensors in sequence depthwise (along third axis).   Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of a tensor that indexes the input tensor along dimensiondimusing the entries inindex?",
        "Y": "Stack",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of the tensor that indexes the input tensor along dimensiondimusing the entries inindex?",
        "Y": "a Long Tensor",
        "Z": "Gathers values along an axis specified bydim.   Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What type of tensor indexes the input tensor according to the boolean maskmask?",
        "Y": "1-D",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of the new tensor that indexes the input tensor along dimensiondimusing the entries inindex?",
        "Y": "a Long Tensor",
        "Z": "Splitsinput, a tensor with one or more dimensions, into multiple tensors horizontally according toindices_or_sections.   Stack tensors in sequence horizontally (column wise).   Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns a new tensor which indexes the input tensor according to the boolean maskmask which is",
        "Y": "1-D",
        "Z": "Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the name of the index that returns a new tensor that indexes the input tensor along dimensiondimusing",
        "Y": "a Long Tensor",
        "Z": "Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns a new 1-D tensor which indexes the input tensor according to what boolean maskmask",
        "Y": "a Bool Tensor",
        "Z": "Returns a new tensor which indexes the input tensor along dimensiondimusing the entries inindexwhich is a Long Tensor.   Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns a new tensor that indexes the input tensor according to what boolean maskmask?",
        "Y": "1-D",
        "Z": "Returns a new 1-D tensor which indexes the input tensor according to the boolean maskmaskwhich is a Bool Tensor.   Moves the dimension(s) ofinputat the position(s) insourceto the position(s) indestination.   Alias fortorch.movedim().   Returns a new tensor that is a narrowed version ofinputtensor.      Returns a tensor with the same data and number of elements as input, but with the specified shape.   Alias oftorch.vstack().   Out-of-place version oftorch.Tensor.scatter_()   Out-of-place version oftorch.Tensor.scatter_add_()   Splits the tensor into chunks.   Returns a tensor with all the dimensions ofinputof size1removed.   Concatenates a sequence of tensors along a new dimension.   Alias fortorch.transpose().   Alias fortorch.transpose().   Expectsinputto be <= 2-D tensor and transposes dimensions 0 and 1.   Returns a new tensor with the elements ofinputat the given indices.   Selects values frominputat the 1-dimensional indices fromindicesalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does a torch.ByteTensor set?",
        "Y": "random number generator state",
        "Z": "Returns the initial seed for generating random numbers as a Pythonlong.   Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution torch.Tensor.log_normal_()- samples from the log-normal distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does a torch.ByteTensor do?",
        "Y": "Sets the random number generator state",
        "Z": "Returns the random number generator state as a torch.ByteTensor.   Sets the random number generator state.   Draws binary random numbers (0 or 1) from a Bernoulli distribution.   Returns a tensor where each row containsnum_samplesindices sampled from the multinomial probability distribution located in the corresponding row of tensorinput.   Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?",
        "Y": "exclusive",
        "Z": "Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.   Returns a tensor of the same size as inputwith each element sampled from a Poisson distribution with rate parameter given by the corresponding element ininputi.e.,   Returns a tensor filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1)   Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Random integers are generated uniformly betweenlow(inclusive) and high(exclusive)?",
        "Y": "exclusive",
        "Z": "Returns a tensor with the same size as inputthat is filled with random numbers from a uniform distribution on the interval[0,1)[0, 1)[0,1).   Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive)?",
        "Y": "tensor",
        "Z": "Returns a tensor filled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor with the same shape as Tensorinputfilled with random integers generated uniformly betweenlow(inclusive) and high(exclusive).   Returns a tensor filled with random numbers from a normal distribution with mean0and variance1(also called the standard normal distribution).   Returns a tensor with the same size as inputthat is filled with random numbers from a normal distribution with mean 0 and variance 1.   Returns a random permutation of integers from0ton-1. There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation: torch.Tensor.bernoulli_()- in-place version oftorch.bernoulli() torch.Tensor.cauchy_()- numbers drawn from the Cauchy distribution torch.Tensor.exponential_()- numbers drawn from the exponential distribution torch.Tensor.geometric_()- elements drawn from the geometric distribution",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does return the maximum value of each slice of the input tensor in the given dimensiondim?",
        "Y": "the minimum value of each slice of the input tensor in the given dimension(s)dim",
        "Z": "Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns what value of each slice of the input tensor in the given dimension(s)dim?",
        "Y": "minimum value",
        "Z": "Returns the indices of the maximum value of all elements in the input tensor.   Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the variance.   Counts the number of non-zero values in the tensorinputalong the givendim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.",
        "Y": "p-norm",
        "Z": "Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does ignoring NaN values return?",
        "Y": "the median of the values ininput",
        "Z": "Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns what value ignoring NaN values?",
        "Y": "the median of the values ininput",
        "Z": "Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned ignoring NaN values?",
        "Y": "the median of the values ininput",
        "Z": "the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does the input tensor return ignoring NaN values?",
        "Y": "the median of the values ininput",
        "Z": "the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What value of all elements in the input tensor is returned?",
        "Y": "maximum value",
        "Z": "Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the log of summed exponentials of each row of the input tensor in the given dimensiondim?",
        "Y": "p-norm",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned, ignoring NaN values?",
        "Y": "the median of the values ininput",
        "Z": "Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.   Returns the unique elements of the input tensor.   Eliminates all but the first element from every consecutive group of equivalent elements.   Ifunbiasedis True, Bessel\u2019s correction will be used.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is the mode value of each row of the input tensor in the given dimensiondim?",
        "Y": "a named tuple",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What returns the log of summed exponentials of each row of the input tensor in the given dimensiondim?",
        "Y": "the p-norm of (input-other) Returns the log of summed exponentials",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns what of the values ininput, ignoring NaN values?",
        "Y": "the median",
        "Z": "Returns the indices of the minimum value(s) of the flattened tensor or along a dimension   Returns the maximum value of each slice of the input tensor in the given dimension(s)dim.   Returns the minimum value of each slice of the input tensor in the given dimension(s)dim.   Tests if all elements ininputevaluate toTrue.    the input tensor.   Returns the maximum value of all elements in the input tensor.   Returns the minimum value of all elements in the input tensor.   Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What returns the mode value of each row of the input tensor in the given dimensiondim?",
        "Y": "a named tuple(values,indices)",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim?",
        "Y": "p-norm",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does return the log of summed exponentials of each row of the input tensor in the given dimensiondim?",
        "Y": "the p-norm",
        "Z": "Returns the p-norm of (input-other)   Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "Returns what of each row of the input tensor in the given dimensiondim?",
        "Y": "the log of summed exponentials",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What does it return, ignoring NaN values?",
        "Y": "the median of the values ininput",
        "Z": "Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What are the quantiles of each row of the input tensor along the dimensiondim?",
        "Y": "q-th",
        "Z": "Returns the log of summed exponentials of each row of the input tensor in the given dimensiondim.   Returns the mean value of all elements in the input tensor.   Returns the median of the values ininput.   Returns the median of the values ininput, ignoring NaN values.   Returns a named tuple(values,indices)wherevaluesis the mode value of each row of the input tensor in the given dimensiondim, i.e.   Returns the matrix norm or vector norm of a given tensor.   Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.   Returns the product of all elements in the input tensor.   Computes the q-th quantiles of each row of the input tensor along the dimensiondim.   This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues, computing the quantilesqas ifNaNvalues ininputdid not exist.   Ifunbiasedis True, Bessel\u2019s correction will be used.   Ifunbiasedis True, Bessel\u2019s correction will be used to calculate the standard deviation.   Returns the sum of all elements in the input tensor.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned when a named tuple(values,indices) returns the cumulative minimum of elements ofinputin the dimensiondim",
        "Y": "cumulative product of elements ofinputin the dimensiondim",
        "Z": "Returns a copy ofinput.   Compute combinations of lengthrrrof the given tensor.   Returns the cross product of vectors in dimensiondimofinputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is returned when a named tuple(values,indices) returns the cumulative product of elements ofinputin the dimensiondim",
        "Y": "cumulative sum of elements ofinputin the dimensiondim",
        "Z": "Returns the cross product of vectors in dimensiondimofinputandother.   Returns a named tuple(values,indices)wherevaluesis the cumulative maximum of elements ofinputin the dimensiondim.   Returns a named tuple(values,indices)wherevaluesis the cumulative minimum of elements ofinputin the dimensiondim.   Returns the cumulative product of elements ofinputin the dimensiondim.   Returns the cumulative sum of elements ofinputin the dimensiondim.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Creates a tensor whose diagonals of certain 2D planes (specified bydim1anddim2) are filled byinput.    If input is a vector (1-D tensor), then returns a 2-D square tensor   Returns a partial view ofinputwith the its diagonal elements with respect todim1anddim2appended as a dimension at the end of the shape.   Computes the n-th forward difference along the given dimension.",
        "source": "https://pytorch.org/docs/stable/torch.html#tensors"
    },
    {
        "X": "What is a tool that allows the collecton of performance metrics during training and inference?",
        "Y": "PyTorch Profiler",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What can be used to better understand what model operators are the most expensive?",
        "Y": "context manager API",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "An earlier version of the API intorch.autogradmodule is considered what?",
        "Y": "legacy",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the API used by PyTorch Profiler?",
        "Y": "context manager",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a tool that allows the collecton of the performance metrics during the training and inference?",
        "Y": "PyTorch Profiler",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What API can be used to better understand what model operators are the most expensive?",
        "Y": "context manager",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What API is considered legacy and will be deprecated?",
        "Y": "API intorch.autogradmodule",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the API that will be deprecated?",
        "Y": "Profiler context manager",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does activities(iterable) list of to use in profiling?",
        "Y": "activity groups",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the default value of Profiler context manager?",
        "Y": "ProfilerActivity.CPU",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "An earlier version of the API is considered legacy and will be deprecated.",
        "Y": "intorch.autogradmodule",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the module that is used for profiling?",
        "Y": "Profiler context manager",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Activities(iterable) \u2013 what does activities(iterable) stand for?",
        "Y": "list of activity groups",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is considered legacy and will be deprecated?",
        "Y": "intorch.autogradmodule",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is an older version of intorch.autogradmodule considered legacy and will be deprecated?",
        "Y": "Profiler context manager",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the profiler that is used for profiling?",
        "Y": "Profiler context manager",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a callable that takes step (int) as a single parameter and returnsProfilerActionvalue?",
        "Y": "schedule",
        "Z": "activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the profiler context manager?",
        "Y": "Profiler context manager",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the list of activity groups to use in profiling?",
        "Y": "activities(iterable)",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the default value of profiler context manager?",
        "Y": "ProfilerActivity.CPU",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the callable that takes step (int) as a single parameter and returns ProfilerActionvalue?",
        "Y": "schedule(callable)",
        "Z": "An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Activities(iterable) \u2013 list of what to use in profiling?",
        "Y": "activity groups",
        "Z": "Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution).",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the default value of ProfilerActivity?",
        "Y": "ProfilerActivity.CPU",
        "Z": "activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the default value for profiling?",
        "Y": "ProfilerActivity.CPU",
        "Z": "activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a callable that takes step as a single parameter and returnsProfilerActionvalue?",
        "Y": "schedule",
        "Z": "schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When is on_trace_ready(callable) called?",
        "Y": "whenschedulereturnsProfilerAction",
        "Z": "schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What saves information about operator's input shapes?",
        "Y": "record_shapes",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the callable that takes step (int) as a single parameter and returnsProfilerActionvalue?",
        "Y": "schedule",
        "Z": "schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the callable that is called at each step whenschedulereturnsProfilerAction?",
        "Y": "on_trace_ready",
        "Z": "schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does record_shapes do?",
        "Y": "save information about operator\u2019s input shapes",
        "Z": "record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When is on_trace_ready(callable) called at each step?",
        "Y": "whenschedulereturnsProfilerAction",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What saves information about operator input shapes?",
        "Y": "record_shapes",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does profile_memory(bool) do?",
        "Y": "track tensor memory allocation/deallocation",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does with_stack(bool) do?",
        "Y": "record source information",
        "Z": "with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a formula to estimate the FLOPS of specific operators?",
        "Y": "with_flops",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the callable that is called at each step whenschedulereturnsProfilerAction.RECORD_",
        "Y": "on_trace_ready(callable)",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_S",
        "Y": "on_trace_ready",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is used to track tensor memory allocation/deallocation?",
        "Y": "profile_memory",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is used to estimate the FLOPS of specific operators?",
        "Y": "with_flops",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When was use_cuda deprecated?",
        "Y": "1.8.1",
        "Z": "use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a bool that tracks tensor memory allocation/deallocation?",
        "Y": "profile_memory",
        "Z": "profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When was use_cuda(bool) deprecated?",
        "Y": "1.8.1",
        "Z": "with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is deprecated since version 1.8.1?",
        "Y": "use_cuda(bool)",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the function that generates the callable schedule?",
        "Y": "Useschedule()",
        "Z": "Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process?",
        "Y": "Non-default schedules",
        "Z": "Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What simply records all the events continuously for the duration of the context manager?",
        "Y": "The default schedule",
        "Z": "Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Usetensorboard_trace_handler() generate result files for?",
        "Y": "TensorBoard",
        "Z": "PyTorch Profiler is a tool that allows the collecton of the performance metrics during the training and inference.\nProfiler\u2019s context manager API can be used to better understand what model operators are the most expensive,\nexamine their input shapes and stack traces, study device kernel activity and visualize the execution trace. Note An earlier version of the API intorch.autogradmodule is considered legacy and will be deprecated. Profiler context manager. activities(iterable) \u2013 list of activity groups (CPU, CUDA) to use in profiling, supported values:torch.profiler.ProfilerActivity.CPU,torch.profiler.ProfilerActivity.CUDA.\nDefault value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA. schedule(callable) \u2013 callable that takes step (int) as a single parameter and returnsProfilerActionvalue that specifies the profiler action to perform at each step. on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the function that is deprecated since version 1.8.1?",
        "Y": "use_cuda",
        "Z": "use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is used to generate the callable schedule?",
        "Y": "Useschedule()",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When are non-default schedules useful?",
        "Y": "when profiling long training jobs",
        "Z": "Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What version of TensorBoard was deprecated since version 1.8.1?",
        "Y": "1.8.1:useactivities",
        "Z": "Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the result file generated by Usetensorboard_trace_handler()?",
        "Y": "on_trace_ready",
        "Z": "Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What generates the callable schedule?",
        "Y": "Useschedule()",
        "Z": "use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Usetensorboard_trace_handler() to generate result files for what?",
        "Y": "TensorBoard",
        "Z": "on_trace_ready(callable) \u2013 callable that is called at each step whenschedulereturnsProfilerAction.RECORD_AND_SAVEduring the profiling. record_shapes(bool) \u2013 save information about operator\u2019s input shapes. profile_memory(bool) \u2013 track tensor memory allocation/deallocation. with_stack(bool) \u2013 record source information (file and line number) for the ops. with_flops(bool) \u2013 use formula to estimate the FLOPS of specific operators\n(matrix multiplication and 2D convolution). use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Usetensorboard_trace_handler() do to generate result files for TensorBoard?",
        "Y": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "Z": "use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What command is used to see the results in TensorBoard?",
        "Y": "tensorboard--logdirdir_name",
        "Z": "After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Enabling shape and stack tracing results in what?",
        "Y": "additional overhead",
        "Z": "Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What functions can be used to generate results for TensorBoard?",
        "Y": "profiler\u2019sschedule,on_trace_readyandstepfunctions",
        "Z": "Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Where can result files be found after profiling?",
        "Y": "specified directory",
        "Z": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are some examples of how to enable shape and stack tracing?",
        "Y": "profiler\u2019sschedule,on_trace_readyandstepfunctions",
        "Z": "Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is used to generate result files for TensorBoard?",
        "Y": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "Z": "Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What functions are used to generate result files for TensorBoard?",
        "Y": "profiler\u2019sschedule,on_trace_readyandstepfunctions",
        "Z": "Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are some examples of tracing functions?",
        "Y": "profiler\u2019sschedule,on_trace_readyandstepfunctions",
        "Z": "Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does on_trace_ready mean?",
        "Y": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)",
        "Z": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler'sschedule add into the trace file?",
        "Y": "a user defined metadata with a string key and a string value",
        "Z": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler'sschedule,on_trace_readyandstepfunctions add to the trace file?",
        "Y": "a string key and a string value",
        "Z": "After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What can be found in the specified directory after profiling?",
        "Y": "result files",
        "Z": "After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Using the profiler'sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with",
        "Y": "trace file",
        "Z": "After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the command to see the results in TensorBoard?",
        "Y": "tensorboard",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler'sschedule,on_trace_readyandstepfunctions add?",
        "Y": "a user defined metadata with a string key and a valid json value into the trace file",
        "Z": "to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the program that displays the results in TensorBoard?",
        "Y": "tensorboard",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler'sschedule,on_trace_readyandstepfunctions add into the trace file?",
        "Y": "a user defined metadata with a string key and a string value",
        "Z": "use_cuda(bool) \u2013 Deprecated since version 1.8.1:useactivitiesinstead. Note Useschedule()to generate the callable schedule.\nNon-default schedules are useful when profiling long training jobs\nand allow the user to obtain multiple traces at the different iterations\nof the training process.\nThe default schedule simply records all the events continuously for the\nduration of the context manager. Note Usetensorboard_trace_handler()to generate result files for TensorBoard: on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "To see the results in what?",
        "Y": "TensorBoard",
        "Z": "to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Exports the collected trace in what format?",
        "Y": "Chrome JSON",
        "Z": "Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What results in additional overhead?",
        "Y": "Enabling shape and stack tracing",
        "Z": "Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When is the list of unaggregated profiler events used?",
        "Y": "trace callback",
        "Z": "Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the profiler'sschedule used on?",
        "Y": "trace",
        "Z": "Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "In what format can stack traces be saved?",
        "Y": "a format suitable for visualization",
        "Z": "Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location;",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Save stack traces in a file in a format suitable for what?",
        "Y": "visualization",
        "Z": "tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What saves stacks file to this location?",
        "Y": "path(str)",
        "Z": "path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the file to save stacks file to?",
        "Y": "path(str)",
        "Z": "Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What format can stack traces be saved in?",
        "Y": "a format suitable for visualization",
        "Z": "Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is an example of using?",
        "Y": "FlameGraph tool",
        "Z": "on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name) After profiling, result files can be found in the specified directory. Use the command: tensorboard--logdirdir_name to see the results in TensorBoard.\nFor more information, seePyTorch Profiler TensorBoard Plugin Note Enabling shape and stack tracing results in additional overhead. Examples: Using the profiler\u2019sschedule,on_trace_readyandstepfunctions: Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool:",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the metric to use?",
        "Y": "self_cpu_time_total",
        "Z": "metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is an example of using FlameGraph tool?",
        "Y": "git clone",
        "Z": "Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does self_cpu_time_total stand for?",
        "Y": "metric",
        "Z": "Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What format does FlameGraph export the collected trace in?",
        "Y": "Chrome JSON",
        "Z": "Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What format should stack traces be saved in?",
        "Y": "a format suitable for visualization",
        "Z": "Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "path(str) \u2013 save what file to this location?",
        "Y": "stacks file",
        "Z": "path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "To use shape/stack functionality make sure to set what when creating profiler context manager?",
        "Y": "record_shapes/with_stack",
        "Z": "Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Set record_shapes/with_stack when creating what?",
        "Y": "profiler context manager",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does it do when a profiler context manager is created?",
        "Y": "Signals the profiler that the next profiling step has started",
        "Z": "Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "How are events grouped?",
        "Y": "by operator name and (optionally) input shapes and stack",
        "Z": "cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When setting record_shapes/with_stack make sure to set record_shapes/with_stack when",
        "Y": "profiler context manager",
        "Z": "Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler do when setting record_shapes/with_stack?",
        "Y": "Signals the profiler that the next profiling step has started",
        "Z": "Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does Brendangregg do?",
        "Y": "git clone",
        "Z": "git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler do when the next profiling step has started?",
        "Y": "Signals the profiler that the next profiling step has started",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are members of the profiler?",
        "Y": "CPU CUDA",
        "Z": "Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What Signals the profiler that the next profiling step has started?",
        "Y": "Signals the profiler that the next profiling step has started",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a member of a profiler?",
        "Y": "CPU CUDA",
        "Z": "cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the file that records CPU time?",
        "Y": "cd FlameGraph",
        "Z": "cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the members of the profiler?",
        "Y": "CPU CUDA",
        "Z": "./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "How are events grouped in profiler.stacks?",
        "Y": "by operator name",
        "Z": "cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the countname of the CPU time?",
        "Y": "us",
        "Z": "./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does perf_viz.svg group events by operator name and (optionally) input shapes and stack?",
        "Y": "profiler.stacks",
        "Z": "./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a member of the profiler that can be taken at the specified intervals?",
        "Y": "CPU CUDA",
        "Z": "Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler group by operator name and input shapes and stack?",
        "Y": "Averages events",
        "Z": "Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the members of the profiler that can be taken at the specified intervals?",
        "Y": "CPU CUDA",
        "Z": "metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What events are grouped by operator name and (optionally) input shapes and stack?",
        "Y": "Averages",
        "Z": "Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What signals the profiler that the next profiling step has started?",
        "Y": "Signals the profiler that the next profiling step has started",
        "Z": "Adds a user defined metadata with a string key and a string value\ninto the trace file Adds a user defined metadata with a string key and a valid json value\ninto the trace file Returns the list of unaggregated profiler events,\nto be used in the trace callback or after the profiling is finished Exports the collected trace in Chrome JSON format. Save stack traces in a file in a format suitable for visualization. path(str) \u2013 save stacks file to this location; metric(str) \u2013 metric to use: \u201cself_cpu_time_total\u201d or \u201cself_cuda_time_total\u201d Note Example of using FlameGraph tool: git clonehttps://github.com/brendangregg/FlameGraph cd FlameGraph ./flamegraph.pl \u2013title \u201cCPU time\u201d \u2013countname \u201cus.\u201d profiler.stacks > perf_viz.svg Averages events, grouping them by operator name and (optionally) input shapes and\nstack. Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What do you need to set to use shape/stack functionality?",
        "Y": "record_shapes/with_stack",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does this do to the profiler?",
        "Y": "Signals the profiler that the next profiling step has started",
        "Z": "To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a member of the profiler?",
        "Y": "CPU CUDA",
        "Z": "Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What do profiler context managers need to set to use shape/stack functionality?",
        "Y": "record_shapes/with_stack",
        "Z": "To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "When creating what do you need to set record_shapes/with_stack?",
        "Y": "profiler context manager",
        "Z": "To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is a member of Profiler actions that can be taken at the specified intervals?",
        "Y": "CPU CUDA",
        "Z": "To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What can be taken at the specified intervals?",
        "Y": "Profiler actions",
        "Z": "Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are actions that can be taken at the specified intervals?",
        "Y": "Profiler",
        "Z": "Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are the members of the Profiler that can be taken at the specified intervals?",
        "Y": "CPU CUDA",
        "Z": "Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What is the name of the person who can take actions at specified intervals?",
        "Y": "Profiler",
        "Z": "Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What are Profiler actions that can be taken at specified intervals Members?",
        "Y": "CPU CUDA",
        "Z": "Profiler actions that can be taken at the specified intervals Members: CPU CUDA",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What returns a callable that can be used as profiler schedule argument?",
        "Y": "CPU CUDA",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does the profiler do after the firstskip_firststeps?",
        "Y": "wait for waitsteps",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What specifies the optional number of cycles?",
        "Y": "the repeat parameter",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Members: CPU CUDA Returns a callable that can be used as what?",
        "Y": "profiler schedule argument",
        "Z": "Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "Who will skip the firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps",
        "Y": "The profiler",
        "Z": "Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What value means that the cycles will continue until the profiling is finished?",
        "Y": "the zero value",
        "Z": "Note To use shape/stack functionality make sure to set record_shapes/with_stack\nwhen creating profiler context manager. Signals the profiler that the next profiling step has started. Profiler actions that can be taken at the specified intervals Members: CPU CUDA Returns a callable that can be used as profiler schedule argument. The profiler will skip\nthe firstskip_firststeps, then wait forwaitsteps, then do the warmup for the nextwarmupsteps,\nthen do the active recording for the nextactivesteps and then repeat the cycle starting withwaitsteps.\nThe optional number of cycles is specified with the repeat parameter, the zero value means that\nthe cycles will continue until the profiling is finished. Outputs tracing files to directory ofdir_name, then that directory can be\ndirectly delivered to tensorboard as logdir.worker_nameshould be unique for each worker in distributed scenario,\nit will be set to \u2018[hostname]_[pid]\u2019 by default.",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "What does this function also eliminate?",
        "Y": "non-consecutive duplicate values",
        "Z": "Returns the unique elements of the input tensor. Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When what is specified,torch.unique always sort the tensor at the beginning regardless of thesortargument?",
        "Y": "dim",
        "Z": "Returns the unique elements of the input tensor. Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Is sorting slow or fast?",
        "Y": "slow",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What type of duplicate values does this function eliminate?",
        "Y": "non-consecutive",
        "Z": "Returns the unique elements of the input tensor. Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When dim is specified, what are the two implementations of torch.unique?",
        "Y": "the CUDA implementation and the CPU implementation",
        "Z": "Returns the unique elements of the input tensor. Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What does this function eliminate?",
        "Y": "non-consecutive duplicate values",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Where does this function currently sort the tensor when dim is specified?",
        "Y": "CUDA implementation and the CPU implementation",
        "Z": "Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Why is this function different fromtorch.unique_consecutive()?",
        "Y": "eliminates non-consecutive duplicate values",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When dim is specified, what two implementations of Torch.unique sort the tensor at the beginning regardless of thesortargument",
        "Y": "CUDA implementation and the CPU implementation",
        "Z": "Note This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the input tensor called?",
        "Y": "input(Tensor)",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What doestorch.unique always sort the tensor regardless of?",
        "Y": "thesortargument",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When dim is specified, what two implementations dotorch.unique always sort the tensor at the beginning regardless of thesortar",
        "Y": "the CUDA implementation and the CPU implementation",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "In what implementation does Torch.unique always sort the tensor at the beginning regardless of thesortargument?",
        "Y": "CUDA",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the input tensor sorted?",
        "Y": "input(Tensor)",
        "Z": "input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "In which implementation does torch.unique always sort the tensor at the beginning regardless of thesortargument?",
        "Y": "CUDA implementation",
        "Z": "Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "When is it recommended to usetorch.unique_consecutive()?",
        "Y": "if your input tensor is already sorted",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Whether to sort the input tensor in ascending order before returning as output?",
        "Y": "unique elements",
        "Z": "Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "In what implementation is torch.unique always sort the tensor at the beginning regardless of thesortargument?",
        "Y": "CUDA",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort what in ascend",
        "Y": "unique elements",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What returns the indices for where elements in the original input ended up in the returned unique list?",
        "Y": "return_inverse(bool)",
        "Z": "input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the function that returns the counts for each unique element?",
        "Y": "return_counts",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the dimension to apply unique?",
        "Y": "dim(int)",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What returns the unique of the flattened input?",
        "Y": "If None",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is another term for a tensor?",
        "Y": "a tuple of tensors",
        "Z": "input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the input tensor sorted(bool)?",
        "Y": "input(Tensor)",
        "Z": "input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the function that returns the indices for where elements in the original input ended up in the returned unique list?",
        "Y": "return_inverse",
        "Z": "This function is different fromtorch.unique_consecutive()in the sense that\nthis function also eliminates non-consecutive duplicate values. Note Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the unique of the flattened input?",
        "Y": "If None",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the default value for a tensor or a tuple of tensors containing unique elements?",
        "Y": "default:None",
        "Z": "input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Whether to sort the unique elements in ascending order before returning as output?",
        "Y": "sorted",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What containing a unique element is default:None?",
        "Y": "tensor or a tuple of tensors",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is sorted(bool)?",
        "Y": "Whether to sort the unique elements in ascending order before returning as output",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "if None, what is returned?",
        "Y": "the unique of the flattened input",
        "Z": "return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "A tensor or a tuple of tensors containing tensors containing tensor",
        "Y": "default:None",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is output(Tensor)?",
        "Y": "output list of unique scalar elements",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Whether to return the indices for where elements in the original input ended up in the returned unique list?",
        "Y": "return_inverse",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is output(Tensor) a tuple of tensors containing?",
        "Y": "output list of unique scalar elements",
        "Z": "Currently in the CUDA implementation and the CPU implementation when dim is specified,torch.uniquealways sort the tensor at the beginning regardless of thesortargument.\nSorting could be slow, so if your input tensor is already sorted, it is recommended to usetorch.unique_consecutive()which avoids the sorting. input(Tensor) \u2013 the input tensor sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "Whether to also return the counts for each unique element. dim(int) \u2013 the dimension to apply unique. if None, the unique",
        "Y": "return_counts",
        "Z": "return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the output list of unique scalar elements?",
        "Y": "output(Tensor)",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the function that returns the output list of unique scalar elements?",
        "Y": "inverse_indices",
        "Z": "dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What will return an additional returned tensor representing the indices for where elements in the original input map to in the output?",
        "Y": "ifreturn_inverseis True",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What function returns an additional returned tensor representing the indices for where elements in the original input map to in the output?",
        "Y": "ifreturn_inverseis True",
        "Z": "A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What does output(Tensor) contain?",
        "Y": "unique scalar elements",
        "Z": "A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the default return value for inverse_indices(Tensor)?",
        "Y": "ifreturn_inverseis True",
        "Z": "inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor. counts(Tensor): (optional) ifreturn_countsis True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What function returns an additional returned tensor for where elements in the original input map to in the output?",
        "Y": "ifreturn_inverseis True",
        "Z": "sorted(bool) \u2013 Whether to sort the unique elements in ascending order\nbefore returning as output. return_inverse(bool) \u2013 Whether to also return the indices for where\nelements in the original input ended up in the returned unique list. return_counts(bool) \u2013 Whether to also return the counts for each unique\nelement. dim(int) \u2013 the dimension to apply unique. if None, the unique of the\nflattened input is returned. default:None  A tensor or a tuple of tensors containing output(Tensor): the output list of unique scalar elements. inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What will this function return if it is not true?",
        "Y": "a single tensor",
        "Z": "inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor. counts(Tensor): (optional) ifreturn_countsis True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the function that returns the indices for where elements in the original input map to in the output?",
        "Y": "inverse_indices",
        "Z": "inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor. counts(Tensor): (optional) ifreturn_countsis True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the default return value for counts(Tensor)?",
        "Y": "ifreturn_countsis True",
        "Z": "inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor. counts(Tensor): (optional) ifreturn_countsis True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is the name of the indices returned by ifreturn_inverseis True?",
        "Y": "inverse_indices",
        "Z": "inverse_indices(Tensor): (optional) ifreturn_inverseis True, there will be an additional\nreturned tensor (same shape as input) representing the indices\nfor where elements in the original input map to in the output;\notherwise, this function will only return a single tensor. counts(Tensor): (optional) ifreturn_countsis True, there will be an additional\nreturned tensor (same shape as output or output.size(dim),\nif dim was specified) representing the number of occurrences\nfor each unique value or tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "What is a package implementing various optimization algorithms?",
        "Y": "torch.optimis",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What can be easily integrated in the future?",
        "Y": "more sophisticated ones",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the package that implements various optimization algorithms?",
        "Y": "usetorch.optim",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you need to construct to use torch.optim?",
        "Y": "an optimizer object",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the object that will hold the current state and update the parameters based on the computed gradients?",
        "Y": "usetorch.optimyou",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you have to give it an iterable containing the parameters (all should beVariables) to optimize?",
        "Y": "an Optimizer",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are some options you can specify for an Optimizer?",
        "Y": "the learning rate, weight decay, etc",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do the parameters update based on?",
        "Y": "computed gradients",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What must you construct to usetorch.optim?",
        "Y": "an optimizer object",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an iterable containing the parameters to optimize?",
        "Y": "an Optimizer",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are some options that you can specify in an Optimizer?",
        "Y": "learning rate, weight decay",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you have to give it an iterable containing the parameters to optimize?",
        "Y": "an Optimizer",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How do you move a model to the GPU?",
        "Y": "via.cuda()",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What will be different objects with those before the call?",
        "Y": "Parameters of a model after.cuda()",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What must you give to construct an Optimizer?",
        "Y": "an iterable",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do you need to do before constructing optimizers for a model?",
        "Y": "move a model to GPU via.cuda()",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Parameters of a model will be different objects with those before the call.",
        "Y": "after.cuda()",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "If you need to move a model to GPU via.cuda(), please do so before constructing what?",
        "Y": "before constructing optimizers",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Parameters of a model after.cuda() will be different objects with what?",
        "Y": "before the call",
        "Z": "Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Where should optimized parameters live when optimizers are constructed and used?",
        "Y": "consistent locations",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an example of a model where optimized parameters live in consistent locations when optimizers are constructed and used?",
        "Y": "Example",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an example of a consistent location when optimizers are constructed and used?",
        "Y": "Example",
        "Z": "To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Optimizers support specifying what?",
        "Y": "per-parameter options",
        "Z": "Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Instead of passing an iterable ofVariables, pass in what?",
        "Y": "iterable ofdicts",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should each of the iterable ofdicts contain?",
        "Y": "aparamskey",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should other keys match?",
        "Y": "keyword arguments",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the key that should match the keyword arguments accepted by the optimizers?",
        "Y": "Note",
        "Z": "Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Options can still be passed as what?",
        "Y": "keyword arguments",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Option arguments will be used as what in the groups that didn't override them?",
        "Y": "defaults",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a keyword argument useful for when you want to only pass options as keyword arguments?",
        "Y": "vary a single option",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an example of a useful feature that can be passed as keyword arguments?",
        "Y": "per-layer learning rates",
        "Z": "Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an example of a way to specify?",
        "Y": "per-layer learning rates",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a good example of what you can pass options as keyword arguments?",
        "Y": "per-layer learning rates",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Options will be used as what in the groups that didn\u2019t override them?",
        "Y": "defaults",
        "Z": "You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What model's parameters will use the default learning rate of1e-2?",
        "Y": "model.base",
        "Z": "Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does all optimizers implement that updates the parameters?",
        "Y": "a step() method",
        "Z": "Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How many ways can a step() method be used?",
        "Y": "two ways",
        "Z": "torch.optimis a package implementing various optimization algorithms.\nMost commonly used methods are already supported, and the interface is general\nenough, so that more sophisticated ones can be also easily integrated in the\nfuture. To usetorch.optimyou have to construct an optimizer object, that will hold\nthe current state and will update the parameters based on the computed gradients. To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What model.classifier's parameters will use a learning rate of1e-3?",
        "Y": "model.classifier",
        "Z": "Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a step() method useful when one wants to specify?",
        "Y": "per-layer learning rates",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What method updates the parameters?",
        "Y": "a step()method",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When can a step()method be called?",
        "Y": "once the gradients are computed using e.g.backward()",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How is the a step()method used by most optimizers?",
        "Y": "simplified version",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How can the function be called once the gradients are computed?",
        "Y": "backward()",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are two examples of optimization algorithms that need to reevaluate the function multiple times?",
        "Y": "Conjugate Gradient and LBFGS",
        "Z": "Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should clear the gradients, compute the loss, and return it?",
        "Y": "The closure",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "The function can be called once the gradients are computed using e.g. what?",
        "Y": "backward()",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are two examples of algorithms that need to reevaluate the function multiple times?",
        "Y": "Conjugate Gradient and LBFGS",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should the closure do?",
        "Y": "compute the loss",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What version of the function is supported by most optimizers?",
        "Y": "simplified version",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which optimization algorithms need to reevaluate the function multiple times?",
        "Y": "Conjugate Gradient and LBFGS",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the closure for all optimizers?",
        "Y": "Base class",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What type of version is supported by most optimizers?",
        "Y": "simplified",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an example of a closure for all optimizers?",
        "Y": "Base class",
        "Z": "Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithms need to reevaluate the function multiple times?",
        "Y": "Conjugate Gradient and LBFGS",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an example for all optimizers?",
        "Y": "Base class",
        "Z": "Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What needs to be specified as collections that have a deterministic ordering that is consistent between runs?",
        "Y": "Warning Parameters",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don't satisfy Warning Parameters' properties?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is params(iterable)?",
        "Y": "iterable oftorch.Tensors ordicts",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does params(iterable) specify to be optimized?",
        "Y": "Tensors",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a dict that contains default values of optimization options?",
        "Y": "defaults",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of all optimizers?",
        "Y": "Base class",
        "Z": "Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What need to be specified as collections that have a deterministic ordering that is consistent between runs?",
        "Y": "Warning Parameters",
        "Z": "If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don't satisfy Warning Parameters?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is an iterable oftorch.Tensors ordict?",
        "Y": "params",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does params(iterable) do?",
        "Y": "Specifies what Tensors should be optimized",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a dict containing default values of optimization options?",
        "Y": "defaults",
        "Z": "defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What specifies what Tensors should be optimized?",
        "Y": "params(iterable)",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the dict that contains default values of optimization options?",
        "Y": "Optimizer.add_param_group",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don\u2019t satisfy Warning Parameters?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Parameters need to be specified as what?",
        "Y": "collections that have a deterministic ordering that is consistent between runs",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don't satisfy deterministic ordering?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are examples of objects that don\u2019t satisfy the properties of parameters?",
        "Y": "sets and iterators over values of dictionaries",
        "Z": "Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What adds a param group to theOptimizersparam_groups?",
        "Y": "Optimizer.add_param_group",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning Parameters need to be specified as collections that have a deterministic\nordering that is consistent between runs. Examples of objects that don\u2019t\nsatisfy those properties are sets and iterators over values of dictionaries. params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Loads the optimizer state?",
        "Y": "Optimizer.load_state_dict",
        "Z": "Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What performs a single optimization step (parameter update)?",
        "Y": "Optimizer.step",
        "Z": "Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the optimization step?",
        "Y": "Optimizer.zero_grad",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Optimizer.zero_grad implement?",
        "Y": "Adagrad",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does Optimizer.load_state_dict load?",
        "Y": "optimizer state",
        "Z": "Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the gradient of all optimizedtorch.Tensors?",
        "Y": "zero",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Adadelta implement?",
        "Y": "Adagrad algorithm",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements Adadelta?",
        "Y": "Adagrad algorithm",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements Adadelta?",
        "Y": "Adagrad",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does theOptimizersparam_groups do?",
        "Y": "Add a param group",
        "Z": "Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements?",
        "Y": "resilient backpropagation algorithm",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements Adadelta algorithm?",
        "Y": "Adagrad algorithm",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Adam implement?",
        "Y": "AdamW algorithm",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the lazy version of Adam algorithm suitable for?",
        "Y": "sparse tensors",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does Optimizer.state_dict do?",
        "Y": "Loads the optimizer state",
        "Z": "Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a variant of Adam based on infinity norm?",
        "Y": "Adamax algorithm",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Returns the state of the optimizer as what?",
        "Y": "adict",
        "Z": "Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Adamax implement?",
        "Y": "Averaged Stochastic Gradient Descent",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements lazy version of Adam algorithm suitable for sparse tensors?",
        "Y": "AdamW",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm is suitable for sparse tensors?",
        "Y": "lazy version of Adam algorithm",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements a variant of Adam based on infinity norm?",
        "Y": "Adamax",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm is heavily inspired byminFunc?",
        "Y": "L-BFGS",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements Adam?",
        "Y": "AdamW",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "L-BFGS algorithm is heavily inspired by what?",
        "Y": "minFunc",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does L-BFGS implement?",
        "Y": "RMSprop algorithm",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Sets the gradients of all optimizedtorch.Tensors to what?",
        "Y": "zero",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements the gradients of all optimizedtorch.Tensors to zero?",
        "Y": "Adadelta algorithm",
        "Z": "Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the L-BFGS algorithm heavily inspired by?",
        "Y": "minFunc",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements the resilient backpropagation algorithm?",
        "Y": "RMSprop",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does RMSprop implement?",
        "Y": "resilient backpropagation algorithm",
        "Z": "Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements the resilient backpropagation algorithm?",
        "Y": "RMSprop algorithm",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements Adagrad?",
        "Y": "Adam",
        "Z": "Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the resilient backpropagation algorithm implement?",
        "Y": "stochastic gradient descent",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm does Adagrad implement?",
        "Y": "Adam algorithm",
        "Z": "Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Which algorithm implements Adam?",
        "Y": "AdamW algorithm",
        "Z": "Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm is implemented optionally with momentum?",
        "Y": "stochastic gradient descent",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What implements the lazy version of Adam algorithm suitable for sparse tensors?",
        "Y": "Adam algorithm",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements the lazy version of Adam algorithm?",
        "Y": "AdamW",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What version of Adam algorithm is suitable for sparse tensors?",
        "Y": "lazy",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What algorithm implements Adam algorithm?",
        "Y": "AdamW algorithm",
        "Z": "Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum).",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What version of the function can be called once the gradients are computed?",
        "Y": "simplified",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does a closure allow an optimization algorithm to do?",
        "Y": "recompute your model",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What will be used for all parameters?",
        "Y": "momentum of 0.9",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is this useful when one wants to specify?",
        "Y": "per-layer learning rates",
        "Z": "To construct an Optimizeryou have to give it an iterable containing the\nparameters (all should beVariables) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc. Note If you need to move a model to GPU via.cuda(), please do so before\nconstructing optimizers for it. Parameters of a model after.cuda()will\nbe different objects with those before the call. In general, you should make sure that optimized parameters live in\nconsistent locations when optimizers are constructed and used. Example: Optimizers also support specifying per-parameter options. To do this, instead\nof passing an iterable ofVariables, pass in an iterable ofdicts. Each of them will define a separate parameter group, and should contain\naparamskey, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group. Note You can still pass options as keyword arguments. They will be used as\ndefaults, in the groups that didn\u2019t override them. This is useful when you\nonly want to vary a single option, while keeping all others consistent\nbetween parameter groups. For example, this is very useful when one wants to specify per-layer learning rates: This means thatmodel.base\u2019s parameters will use the default learning rate of1e-2,model.classifier\u2019s parameters will use a learning rate of1e-3, and a momentum of 0.9will be used for all parameters. All optimizers implement a step()method, that updates the\nparameters. It can be used in two ways: This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example: Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it. Example: Base class for all optimizers. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What kind of version is supported by most optimizers?",
        "Y": "simplified",
        "Z": "This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.backward(). Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does torch.optim.lr_scheduler provide methods to adjust the learning rate based on?",
        "Y": "epochs.torch.optim.lr_scheduler",
        "Z": "torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When should learning rate scheduling be applied?",
        "Y": "after optimizer\u2019s update",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does reduceLROnPlateauallow learning rate reducing based on?",
        "Y": "validation measurements",
        "Z": "torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should be applied after optimizer\u2019s update?",
        "Y": "Learning rate scheduling",
        "Z": "params(iterable) \u2013 an iterable oftorch.Tensors ordicts. Specifies what Tensors should be optimized. defaults\u2013 (dict): a dict containing default values of optimization\noptions (used when a parameter group doesn\u2019t specify them). Optimizer.add_param_group Add a param group to theOptimizersparam_groups. Optimizer.load_state_dict Loads the optimizer state. Optimizer.state_dict Returns the state of the optimizer as adict. Optimizer.step Performs a single optimization step (parameter update). Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are most learning rate schedulers called?",
        "Y": "back-to-back",
        "Z": "torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How is each learning rate scheduler applied?",
        "Y": "one after the other on the learning rate obtained by the one preceding it",
        "Z": "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What do we use the following template to refer to in many places in the documentation?",
        "Y": "schedulers",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should be applied after an optimizer's update?",
        "Y": "Learning rate scheduling",
        "Z": "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "In many places in the documentation, we will use what to refer to schedulers algorithms?",
        "Y": "template",
        "Z": "Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What are learning rate schedulers called back-to-back?",
        "Y": "chaining schedulers",
        "Z": "Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How is each scheduler applied?",
        "Y": "one after the other on the learning rate obtained by the one preceding it",
        "Z": "torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "In many places in the documentation, we will use the following template to refer to what?",
        "Y": "schedulers",
        "Z": "Optimizer.zero_grad Sets the gradients of all optimizedtorch.Tensors to zero.   Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How is each scheduler applied on the learning rate obtained by the one preceding it?",
        "Y": "one after the other",
        "Z": "Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What type of algorithms are often referenced in documentation?",
        "Y": "schedulers",
        "Z": "Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What type of algorithms are often referenced in the documentation?",
        "Y": "schedulers",
        "Z": "In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When was the learning rate scheduler expected to be called before the optimizer's update?",
        "Y": "1.1.0",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the learning rate scheduler call before the optimizer's update?",
        "Y": "callingoptimizer.step()",
        "Z": "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should you check if you are unable to reproduce results after upgrading to PyTorch 1.1.0?",
        "Y": "if you are callingscheduler.step()at the wrong time",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "In what year did 1.1.0 change the behavior of the learning rate scheduler?",
        "Y": "BC",
        "Z": "Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate scheduler called before the optimizer's update?",
        "Y": "lr_scheduler.ExponentialLR",
        "Z": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check what?",
        "Y": "if you are callingscheduler.step()at the wrong time",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called when?",
        "Y": "before the optimizer\u2019s update",
        "Z": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What should be called if you are callingscheduler.step() at the wrong time?",
        "Y": "lr_scheduler.LambdaLR",
        "Z": "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How did 1.1.0 change the learning rate scheduler behavior?",
        "Y": "BC",
        "Z": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.LambdaLR call?",
        "Y": "lr_scheduler.LambdaLR",
        "Z": "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Multiply the learning rate of each parameter group by the factor given in the specified function?",
        "Y": "MultiplicativeLR",
        "Z": "Implements Adadelta algorithm.   Implements Adagrad algorithm.   Implements Adam algorithm.   Implements AdamW algorithm.   Implements lazy version of Adam algorithm suitable for sparse tensors.   Implements Adamax algorithm (a variant of Adam based on infinity norm).   Implements Averaged Stochastic Gradient Descent.   Implements L-BFGS algorithm, heavily inspired byminFunc.   Implements RMSprop algorithm.   Implements the resilient backpropagation algorithm.   Implements stochastic gradient descent (optionally with momentum). torch.optim.lr_schedulerprovides several methods to adjust the learning\nrate based on the number of epochs.torch.optim.lr_scheduler.ReduceLROnPlateauallows dynamic learning rate reducing based on some validation measurements. Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: Example: Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. Example: In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. Warning Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before\nthe optimizer\u2019s update; 1.1.0 changed this behavior in a BC-breaking way.  If you use\nthe learning rate scheduler (callingscheduler.step()) before the optimizer\u2019s update\n(callingoptimizer.step()), this will skip the first value of the learning rate schedule.\nIf you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check\nif you are callingscheduler.step()at the wrong time. lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every what",
        "Y": "step",
        "Z": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does MultiStepLR Decay the learning rate of each parameter group by?",
        "Y": "gamma",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What type of LR does lr_scheduler?",
        "Y": "Exponential",
        "Z": "lr_scheduler.LambdaLR Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Sets the learning rate of each parameter group to what?",
        "Y": "the initial lr times a given function",
        "Z": "Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler stand for?",
        "Y": "ExponentialLR",
        "Z": "Sets the learning rate of each parameter group to the initial lr times a given function. lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by?",
        "Y": "the factor given in the specified function",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How does lr_scheduler.StepLR determine the learning rate of each parameter group?",
        "Y": "gamma",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group determined by?",
        "Y": "gamma",
        "Z": "lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.CosineAnnealingLR do?",
        "Y": "lr_scheduler.CosineAnnealingLR",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the",
        "Y": "one of the milestones",
        "Z": "Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Decays the learning rate of each parameter group by gamma every epoch?",
        "Y": "ExponentialLR",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is another name for CosineAnnealingLR?",
        "Y": "CosineAnnealingLR",
        "Z": "Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group by the factor given in the specified function?",
        "Y": "Multiply",
        "Z": "Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How does lr_scheduler determine the learning rate of each parameter group?",
        "Y": "gamma",
        "Z": "Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "StepLR Decays the learning rate of each parameter group by gamma every what?",
        "Y": "step_size epochs",
        "Z": "Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How does MultiStepLR determine the learning rate of each parameter group?",
        "Y": "gamma",
        "Z": "lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestone",
        "Y": "MultiStepLR",
        "Z": "Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Decays the learning rate of each parameter group by gamma what?",
        "Y": "every step_size epochs",
        "Z": "Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does MultiStepLR use to determine the learning rate of each parameter group?",
        "Y": "gamma",
        "Z": "Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "Decays the learning rate of each parameter group by what?",
        "Y": "gamma",
        "Z": "Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group determined by every epoch?",
        "Y": "gamma",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What sets the learning rate of each parameter group by gamma every epoch?",
        "Y": "lr_scheduler",
        "Z": "Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What reduces learning rate when a metric has stopped improving?",
        "Y": "lr_scheduler.CyclicLR",
        "Z": "Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does ExponentialLR Decay the learning rate of each parameter group by every epoch?",
        "Y": "gamma",
        "Z": "lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When does ReduceLROnPlateau Reduce learning rate?",
        "Y": "when a metric has stopped improving",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What type of LR reduces learning rate when a metric has stopped improving?",
        "Y": "Cyclic",
        "Z": "Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does it do to the learning rate of each parameter group by gamma every epoch?",
        "Y": "Decays the learning rate of each parameter group by gamma every epoch",
        "Z": "Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group by gamma every epoch?",
        "Y": "Decays",
        "Z": "Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "When does ReduceLROnPlateau reduce learning rate?",
        "Y": "when a metric has stopped improving",
        "Z": "Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What set the learning rate of each parameter group according to cyclical learning rate policy?",
        "Y": "OneCycleLR",
        "Z": "lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is used to set the learning rate of each parameter group?",
        "Y": "a cosine annealing schedule",
        "Z": "Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.CyclicLR stand for?",
        "Y": "OneCycleLR",
        "Z": "Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the name of the cosine annealing schedule?",
        "Y": "lr_scheduler",
        "Z": "Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group according to cyclical learning rate policy?",
        "Y": "OneCycleLR",
        "Z": "lr_scheduler.MultiplicativeLR Multiply the learning rate of each parameter group by the factor given in the specified function. lr_scheduler.StepLR Decays the learning rate of each parameter group by gamma every step_size epochs. lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.ReduceLROnPlateau do when a metric has stopped improving?",
        "Y": "Reduce learning rate",
        "Z": "lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group according to?",
        "Y": "1cycle",
        "Z": "lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.CosineAnnealingWarmRestart?",
        "Y": "lr_scheduler.CosineAnnealingWarmRestarts",
        "Z": "lr_scheduler.MultiStepLR Decays the learning rate of each parameter group by gamma once the number of epoch reaches one of the milestones. lr_scheduler.ExponentialLR Decays the learning rate of each parameter group by gamma every epoch. lr_scheduler.CosineAnnealingLR Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr andTcurT_{cur}Tcur\u200bis the number of epochs since the last restart in SGDR: lr_scheduler.ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What happens when a metric has stopped improving?",
        "Y": "Reduce learning rate",
        "Z": "Reduce learning rate when a metric has stopped improving. lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group according to the OneCycleLR learning rate policy?",
        "Y": "1cycle",
        "Z": "lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to what?",
        "Y": "1cycle learning rate policy",
        "Z": "lr_scheduler.CyclicLR Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.CosineAnnealingWarmRestart do?",
        "Y": "lr_scheduler.CosineAnnealingWarmRestarts",
        "Z": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to what learning rate policy?",
        "Y": "1cycle",
        "Z": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the cyclical learning rate policy?",
        "Y": "CLR",
        "Z": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does lr_scheduler.OneCycleLR set the learning rate of each parameter group according to?",
        "Y": "1cycle learning rate policy",
        "Z": "Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How long is the learning rate of each parameter group?",
        "Y": "1cycle",
        "Z": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate of each parameter group set according to?",
        "Y": "1cycle learning rate policy",
        "Z": "lr_scheduler.OneCycleLR Sets the learning rate of each parameter group according to the 1cycle learning rate policy. lr_scheduler.CosineAnnealingWarmRestarts Set the learning rate of each parameter group using a cosine annealing schedule, where\u03b7max\\eta_{max}\u03b7max\u200bis set to the initial lr,TcurT_{cur}Tcur\u200bis the number of epochs since the last restart andTiT_{i}Ti\u200bis the number of epochs between two warm restarts in SGDR:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does SWA stand for?",
        "Y": "Stochastic Weight Averaging",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the utility function used to do at the end of training?",
        "Y": "update SWA batch normalization statistics",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What has SWA been proposed in?",
        "Y": "Wider Optima and Better Generalization",
        "Z": "SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does SWA lead to?",
        "Y": "Wider Optima and Better Generalization",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What class computes the weights of the SWA model?",
        "Y": "AveragedModelclass",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What will keep track of the running averages of the parameters of the model?",
        "Y": "arbitrarytorch.nn.Moduleobject.swa_model",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What function is used to update the averages of the parameters of the model?",
        "Y": "theupdate_parameters()function",
        "Z": "torch.optim.swa_utilsimplements Stochastic Weight Averaging (SWA). In particular,torch.optim.swa_utils.AveragedModelclass implements SWA models,torch.optim.swa_utils.SWALRimplements the SWA learning rate scheduler andtorch.optim.swa_utils.update_bn()is a utility function used to update SWA batch\nnormalization statistics at the end of training. SWA has been proposed inAveraging Weights Leads to Wider Optima and Better Generalization. AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What function can be used to update the averages of the parameters of the model?",
        "Y": "theupdate_parameters()function",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What will keep track of the running averages of the parameters of the modelmodel?",
        "Y": "arbitrarytorch.nn.Moduleobject.swa_model",
        "Z": "Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What function can be used to update the averages of the parameters of a model?",
        "Y": "theupdate_parameters()function",
        "Z": "Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate set to in SWA?",
        "Y": "a high constant value",
        "Z": "Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\".",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is the learning rate annealed to in each parameter group?",
        "Y": "0.05 in 5 epochs",
        "Z": "Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\".",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does SWALR do?",
        "Y": "anneals the learning rate to a fixed value, and then keeps it constant",
        "Z": "Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\".",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What annealing to a fixed value can be used instead of linear annealing?",
        "Y": "cosine",
        "Z": "Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\".",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How can you use cosine annealing to a fixed value instead of linear annealing?",
        "Y": "settinganneal_strategy=\"cos\"",
        "Z": "You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloaderloader?",
        "Y": "update_bn()",
        "Z": "Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What type of annealing can you use instead of linear annealing?",
        "Y": "cosine",
        "Z": "You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is a utility function that allows to compute the batchnorm statistics for the SWA model on a given dataloaderloaderat",
        "Y": "update_bn()",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() apply to every element in the dataloader?",
        "Y": "theswa_model",
        "Z": "update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What applies theswa_modelto every element in the dataloader?",
        "Y": "update_bn()",
        "Z": "update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "what does update_bn() apply to every element in the dataloader?",
        "Y": "theswa_model",
        "Z": "update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() assume each batch in the dataloaderloader is a?",
        "Y": "tensor",
        "Z": "Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "If your dataloader has a different structure, you can update the batch normalization statistics of what?",
        "Y": "theswa_model",
        "Z": "You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() assume that each batch in the dataloaderloader is either a list of tensors or",
        "Y": "tensors",
        "Z": "Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "If your dataloader has what, you can update the batch normalization statistics of theswa_model by doing a forward pass with the",
        "Y": "a different structure",
        "Z": "update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does update_bn() assume that each batch in the dataloaderloader is a tensors or?",
        "Y": "a list of tensors",
        "Z": "update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset.",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What does the SWA model accumulate?",
        "Y": "the averages of the weights",
        "Z": "In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How many epochs does the SWA model train?",
        "Y": "300",
        "Z": "Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is swa_model?",
        "Y": "SWA model",
        "Z": "AveragedModelclass serves to compute the weights of the SWA model. You can create an\naveraged model by running: Here the modelmodelcan be an arbitrarytorch.nn.Moduleobject.swa_modelwill keep track of the running averages of the parameters of themodel. To update these\naverages, you can use theupdate_parameters()function: Typically, in SWA the learning rate is set to a high constant value.SWALRis a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: You can also use cosine annealing to a fixed value instead of linear annealing by settinganneal_strategy=\"cos\". update_bn()is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloaderloaderat the end of training: update_bn()applies theswa_modelto every element in the dataloader and computes the activation\nstatistics for each batch normalization layer in the model. Warning update_bn()assumes that each batch in the dataloaderloaderis either a tensors or a list of\ntensors where the first element is the tensor that the networkswa_modelshould be applied to.\nIf your dataloader has a different structure, you can update the batch normalization statistics of theswa_modelby doing a forward pass with theswa_modelon each element of the dataset. By default,torch.optim.swa_utils.AveragedModelcomputes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with theavg_fnparameter. In the following exampleema_modelcomputes an exponential moving average. Example: In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How many epochs do we train the model for?",
        "Y": "300",
        "Z": "In the example below,swa_modelis the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "What is filled with numbers sampled from the discrete uniform distribution over[from,to-1]?",
        "Y": "Fillsselftensor",
        "Z": "Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"
    },
    {
        "X": "If not specified, the values are usually only what?",
        "Y": "bounded byselftensor\u2019s data type",
        "Z": "Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"
    },
    {
        "X": "What will be [0,2mantissa] for floating point types if unspecified?",
        "Y": "range",
        "Z": "Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"
    },
    {
        "X": "What will be uniform in[0,253]?",
        "Y": "random_()",
        "Z": "Fillsselftensor with numbers sampled from the discrete uniform\ndistribution over[from,to-1]. If not specified, the values are usually\nonly bounded byselftensor\u2019s data type. However, for floating point\ntypes, if unspecified, range will be[0,2^mantissa]to ensure that every\nvalue is representable. For example,torch.tensor(1, dtype=torch.double).random_()will be uniform in[0,2^53].",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"
    },
    {
        "X": "What does the variant oftorch.quantile() do with NaNvalues?",
        "Y": "ignores",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What happens if all values in a reduced row areNaN?",
        "Y": "If all values in a reduced row areNaNthen the quantiles for that reduction will beNaN",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What is the name of the variant oftorch.quantile() that \"ignores\"NaNvalues?",
        "Y": "documentation fortorch.quantile()",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What does fortorch.quantile() do?",
        "Y": "ignores",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "If all values in a reduced row areNaN, then the quantiles for that reduction will beNaN?",
        "Y": "If all values in a reduced row areNaN",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What is a variant oftorch.quantile() that \"ignores\"NaNvalues?",
        "Y": "fortorch.quantile()",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What is a scalar or 1D tensor of quantile values in the range [0, 1] dim(int",
        "Y": "q(floatorTensor)",
        "Z": "This is a variant oftorch.quantile()that \u201cignores\u201dNaNvalues,\ncomputing the quantilesqas ifNaNvalues ininputdid\nnot exist. If all values in a reduced row areNaNthen the quantiles for\nthat reduction will beNaN. See the documentation fortorch.quantile(). input(Tensor) \u2013 the input tensor. q(floatorTensor) \u2013 a scalar or 1D tensor of quantile values in the range [0, 1] dim(int) \u2013 the dimension to reduce. keepdim(bool) \u2013 whether the output tensor hasdimretained or not. out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "What represents the eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrixinputor",
        "Y": "a named tuple",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What does this function calculate?",
        "Y": "all eigenvalues (and vectors)",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What defines computation of both eigenvectors and eigenvalues or eigenvalues only?",
        "Y": "boolean argumenteigenvectors",
        "Z": "The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is represented by eigenvalues and eigenvectors of a real symmetric or complex Hermitian matrixinput",
        "Y": "a named tuple",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "The boolean argumenteigenvectorsdefines computation of both eigenvectors and what else?",
        "Y": "eigenvalues",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is computed if it is False?",
        "Y": "eigenvalues",
        "Z": "If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "When are both eigenvalues and eigenvectors computed?",
        "Y": "If it is True",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What portion of the input matrix is used by default?",
        "Y": "upper triangular portion",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What defines computation of both eigenvectors and eigenvalues?",
        "Y": "eigenvectors",
        "Z": "The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What are computed if argumenteigenvectors is False?",
        "Y": "eigenvalues",
        "Z": "The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is used if upperis True?",
        "Y": "lower triangular portion",
        "Z": "The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What are computed if it is False?",
        "Y": "eigenvalues",
        "Z": "If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What are computed if it is True?",
        "Y": "both eigenvalues and eigenvectors",
        "Z": "If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What portion of the input matrixinput is used by default?",
        "Y": "upper triangular portion",
        "Z": "If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Ifupperis False, what is used?",
        "Y": "lower triangular portion",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the default value for the lower triangular portion of the matrix?",
        "Y": "Ifupperis False",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is deprecated in favor oftorch.linalg.eigh()?",
        "Y": "Warning torch.symeig()is",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "The default behavior has changed from using the upper triangular portion of the matrix by default to using what?",
        "Y": "lower triangular portion",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What should be replaced with the lower triangular portion?",
        "Y": "L,_=torch.symeig(A,upper=upper)",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the default behavior of the lower triangular portion of the matrix?",
        "Y": "Ifupperis False",
        "Z": "Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Is torch.symeig() deprecated?",
        "Y": "Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()",
        "Z": "This function returns eigenvalues and eigenvectors\nof a real symmetric or complex Hermitian matrixinputor a batch thereof,\nrepresented by a named tuple (eigenvalues, eigenvectors). This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "The default behavior has changed from using which portion of the matrix by default to using the lower triangular portion?",
        "Y": "upper triangular portion",
        "Z": "The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper",
        "Y": "L,_=torch.symeig",
        "Z": "The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is used ifupperis False?",
        "Y": "lower triangular portion",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What will be removed in a future PyTorch release?",
        "Y": "Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()",
        "Z": "Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What should be replaced with L,V=torch.symeig?",
        "Y": "L,_=torch.symeig",
        "Z": "L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "When are the eigenvalues of each matrix in a batch of matrices returned in ascending order?",
        "Y": "If input is",
        "Z": "L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Which matrix will be transposed regardless of the original strides?",
        "Y": "matrixV",
        "Z": "L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is used to transpose the matrixV?",
        "Y": "stridesV.contiguous().transpose",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "In what order are the eigenvalues returned?",
        "Y": "ascending order",
        "Z": "The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If inputis a batch of matrices, then the eigenvalues of each matrix in the batch is returned in ascending order",
        "Y": "If input is a batch of matrices",
        "Z": "L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Irrespective of the original strides, what happens to the returned matrix?",
        "Y": "matrixVwill be transposed",
        "Z": "L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "In what order are eigenvalues returned?",
        "Y": "ascending",
        "Z": "L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Irrespective of the original strides, what happens to the returned matrixV?",
        "Y": "the returned matrixVwill be transposed",
        "Z": "This function calculates all eigenvalues (and vectors) ofinputsuch thatinput=Vdiag(e)VT\\text{input} = V \\text{diag}(e) V^Tinput=Vdiag(e)VT. The boolean argumenteigenvectorsdefines computation of\nboth eigenvectors and eigenvalues or eigenvalues only. If it is False, only eigenvalues are computed. If it is True,\nboth eigenvalues and eigenvectors are computed. Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What matrix will be transposed regardless of the original strides?",
        "Y": "matrixV",
        "Z": "Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "When are the eigenvalues of each matrix in a batch returned in ascending order?",
        "Y": "If input is a batch of matrices",
        "Z": "Since the input matrixinputis supposed to be symmetric or Hermitian,\nonly the upper triangular portion is used by default. Ifupperis False, then lower triangular portion is used. Warning torch.symeig()is deprecated in favor oftorch.linalg.eigh()and will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion. L,_=torch.symeig(A,upper=upper)should be replaced with L,V=torch.symeig(A,eigenvectors=True,upper=upper)should be replaced with Note The eigenvalues are returned in ascending order. If input is a batch of matrices,\nthen the eigenvalues of each matrix in the batch is returned in ascending order. Note Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "Warning Extra care needs to be taken when what?",
        "Y": "backward through outputs",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "When is a backward through output operation stable?",
        "Y": "when all eigenvalues are distinct",
        "Z": "Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the input tensor of size(*,n,n)(*,n,n)(*,n,n)",
        "Y": "symmetric or Hermitian matrices",
        "Z": "Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What controls whether eigenvectors have to be computed?",
        "Y": "eigenvectors",
        "Z": "Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What needs to be taken when backward through outputs?",
        "Y": "Extra care",
        "Z": "Irrespective of the original strides, the returned matrixVwill\nbe transposed, i.e. with stridesV.contiguous().transpose(-1, -2).stride(). Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "When is backward through outputs stable?",
        "Y": "when all eigenvalues are distinct",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the input tensor of size(,n,n)(*, n, n)(,n,",
        "Y": "zero",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What are the batch dimensions consisting of?",
        "Y": "symmetric or Hermitian matrices",
        "Z": "input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is a named tuple?",
        "Y": "eigenvalues, eigenvectors",
        "Z": "input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "eigenvectors(boolean,optional) \u2013 controls whether eigenvectors have to be computed what?",
        "Y": "upper",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What controls whether eigenvectors have to be computed upper?",
        "Y": "eigenvectors",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "In what order are the eigenvalues in?",
        "Y": "ascending",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "eigenvectors(boolean,optional) \u2013 controls whether to consider what regions?",
        "Y": "upper-triangular or lower-triangular region",
        "Z": "eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order.",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What controls whether to consider upper-triangular or lower-triangular region out?",
        "Y": "upper",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the eigenvectors(Tensor)?",
        "Y": "Shape",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is an empty tensor?",
        "Y": "If eigenvectors=False",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If eigenvectors=False, it's an empty tensor. Otherwise, this tensor contains what?",
        "Y": "orthonormal eigenvectors of theinput",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the name of the tuple that contains the orthonormal eigenvectors of the input?",
        "Y": "Tensor",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output",
        "Y": "upper(boolean,optional)",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What order are the eigenvalues in?",
        "Y": "ascending",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the output tuple of (Tensor, Tensor) containing eigenvalues in ascending order?",
        "Y": "eigenvectors",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor contains the ortho",
        "Y": "If eigenvectors=False",
        "Z": "upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "If eigenvectors=False, it\u2019s an empty tensor. Otherwise, this tensor contains what?",
        "Y": "orthonormal eigenvectors",
        "Z": "Warning Extra care needs to be taken when backward through outputs. Such\noperation is only stable when all eigenvalues are distinct and becomes\nless stable the smallermin\u2061i\u2260j\u2223\u03bbi\u2212\u03bbj\u2223\\min_{i \\neq j} |\\lambda_i - \\lambda_j|mini\ue020=j\u200b\u2223\u03bbi\u200b\u2212\u03bbj\u200b\u2223is. input(Tensor) \u2013 the input tensor of size(\u2217,n,n)(*, n, n)(\u2217,n,n)where*is zero or more\nbatch dimensions consisting of symmetric or Hermitian matrices. eigenvectors(bool,optional) \u2013 controls whether eigenvectors have to be computed upper(boolean,optional) \u2013 controls whether to consider upper-triangular or lower-triangular region out(tuple,optional) \u2013 the output tuple of (Tensor, Tensor)  A named tuple (eigenvalues, eigenvectors) containing eigenvalues(Tensor): Shape(\u2217,m)(*, m)(\u2217,m). The eigenvalues in ascending order. eigenvectors(Tensor): Shape(\u2217,m,m)(*, m, m)(\u2217,m,m).\nIf eigenvectors=False, it\u2019s an empty tensor.\nOtherwise, this tensor contains the orthonormal eigenvectors of theinput. (Tensor,Tensor) Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "What is the name of the function used by Alias fortorch.special.expit?",
        "Y": "Alias fortorch.special.expit()",
        "Z": "Alias fortorch.special.expit().",
        "source": "https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid"
    },
    {
        "X": "What does Alias fortorch.special.expit() do?",
        "Y": "Alias fortorch.special.expit()",
        "Z": "Alias fortorch.special.expit().",
        "source": "https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid"
    },
    {
        "X": "What is the element-wise greatest common divisor of inputandother?",
        "Y": "GCD",
        "Z": "Computes the element-wise greatest common divisor (GCD) ofinputandother. Bothinputandothermust have integer types. Note This definesgcd(0,0)=0gcd(0, 0) = 0gcd(0,0)=0. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"
    },
    {
        "X": "Both inputandothermust have what?",
        "Y": "integer types",
        "Z": "Computes the element-wise greatest common divisor (GCD) ofinputandother. Bothinputandothermust have integer types. Note This definesgcd(0,0)=0gcd(0, 0) = 0gcd(0,0)=0. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"
    },
    {
        "X": "What does gcd(0,0)=0gcd(0,0) =?",
        "Y": "0gcd(0,0)",
        "Z": "Computes the element-wise greatest common divisor (GCD) ofinputandother. Bothinputandothermust have integer types. Note This definesgcd(0,0)=0gcd(0, 0) = 0gcd(0,0)=0. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"
    },
    {
        "X": "Other(Tensor) \u2013 what is the output tensor?",
        "Y": "second input tensor out",
        "Z": "Computes the element-wise greatest common divisor (GCD) ofinputandother. Bothinputandothermust have integer types. Note This definesgcd(0,0)=0gcd(0, 0) = 0gcd(0,0)=0. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the second input tensor out(Tensor,optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"
    },
    {
        "X": "What returns true if the data type ofinputis a floating point data type?",
        "Y": "if the data type ofinputis a floating point data type",
        "Z": "Returns True if the data type ofinputis a floating point data type i.e.,\none oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_floating_point.html#torch.is_floating_point"
    },
    {
        "X": "Returns True what if the data type ofinputis a floating point data type?",
        "Y": "if the data type ofinputis a floating point data type",
        "Z": "Returns True if the data type ofinputis a floating point data type i.e.,\none oftorch.float64,torch.float32,torch.float16, andtorch.bfloat16. input(Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_floating_point.html#torch.is_floating_point"
    },
    {
        "X": "What are the basic building blocks for graphs?",
        "Y": "torch",
        "Z": "These are the basic building blocks for graphs: torch.nn Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1).",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is to be considered a module parameter?",
        "Y": "Tensor",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) What are Pooling layers?",
        "Y": "Containers Convolution Layers",
        "Z": "Containers Convolution Layers Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a weighted sum, nonlinearity?",
        "Y": "Padding Layers Non-linear Activations",
        "Z": "Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a parameter that is to be considered a module parameter?",
        "Y": "not initialized",
        "Z": "Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other",
        "Y": "Pooling layers",
        "Z": "Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d a torch.nn.Conv1dmodule with lazy initialization of thein_channelsargument of theConv1dthat is inferred from theinput.size(1). nn.LazyConv2d a torch.nn.Conv2dmodule with lazy initialization of thein_channelsargument of theConv2dthat is inferred from theinput.size(1). nn.LazyConv3d a torch.nn.Conv3dmodule with lazy initialization of thein_channelsargument of theConv3dthat is inferred from theinput.size(1). nn.LazyConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Padding Layers?",
        "Y": "Pooling layers",
        "Z": "Pooling layers Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normal",
        "Y": "Padding Layers",
        "Z": "Padding Layers Non-linear Activations (weighted sum, nonlinearity) Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is not initialized?",
        "Y": "buffer",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is another term for Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Function",
        "Y": "Non-linear Activations",
        "Z": "Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of container is a base class for all neural network modules?",
        "Y": "sequential container",
        "Z": "Non-linear Activations (other) Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shu",
        "Y": "Normalization Layers",
        "Z": "Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a sequential container hold in a list?",
        "Y": "parameters",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What class holds submodules in a list?",
        "Y": "Base class",
        "Z": "Normalization Layers Recurrent Layers Transformer Layers Linear Layers Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a sequential container do?",
        "Y": "Holds submodules in a list",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of container holds submodules in a list?",
        "Y": "sequential container",
        "Z": "A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d Applies a 2D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose3d Applies a 3D transposed convolution operator over an input image composed of several input planes. nn.LazyConv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Sparse Layers?",
        "Y": "Dropout Layers",
        "Z": "Dropout Layers Sparse Layers Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Holds submodules in what?",
        "Y": "a dictionary",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "A sequential container. Holds submodules in a list. Holds parameters in a dictionary. Holds what for all neural network modules",
        "Y": "Base class",
        "Z": "A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a base class for all neural network modules?",
        "Y": "sequential container",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a module parameter a kind of?",
        "Y": "Tensor",
        "Z": "Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Holds parameters in a list?",
        "Y": "Holds parameters in a list",
        "Z": "Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are parameters stored?",
        "Y": "a dictionary",
        "Z": "Distance Functions Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Shuffle Layers?",
        "Y": "Loss Functions Vision Layers",
        "Z": "Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does Holds parameters in a dictionary?",
        "Y": "Holds parameters in a list",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where can parameters be held?",
        "Y": "a dictionary",
        "Z": "A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are Vision Layers Shuffle Layers?",
        "Y": "Loss Functions",
        "Z": "Loss Functions Vision Layers Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What are multi-GPU, distributed?",
        "Y": "Shuffle Layers DataParallel Layers",
        "Z": "Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a container that holds submodules in a list?",
        "Y": "sequential container",
        "Z": "A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "Where are parameters held?",
        "Y": "a dictionary",
        "Z": "Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is a global hook for module?",
        "Y": "Global Hooks For Module",
        "Z": "Shuffle Layers DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is multi-GPU, distributed?",
        "Y": "DataParallel Layers",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What does a global hook for module do?",
        "Y": "Holds submodules in a list",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What is the name of the hook that is to be considered a module parameter?",
        "Y": "Global Hooks For Module",
        "Z": "DataParallel Layers (multi-GPU, distributed) Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
    {
        "X": "What type of initialization is to be considered a module parameter?",
        "Y": "Tensor",
        "Z": "Utilities Quantized Functions Lazy Modules Initialization   A kind of Tensor that is to be considered a module parameter.   A parameter that is not initialized.   A buffer that is not initialized.   Base class for all neural network modules.   A sequential container.   Holds submodules in a list.   Holds submodules in a dictionary.   Holds parameters in a list.   Holds parameters in a dictionary. Global Hooks For Module   Registers a forward pre-hook common to all modules.   Registers a global forward hook for all the modules   Registers a backward hook common to all the modules. nn.Conv1d Applies a 1D convolution over an input signal composed of several input planes. nn.Conv2d Applies a 2D convolution over an input signal composed of several input planes. nn.Conv3d Applies a 3D convolution over an input signal composed of several input planes. nn.ConvTranspose1d Applies a 1D transposed convolution operator over an input image composed of several input planes. nn.ConvTranspose2d",
        "source": "https://pytorch.org/docs/stable/nn.html"
    },
]
